name: Pipeline Run

on:
  workflow_dispatch:
    inputs:
      stage:
        description: 'Run specific stage (extract, enrich, flora, author, author-randomize, email) or all'
        type: choice
        default: 'all'
        options:
          - all
          - extract
          - enrich
          - flora
          - author
          - author-randomize
          - email
      limit:
        description: 'Max preprints per stage (0 = unlimited)'
        default: '0'
      email_limit:
        description: 'Max emails (recipients) to send per run (0 = skip email)'
        default: '0'
      enrich_workers:
        description: 'Parallel workers for Crossref/OpenAlex enrichment'
        default: '6'
      orcid_workers:
        description: 'Parallel workers for ORCID lookups'
        default: '3'
      backlog_depth:
        description: 'How many follow-up runs remain (0 = no re-trigger)'
        default: '5'

jobs:
  pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 330

    env:
      INPUT_STAGE: ${{ github.event.inputs.stage || inputs.stage || 'all' }}
      INPUT_LIMIT: ${{ github.event.inputs.limit || inputs.limit || '0' }}
      INPUT_EMAIL_LIMIT: ${{ github.event.inputs.email_limit || inputs.email_limit || '0' }}
      INPUT_ENRICH_WORKERS: ${{ github.event.inputs.enrich_workers || inputs.enrich_workers || '6' }}
      INPUT_ORCID_WORKERS: ${{ github.event.inputs.orcid_workers || inputs.orcid_workers || '3' }}
      INPUT_BACKLOG_DEPTH: ${{ github.event.inputs.backlog_depth || inputs.backlog_depth || '5' }}
      PIPELINE_ENV: prod
      DDB_BILLING_MODE: PAY_PER_REQUEST
      DDB_TABLE_PREPRINTS: prod_preprints
      DDB_TABLE_REFERENCES: prod_preprint_references
      DDB_TABLE_TEI: prod_preprint_tei
      DDB_TABLE_SYNCSTATE: prod_sync_state
      DDB_TABLE_EXCLUDED_PREPRINTS: prod_excluded_preprints
      DDB_TABLE_API_CACHE: prod_api_cache
      DDB_TABLE_TRIAL_AUTHOR_NODES: prod_trial_author_nodes
      DDB_TABLE_TRIAL_AUTHOR_TOKENS: prod_trial_author_tokens
      DDB_TABLE_TRIAL_CLUSTERS: prod_trial_clusters
      DDB_TABLE_TRIAL_ASSIGNMENTS: prod_trial_preprint_assignments
      DDB_TABLE_EMAIL_SUPPRESSION: prod_email_suppression
      PDF_DEST_ROOT: /tmp/preprints
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      OPENALEX_EMAIL: ${{ secrets.OPENALEX_EMAIL }}
      OSF_API_TOKEN: ${{ secrets.OSF_API_TOKEN }}
      ORCID_CLIENT_ID: ${{ secrets.ORCID_CLIENT_ID }}
      ORCID_CLIENT_SECRET: ${{ secrets.ORCID_CLIENT_SECRET }}
      CROSSREF_MAILTO: ${{ secrets.CROSSREF_MAILTO }}
      OPENALEX_MAILTO: ${{ secrets.OPENALEX_MAILTO }}
      TEI_S3_BUCKET: flora-preprint-tei-cache
      GMAIL_SENDER_ADDRESS: flora@replications.forrt.org
      GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
      PIPELINE_NOTIFY_EMAIL: ${{ secrets.PIPELINE_NOTIFY_EMAIL }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Python dependencies
        run: pip install -r requirements.txt

      - name: Run pipeline
        id: run
        run: |
          STAGE="$INPUT_STAGE"
          LIMIT="$INPUT_LIMIT"
          ENRICH_WORKERS="$INPUT_ENRICH_WORKERS"
          ORCID_WORKERS="$INPUT_ORCID_WORKERS"

          # Convert limit 0 to a large number (effectively unlimited)
          if [ "$LIMIT" = "0" ]; then
            LIMIT="100000"
          fi

          EMAIL_LIMIT="$INPUT_EMAIL_LIMIT"

          EMAIL_ARGS=""
          if [ "$EMAIL_LIMIT" != "0" ]; then
            EMAIL_ARGS="--include-email --email-limit $EMAIL_LIMIT"
          fi

          if [ "$STAGE" = "all" ]; then
            python -m osf_sync.pipeline run-post-grobid \
              --extract-limit "$LIMIT" \
              --enrich-limit "$LIMIT" \
              --enrich-workers "$ENRICH_WORKERS" \
              --orcid-workers "$ORCID_WORKERS" \
              --max-seconds-per-stage 3600 \
              $EMAIL_ARGS \
              | tee /tmp/pipeline_output.json
          else
            EXTRA_ARGS=""
            STAGE_LIMIT="$LIMIT"
            if [ "$STAGE" = "email" ]; then
              STAGE_LIMIT="$EMAIL_LIMIT"
              # Spread emails: 45 min per 10 messages = 270s per message
              SPREAD=$(( STAGE_LIMIT * 270 ))
              EXTRA_ARGS="--spread-seconds $SPREAD"
            fi
            python -m osf_sync.pipeline run \
              --stage "$STAGE" \
              --limit "$STAGE_LIMIT" \
              --enrich-workers "$ENRICH_WORKERS" \
              --orcid-workers "$ORCID_WORKERS" \
              --max-seconds 18000 \
              $EXTRA_ARGS \
              | tee /tmp/pipeline_output.json
          fi

          # Re-trigger only on explicit backlog signals.
          if python -c "
          import json, sys
          try:
              data = json.load(open('/tmp/pipeline_output.json'))
              stages = data.get('stages')
              if isinstance(stages, dict):
                  if any(bool(s.get('stopped_due_to_time')) for s in stages.values() if isinstance(s, dict)):
                      sys.exit(0)
                  if any(bool(s.get('limit_reached')) for s in stages.values() if isinstance(s, dict)):
                      sys.exit(0)
              elif bool(data.get('stopped_due_to_time')) or bool(data.get('limit_reached')):
                  sys.exit(0)
          except Exception:
              pass
          sys.exit(1)
          "; then
            echo "has_backlog=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_backlog=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Re-trigger if backlog remains
        if: steps.run.outputs.has_backlog == 'true' && env.INPUT_BACKLOG_DEPTH != '0'
        env:
          GH_TOKEN: ${{ secrets.WORKFLOW_DISPATCH_TOKEN }}
        run: |
          if [ -z "$GH_TOKEN" ]; then
            echo "::warning::WORKFLOW_DISPATCH_TOKEN secret not set - cannot re-trigger. Set a PAT with 'actions:write' scope."
            exit 0
          fi
          CURRENT_DEPTH="$INPUT_BACKLOG_DEPTH"
          NEXT_DEPTH=$((CURRENT_DEPTH - 1))
          if [ "$NEXT_DEPTH" -gt 0 ]; then
            echo "Backlog detected, re-triggering with depth=$NEXT_DEPTH"
            gh workflow run pipeline.yml \
              --ref "${{ github.ref_name }}" \
              -f stage="$INPUT_STAGE" \
              -f limit="$INPUT_LIMIT" \
              -f email_limit="$INPUT_EMAIL_LIMIT" \
              -f enrich_workers="$INPUT_ENRICH_WORKERS" \
              -f orcid_workers="$INPUT_ORCID_WORKERS" \
              -f backlog_depth="$NEXT_DEPTH"
          else
            echo "Backlog depth exhausted, not re-triggering"
          fi
