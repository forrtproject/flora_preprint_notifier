<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Awareness of gaze behaviour is limited: Insights from a novel tracking paradigm</title>
				<funder ref="#_tE78hfS">
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Toronto Mississauga</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Avery</forename><forename type="middle">Hannah</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Mississauga</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Mississauga</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Mississauga</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Kosovicheva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Mississauga</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Mississauga</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Awareness of gaze behaviour is limited: Insights from a novel tracking paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD33E36672DB210821F4604E17CFEC7B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-03T14:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>awareness</term>
					<term>eye movements</term>
					<term>gaze behaviour</term>
					<term>individual differences</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When asked where they have previously looked, people rarely report their visual behaviour correctly. However, previous tasks have probed participants' awareness of their gaze behaviour through explicit measures, like recognition, and it remains unclear whether poor awareness is also seen in implicit measures. We investigated this with a novel tracking paradigm in which participants first completed a visual search task while their eye movements were recorded. Next, participants completed a tracking task where they followed a moving red dot on the screen with their eyes. In Experiment 1, the dot replayed either their own previously recorded gaze position or that of another participant. We measured tracking performance by crosscorrelating the previously recorded gaze position with the tracked positions. Participants were not significantly faster or more accurate in tracking their own eye movements compared to another participant's. Experiment 2 tested this with a more extreme manipulation, in which participants tracked either unaltered or temporally reversed sequences of their own eye movements, which resulted in higher accuracy in the forward compared to the reversed condition. Finally, Experiment 3 examined whether performance would decrease when participants tracked a participant with very dissimilar gaze behaviour from their own. Tracking performance was similar when tracking their own scanpath compared to a dissimilar participant, but participants' tracking latencies were shorter when tracking a participant with consistent scanning behaviour. Overall, our results suggest that though awareness of one's eye movements is generally poor, extreme manipulations, like temporally reversing one's eye movements, can influence tracking performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Eye movements are an essential part of how we gather information from the world. Every day, when we look for our house keys or try to spot a friend in the crowd, we make directed eye movements towards objects of interest across the visual field. Previous research has shown that the planning and programming of eye movements is reliant on a combination of top-down influences like strategy <ref type="bibr" target="#b3">(Boot et al., 2009;</ref><ref type="bibr" target="#b7">Clarke et al., n.d.)</ref> and bottom-up factors like saliency <ref type="bibr" target="#b14">(Itti &amp; Koch, 2000;</ref><ref type="bibr" target="#b26">Theeuwes et al., 1998</ref><ref type="bibr" target="#b27">Theeuwes et al., , 1999))</ref>. Moreover, these factors are often highly idiosyncratic and are a product of the limitations of the visual system as well as cognitive variables, which translates into a vast array of individual differences in gaze behaviour across different tasks <ref type="bibr" target="#b1">(Andrews &amp; Coppola, 1999;</ref><ref type="bibr" target="#b7">Clarke et al., n.d.;</ref><ref type="bibr" target="#b13">Henderson &amp; Luke, 2014;</ref><ref type="bibr" target="#b23">Rayner et al., 2007;</ref><ref type="bibr" target="#b30">Veríssimo et al., 2021)</ref>.</p><p>In most everyday tasks, it may not be necessary for us to know where we last looked, or to maintain a robust and lasting representation of this information. This makes intuitive sense, given that we make two to four saccades per second, each separated by about 200-300 ms <ref type="bibr" target="#b5">(Carpenter, 1988)</ref>. In other words, storing detailed information of our own gaze behaviour is computationally costly for the brain and may be unnecessary. However, there are cases where it may be beneficial to have some knowledge of our own gaze behaviour. For example, when we search for items, knowledge of where we last looked may improve efficiency and prevent revisitations to previously examined locations <ref type="bibr" target="#b15">(Klein, 1988;</ref><ref type="bibr" target="#b22">Posner &amp; Cohen, 1984)</ref>. This may also be crucial to accurately gauge when to terminate search, especially when the target is absent <ref type="bibr" target="#b17">(Kok et al., 2017;</ref><ref type="bibr" target="#b31">Võ et al., 2016)</ref>. Put more simply, how do we know that we have "looked everywhere"? Lastly, with increased interest in eye-tracking as a way to train gaze behaviour in real-world tasks, like radiology or airport baggage screening (e.g., <ref type="bibr" target="#b2">Ashraf et al., 2018;</ref><ref type="bibr" target="#b24">Sadasivan et al., 2005;</ref><ref type="bibr" target="#b28">van Gog &amp; Jarodzka, 2013)</ref>, it would be useful to know if we are able to monitor our gaze behaviour for learning and self-regulation, since monitoring one's own behaviour has been shown to be important for learning in non-visual domains <ref type="bibr" target="#b12">(Greene &amp; Azevedo, 2010;</ref><ref type="bibr" target="#b19">Nelson, 1990)</ref>.</p><p>Several studies have examined the extent of our awareness of our gaze behaviour, finding that we generally have somewhat limited knowledge of where we have previously looked. <ref type="bibr" target="#b10">Foulsham and Kingstone (2013)</ref> were the first to explicitly ask if people could identify their own fixation pattern compared to one of the following foils: random fixations, fixations from another image, and fixations from another person. Performance was the lowest when participants were tasked with distinguishing their own fixations from another person's, but remained above chance levels in all conditions. In a separate study, when asked to replicate their previous fixations using mouse clicks, participants missed fixations and made false reports <ref type="bibr" target="#b18">(Marti et al., 2015)</ref>, though for the most part, they were able to report their gaze behaviour correctly.</p><p>However, there are a number of challenges in probing participants' knowledge of their own gaze behaviour. For example, participants may develop particular gaze strategies if they know that they will be asked about where they've looked, or rely on other sources of information. For example, participants may infer that they made more fixations when the trial was longer <ref type="bibr" target="#b18">(Marti et al., 2015)</ref>. Participants might also remember the objects they fixated on <ref type="bibr" target="#b10">(Foulsham &amp; Kingstone, 2013)</ref> or the target object being searched for <ref type="bibr" target="#b8">(Clarke et al., 2017)</ref>, but not necessarily where they've looked. Previous studies controlling for these variables show reduced ability to report on one's own gaze behaviour. For example, <ref type="bibr" target="#b31">Võ et al. (2016)</ref> also used mouse clicks, but only probed one-fourth of the total trials, to prevent people from adopting a strategy to boost their performance. They found that people were no better at distinguishing between their own fixations from another person's. In another study, <ref type="bibr" target="#b8">Clarke et al. (2017)</ref>, separately analyzed participants' memory for objects within a scene compared to memory for objects that had been fixated. Participants had difficulty distinguishing objects they had fixated from objects they had not. To add to this, other research has shown that even when using a gaze-contingent display, which showed participants their own eye movements in real-time, participants' ability to identify their own eye movements remained incredibly poor <ref type="bibr" target="#b17">(Kok et al., 2017)</ref>. Together, these studies report that our awareness of our own gaze behaviour is very limited.</p><p>These studies also highlight the importance of controlling for extraneous variables, including strategy or other sources of information. Explicit probes, like forced choice or free report tasks, may not fully capture participants' knowledge of their own gaze behaviour.</p><p>Moreover, since the majority of these are more likely to test memory, rather than awareness, they may be prone to serial position effects, limited working memory capacity, and poor metacognition.</p><p>Another possibility is that we retain some knowledge of our typical gaze behaviour, but it is not accessible through explicit recall or recognition. To investigate whether this is the case, we designed a novel tracking paradigm as an indirect probe of whether participants have some information about their typical gaze behaviour. In this paradigm, we first recorded participants' eye movements while they completed a visual search task. Next, participants reproduced their previous eye movements that were played back to them by following a dot that moved around the screen with their eyes. The underlying logic is that, if participants have some knowledge of where they tend to look, whether it is accessible to them or not, they should be faster and more accurate in reproducing that same behaviour compared to another individual's sequence or an altered scanpath. This task design was partly based on previous work showing that dynamic displays boost recognition performance <ref type="bibr" target="#b29">(Van Wermeskerken et al., 2018)</ref>. In this previous study, participants were asked to discriminate their own eye movements from someone else's in a forced choice task.</p><p>Recognition performance was better in this task when viewing dynamic replays compared to static images, likely due to the presence of both spatial and temporal information. Similarly, our dynamic displays preserved the spatial and temporal information in the original gaze recording.</p><p>In addition to this, by using an implicit measure, we did not probe participants' memory for where they looked on a particular trial. Instead, the measure is intended to capture whether participants have some information about their typical gaze behaviour within a search array. In addition, we used a simple visual search display (a search for T shapes among distractors) to minimize the impact of scene content on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>In Experiment 1, we tested our novel tracking task, which involved dynamic playback of previously recorded eye movements. As shown in Figure <ref type="figure">1A</ref>, participants completed two tasks: a search task and a tracking task. In the search task, on each trial, participants searched for a perfect "T" shape among near-"T" shapes and "L"-shaped distractors (Figure <ref type="figure">1B</ref>). In the tracking task, accuracy and latency in tracking performance were measured by cross-correlating the previously recorded gaze position with the tracked gaze position (Figure <ref type="figure" target="#fig_0">2</ref>). To identify whether people are able to track their own eye movements better than those of another person, we compared tracking accuracy and latency for previously recorded eye movements from the same participant versus the previous participant. In addition, we played the gaze positions back either on the same background that they were recorded from, or on a blank background, to test the prediction that tracking performance would improve when the set of possible gaze locations is constrained by the visual information in the search stimulus array (i.e., display). We compared tracking accuracy in a 2×2 design (Figure <ref type="figure">1C</ref>): replaying eye movements that were the participant's own versus another observer's (Self versus Other), and superimposing the replayed position on the original stimulus array or not <ref type="bibr">(Background versus No Background)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Thirty participants were recruited from the paid participant pool at the University of Toronto Mississauga. Following exclusions (see Exclusion Criteria), the final sample consisted of twenty-four participants (age: M = 20; 6 males, 16 females, and 2 unreported). Two participants were excluded due to a high proportion of lost eye-tracking samples (greater than 20%), while another was excluded due to low accuracy in the search task (not significantly better than 50%, based on a binomial test), and three did not finish the study and their data were discarded. All participants had normal or corrected-to-normal vision, defined as 20/25 or better in each eye using an ETDRS Near Vision chart. Participants provided informed consent prior to participating in the study, and all study procedures were approved by the Research Ethics Board at the University of Toronto. Two participants were compensated with course credit in accordance with their request, while the rest were compensated at $15 CAD/hour. On average, the task took between 75 to 90 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material and Stimuli</head><p>Apparatus. Participants maintained a stable viewing distance of 57 cm from a gammacorrected LG UltraGear LCD screen (2560 × 1440 pixels; 60 Hz refresh rate), such that the full display spanned approximately 55.3º × 32.8º. All visual stimuli were programmed using MATLAB and the Psychophysics Toolbox <ref type="bibr" target="#b4">(Brainard, 1997;</ref><ref type="bibr" target="#b16">Kleiner et al., 2007;</ref><ref type="bibr" target="#b20">Pelli, 1997)</ref>.</p><p>The experiment was run on an Apple Mac Pro running MacOS 10.14.6 and a NVidia GTX 680 4GB graphics card. Throughout the experiment, eye movements were recorded using an Eyelink 1000 Tower Mount tracker, which sampled at 1000 Hz. All eye movements were recorded from the right eye. A chin and head rest were used to minimize movements during the experiment.</p><p>Calibration was performed prior to the experiment and at the half-way point. Drift correction was performed every 40 trials, in which participants were required to fixate on a central dot prior to resuming the task and the eye-tracking recording. Additional calibration was performed when needed.</p><p>Search stimulus array. Each stimulus array (Figure <ref type="figure">1B</ref>) consisted of 25 varying shapes assigned to randomly selected positions in an invisible 8-row × 11-column screen-centred grid that spanned 15º × 20.6º on a gray background (96.1 cd/m 2 ). On each trial, random horizontal and vertical spatial jitters were added to the center positions of each of the 8 × 11 grid positions, ranging from -0.59º to +0.59º to prevent overlap in the stimuli. The shapes all consisted of one horizontal and one vertical line segment, joined at right angles to form a continuum of shapes ranging from a perfect T to an L. The line segments were 0.23º thick with each bar of the 'T' or 'L'-variant shapes set to 1.29º in length. Search targets were perfect "T" shapes, such that the vertical bar perfectly bisected the horizontal bar. Distractors were non-T stimuli that randomly varied in the offset of their horizontal and vertical bars. To clearly differentiate the non-T distractors from the T-shapes, the vertical segment was offset from the midline of the horizontal segment by at least 0.12º, and up to a maximum of 0.52º (making a perfect "L" shape). Likewise, the stimuli, including the target T, randomly varied in their orientation (selected from 0°, 90°, 180°, or 270°) and greyscale intensity value, which ranged from 38.4 to 57.6 cd/m 2 (different shades of dark grey). Each stimulus array was randomly generated, such that a new array was presented on each trial and for each participant.</p><p>Tracking conditions. In the second half of the experiment, participants completed a tracking task (Figure <ref type="figure">1C</ref>) in which they followed a moving red dot on the screen (0.9º diameter) with their eyes. On each trial, the dot's position matched either their own previously recorded gaze position or that of the previous participant (Self versus Other conditions, respectively).</p><p>Gaze positions were replayed from a random subset of 50% of the trials corresponding to these conditions, and in a random order, such that no sequence was replayed more than once. Positions were downsampled from the original 1000 Hz recording to the display frame rate (60 Hz) to match the original speed of the recorded eye movements. Furthermore, replayed eye movements were either superimposed on the same stimulus array displayed during the search task that they were originally recorded from, or on a plain grey background (Background versus No Background).</p><p>In both conditions, the dot was visible for only 50% of the time and randomly disappeared for brief segments of time. Pilot experiments showed near-ceiling tracking performance across all conditions when the dot was visible for the full duration, and this was mitigated by the visibility manipulation. As shown in Figure <ref type="figure">1D</ref>, the visibility manipulation was introduced by splitting the gaze position time series into segments that were 20 frames (333 ms) long, except for the last segment, which spanned any remaining frames. A random 50% of the segments were selected to have the dot visible, while the other 50% were selected to have the dot invisible. The result is that participants saw a red dot moving overlaid on the display elements (stimulus array present) or on a grey background (stimulus array absent) and would unpredictably disappear and reappear for brief periods of time. This duration of each segment was chosen to capture the length of approximately one fixation. Participants completed 20 practice trials before another 160 experimental trials (Figure <ref type="figure">1A</ref>). In total, the 2×2 experimental design consisted of 40 trials per unique condition of the tracking task (e.g., 40 Self-Background Present trials; 40 Other-Background Present trials, etc.), with conditions randomly interleaved across trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Design and procedure for Experiments 1 and 2. (A) Schematic of trial order.</head><p>Participants completed 20 practice trials followed by 160 trials of the experiment proper for both the search and tracking tasks, with an option for breaks after every 40 trials. In between the two tasks, a longer break occurred, in which we processed the eye-tracking data and prepared it for the tracking task. (B) Trial sequence for search task. Participants were presented with a fixation dot for 500 ms prior to the search stimulus array. Search trials terminated upon key response (left arrow for "absent" and right arrow for "present"), followed by a feedback screen for 500 ms. (C) Conditions for the tracking task, Experiment 1. Participants were either presented with replayed eye movements from themselves (Self) or from the previous participant (Other), superimposed on top of the original stimulus array (Background) or on a grey background (No Background). Conditions were randomly interleaved. (D) Visibility manipulation. To reduce ceiling effects on tracking performance, the dot was visible for 50% of the time, which was done by dividing the trial length into segments (333 ms each), and randomly selecting 50% of these segments to have the dot visible. (E) Spatial arrangements for Experiments 2 and 3. Item locations in these experiments were selected from a set of ten possible spatial arrangement (layout). Each iteration of the same spatial arrangement was different, with a different target location (if present), orientation, and color intensity. In short, only the spatial arrangement remained the same across all iterations. The red circles in the third panel emphasize that across all iterations, only the locations of these items remained constant. (F) Conditions for tracking task, Experiment 2. Participants were either presented with their own, unaltered eye movements (Forward) or a temporally reversed sequence (Reverse), superimposed on top of the original stimulus array (Background) or on a grey background (No Background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Search task. As shown in Figure <ref type="figure">1B</ref>, in the first half of the experiment, participants completed a visual search task consisting of L-shapes and T-shapes, where they were instructed to identify the presence of a perfect T among 25 other distractors. Trials terminated with the participant's response via a key press (left arrow key for 'absent' and right arrow key for 'present'). In total, participants completed 20 practice and 160 experimental search trials, with half of the trials containing a target (50% target present). Participants were given the opportunity to take a break every 40 trials. Each trial sequence started with a fixation dot (a small black dot in the center of the screen, 0.46º diameter) presented for 500 ms, followed by the visual search display that terminated upon participant response. Feedback regarding accuracy was then shown for 500 ms (the text "Correct" vs "Incorrect" at the center of the display, see Figure <ref type="figure">1B</ref>), with a red square outlining the perfect T when participants responded 'absent' on target-present trials. Lastly, participants were not informed of the second task to prevent deliberate gaze strategies that would allow them to recall their previous eye movements. Once the search task was completed, eye-tracking data were processed and cleaned in between the two experimental blocks. Eye blinks and other missing samples were replaced by interpolated data (using the nearest-neighbour method), for the duration of the eye blink as well as the 70 ms before and after the blink. This was done to maintain experimental control over the visibility of the dot during the tracking task (i.e., such that it would be visible for 50% of the time and not more).</p><p>Tracking task. On continuing to the second experimental block, participants were presented with previously recorded gaze positions represented as a red dot (Figure <ref type="figure">1C</ref>). On each trial, participants first saw a fixation dot (identical to the one present during the search task) presented for 500 ms, followed by the red dot, which represented either their own previously recorded gaze positions or those of the previous participant (N-1, see Exclusion Criteria for how we replaced participants and ensured continual staggering of the data presented). Replayed recordings were matched onto the original stimulus array that the gaze positions were recorded from during Background-Present trials, while in Background-Absent trials, participants only saw the red dot on a plain grey background. This was to determine whether having a stimulus array present would facilitate tracking performance. On each trial in the tracking task, participants were then instructed to follow the red dot as accurately as possible. If invisible, they were told to move their eyes to where they believed the dot would appear next, until the red dot appeared again. At the end of each trial, participants manually initiated the next trial by pressing the spacebar. Importantly, participants were not instructed to search for any target, but were simply told to follow the dot with their eyes. Participants were not informed of the condition before each trial, nor were they informed of the different unique conditions in the tracking task. As before, participants had the option for breaks after every 40 trials, with a drift correction prior to resuming the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Exclusion Criteria. Data were assessed for overall quality based on the following preregistered criteria: (1) Search performance. Participant data were excluded and replaced if their overall search accuracy was not significantly better than chance (50% accuracy) based on a binomial test, or if their median reaction times were excessively fast (&lt; 3.5 s, based on pilot data; see Participants). As these measurements only apply to the search task, these exclusions were based on the first experimental block only. Given our Self versus Other manipulation, we ensured that replacement participants viewed the gaze paths from the participant prior to the excluded participant in the 'Other' condition. For example, if participant N was following X, but N was excluded, then the replacement Y participant will follow X's eye movements as well. ( <ref type="formula">2</ref>)</p><p>Eye tracking quality. Participants with 20% or more missing or unusable eye-tracking samples during the first experimental block were excluded. Lost or unusable eye tracking are gaze positions undetected or reported outside the bounds of the tracking region (a 19.7º × 25.1º area centred on the search display).</p><p>Tracking measures. Prior to analysis, eye-tracking data were processed and cleaned using the same method for processing the gaze data for the tracking task (i.e., removing missing samples and interpolating them using the nearest-neighbour method). As shown in Figure <ref type="figure" target="#fig_0">2</ref>, tracking accuracy and latency were calculated by cross-correlating the original gaze position in the search task with the corresponding gaze positions in the tracking task. For the search scanpath, we used all samples in the original recording, without any breaks or gaps. In other words, these included gaze samples from the blank segments, not shown to the participant in the tracking task. To calculate the cross-correlograms (CCGs), we removed the first 500 ms of the trial recording to account for participants orienting to the dot's position from the fixation dot.</p><p>Moreover, we only analyzed up to 4000 ms of each trial to ensure the same length of data across our analysis. As participants varied in their reaction times, and therefore the trial duration, this was done to match the number of samples used for calculating the CCG between the Self and Other conditions (though we note that we found similar results when using all samples in each trial; see Supplemental Figure <ref type="figure">1</ref>). Trials that were shorter than 4000 ms were excluded from the analysis. From here, separate cross-correlations were calculated from the replayed x-and y-gaze positions (horizontal and vertical positions, respectively) collected during the first experimental block (visual search task) and the recorded x-and y-gaze positions from the second experimental block (tracking task).</p><p>Tracking accuracy was calculated from the resulting x-and y-peak correlation values from each trial. These values were Fisher Z-transformed prior to averaging across trials, and then averaged across x-and y-values for an overall tracking accuracy measure. This provided a measure of tracking accuracy, accounting for lags. Similarly, we extracted the delay corresponding to the peak in the cross-correlation as the tracking latency measure. This was first calculated from each trial, and then averaged across all trials within a condition, and then averaged across the x-and y-coordinates. Prior to averaging either the peak or latency values, we removed outliers using the interquartile range method (IQR), such that individual trials that were more than 1.5 interquartile ranges above the upper quartile or below the lower quartile were removed. This resulted in the removal of 4.35% of peak or latency values on average across participants from all three experiments. Both tracking accuracy and latency were then compared across the four conditions (Background-Self, Background-Other, No Background-Self, No Background-Other) using a 2×2 repeated-measures ANOVA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Tracking accuracy. Following our analysis described above, we calculated the average tracking accuracy for each condition (Figure <ref type="figure" target="#fig_1">3A</ref>). A 2 (source of eye movements: Self versus Other) × 2 (stimulus array: Background versus No Background) repeated-measures ANOVA revealed no significant main effect of eye movement source, F(1,23) = 0.03, p = .875. Participants' tracking accuracy was no different when the dot replayed eye movements from themselves (Self; M = 0.94, SD = 0.10) compared to another participant's eye movements (Other; M = 0.94, SD = 0.10). Likewise, the presence or absence of the stimulus array did not affect tracking accuracy, F(1,23) = 0.5121, p = .481, (M = 0.95, SD= 0.11 versus M = 0.94, SD = 0.10).</p><p>There was also no significant interaction, F(1,23) = 0.01, p = .916. On average, participants were not significantly better at tracking their own eye movements compared to another's, nor did the presence of the original stimulus array influence tracking accuracy.</p><p>Tracking latency. A separate 2×2 repeated-measures ANOVA revealed a significant main effect of stimulus array presence (F(1,23) = 44.25, p &lt; .001, η 2 p = 0.658) on tracking latency. As shown in Figure <ref type="figure" target="#fig_1">3B</ref>, on average, participants had a lower latency (i.e., faster tracking), when the original stimulus array was present (M = 207.62 ms, SD = 43.68 ms) compared to when it was absent (M = 241.83 ms, SD = 35.19 ms). However, no main effect of eye movement source was found (F(1,23) = 0.33, p = .573) meaning that, on average, people did not have shorter latencies when the gaze positions presented came from themselves (Self; M = 221.95 ms SD = 41.95 ms) or from another participant (Other; M = 227.50 ms, SD = 44.37 ms).</p><p>There was also no significant interaction, F(1,23) = 0.40, p = .533. In sum, these results suggest a latency benefit when the original stimulus array was present, regardless of the source of the eye movements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Experiment 1 showed no difference in tracking accuracy or latency between the Self and Other conditions, which may be consistent with several interpretations. Firstly, the lack of a difference may suggest that the idiosyncrasies in gaze behaviour are too small to detect with this method. For example, this could result from inconsistent gaze patterns. When people employ random eye movement sequences across randomly generated visual search arrays, a lack of a consistent difference between individuals in scanning behaviour would produce comparable tracking performance for their own eye movements versus others'. Similarly, these results may also stem from the staggered nature of the Other condition. To elaborate, the eye movements for the Other condition always came from the person before the current participant. This method may pair people with similar gaze behaviour by random chance, which could make it unlikely or difficult to produce a difference in tracking accuracy between their own gaze patterns and those of another participant. However, given that each trial was a unique search array with a distinct target location and set of distractors, we are limited in our ability to answer this question. Finally, despite the limitations mentioned above, our results may be a true product of poor knowledge of one's own gaze behaviour, which would be consistent with previous literature <ref type="bibr" target="#b8">(Clarke et al., 2017;</ref><ref type="bibr" target="#b17">Kok et al., 2017;</ref><ref type="bibr" target="#b18">Marti et al., 2015;</ref><ref type="bibr" target="#b31">Võ et al., 2016)</ref>. The results of Experiment 1 alone are not enough to establish whether or not participants retain information about their own typical gaze behaviour.</p><p>Unsurprisingly, Experiment 1 showed a latency benefit for the Background Present compared to the Background Absent condition. Participants' tracking latency was lower when the original stimulus array was present. Having this stimulus array of 25 items visible during the replay phase of the experiment constrained the set of possible locations where the dot would likely appear next. In contrast, when the stimulus array was absent, the number of possible locations is much higher. By reducing the number of possible saccade locations, we also reduce the computational demand on saccadic planning and programming <ref type="bibr" target="#b0">(Abrams &amp; Jonides, 1988)</ref>, as evident by the latency difference of about 40 ms. Overall, Experiment 1 served as a proof of concept for our novel tracking task and its potential as an implicit probe for awareness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>Experiment 1 demonstrated shorter tracking latencies when the original stimulus array was present compared to when it was absent, but we observed no difference in tracking accuracy or latency between the Self and Other conditions. As we note above, one possibility is that the Self versus Other manipulation may have been too subtle. To address this uncertainty, we tested a more extreme tracking manipulation. In Experiment 2, participants completed the same task, except instead of viewing another participant's eye movements in the Other condition, we presented their own temporally reversed eye movements during the second task. In other words, we replaced the Other condition with a Reverse scanpath condition (Figure <ref type="figure">1F</ref>). This way, we can control for differences in variables like fixation duration, saccade amplitude, and velocity between the conditions. An exploratory analysis of the data from Experiment 1 (Supplemental Figure <ref type="figure" target="#fig_0">2</ref>) found individual differences in saccade amplitude to be a mediating factor for tracking accuracy (r(22) = -0.44, p = 0.03), with reduced tracking accuracy when the median saccade amplitude in the replayed data were large.</p><p>In addition, as each display in Experiment 1 was randomly generated, we did not have a way to directly assess consistency in gaze behaviour within each participant (i.e., whether participants tend to look in the same locations and in a similar order). We modified this in Experiment 2, in which each search display consisted of one of ten unique spatial arrangements (i.e., layouts), with targets and distractors assigned to random locations within each layout (Figure <ref type="figure">1E</ref>). This allowed us to assess consistency by comparing scanning behaviour for displays in which items were shown in the same spatial configuration. In an exploratory analysis, we tested whether participants who were more consistent in their scanning behaviour would be more affected by the tracking manipulation (i.e., Forward versus Reverse).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants.</head><p>Twenty-nine participants were recruited from the participant pool at the University of Toronto Mississauga. Following the same exclusions in Experiment 1 (see Exclusion Criteria), the final sample consisted of twenty-four participants (age: M = 20, 6 males, 17 females, and 1 unreported). Four participants were excluded due to a high proportion of lost eye-tracking samples (&gt; 20%), and one participant did not finish the study and their data were discarded. Like Experiment 1, all participants provided informed consent, as approved by the Research Ethics Board at the University of Toronto and were compensated at $15 CAD/hour (average study time: 75 to 90 minutes). Eligibility requirements are also similar to Experiment 1, with the addition that we excluded any participant from Experiment 1 to avoid overfamiliarity with the task and to preserve participant naivety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Stimuli.</head><p>Apart from the following changes, Experiment 2 followed an identical procedure to Experiment 1. Spatial arrangements. To investigate the consistency of gaze behaviour within individuals, ten spatial arrangements were randomly generated prior to data collection (Figure <ref type="figure">1E</ref>). This was done to control for the items' locations (i.e., the locations of the "slots" that items could be in within an invisible grid), but to vary what was placed at each location. Each spatial arrangement consisted of 25 allocated positions in an invisible 8 × 11 grid, with random horizontal and vertical spatial jitters added. Unlike in Experiment 1, where we presented 180 uniquely random stimulus arrays to each participant, such that no participant saw the exact same set of stimulus arrays, we presented the same ten spatial arrangements to all participants multiple times. Each iteration was randomly selected on each trial, which resulted in approximately 16-20 iterations per spatial arrangement since there were 180 trials in total, including practice trials. Importantly, only the item "slots" remained constant across each repetition. Each display had a different set of items (target and distractors) assigned to these positions, with varying orientations and greyscale intensity values. In this manner, on target-present trials (50% of the trials), the target was in a random location, and only the spatial arrangement of the stimuli was held constant. Aside from the spatial locations, each iteration (i.e., stimulus array) was random and unique from the other.</p><p>Tracking conditions. As in Experiment 1, participants completed a tracking task in which they followed a moving red dot on the screen with their eyes. However, instead of viewing either their own eye movements or that of the previous participant, participants in Experiment 2 only viewed their own. On each trial, the dot replayed the original, unaltered sequence of positions (Forward condition) or a temporally reversed one (Reverse condition), with the same 50% visibility manipulation used in Experiment 1 (see <ref type="bibr">Materials and Stimuli)</ref>. This allowed us to match individual gaze metrics (i.e., saccade amplitude, fixation duration) between these conditions and minimize noise due to between-subject differences. As such, excluded participants were no longer replaced in a staggered manner. Apart from this difference, the task proceeded identically to Experiment 1. As shown in Figure <ref type="figure">1F</ref>, we used a 2×2 design, with the factors of tracking condition (Forward versus Reverse) and stimulus array (Background versus No Background). As in Experiment 1, trials were randomly interleaved and participants were not informed of this manipulation, or that the tracking task would follow the search task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis.</head><p>Consistency analysis. In addition to our main analysis described in Experiment 1 (see Analysis), we employed a secondary analysis based on the ten spatial configurations in our experimental design. To extract a measure of participants' consistency in scanning the displays, gaze positions over time for each spatial configuration were correlated across all possible pairs of trials with that spatial configuration. This was done separately for the x-coordinates and y-coordinates, and then each set of correlation values underwent Fisher Z transformation prior to taking the average of both x-and y-gaze position correlations. This resulted in ten average correlation values, one for each layout. To estimate overall within-subject consistency, we took the average of all ten correlation values. Higher correlation values indicated higher consistency (i.e., participants reliably looked at the same locations in each iteration of the spatial configuration in a similar sequence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Tracking accuracy. Using the same analysis in Experiment 1, a 2 (condition: Forward versus Reverse) × 2 (stimulus array: Background versus No Background) repeated-measures ANOVA revealed a significant difference between the Forward and Reverse conditions on tracking accuracy, F(1,23) = 16.57, p &lt; .001, η 2 p = 0.42. As shown in Figure <ref type="figure" target="#fig_2">4A</ref>, participants were more accurate at tracking an unaltered sequence of their own eye movements (Forward; M = 0.97, SD= 0.10) compared to when the sequence was temporally reversed (Reverse; M = 0.92, SD = 0.09). Similarly, we observed a main effect of stimulus array presence on tracking accuracy, F(1,23) = 8.55, p = .008, η 2 p = 0.27, wherein the presence of the original stimulus array facilitated more accurate tracking (Background; M = 0.96, SD = 0.10) compared to when it was absent (No Background; M = 0.93, SD= 0.09). However, there was no interaction between these factors, F(1,23) = 0.18, p = .678.</p><p>Tracking latency. We found similar results to Experiment 1, as shown in Figure <ref type="figure" target="#fig_2">4B</ref>. On average, participants had lower tracking latencies when the background was present (Background; M = 181.92 ms, SD = 36.12 ms) versus when it was absent (No Background; M = 206.76 ms, SD = 38.35 ms), F(1,23) = 23.57, p &lt; .001, η 2 p = 0.50. However, the tracking manipulation (Forward versus Reverse) did not affect latency, F(1,23) = 1.53, p = .23, (M = 191.14, ms SD = 40.48 ms versus M = 197.55 ms, SD = 37.84 ms). There was also no significant interaction, F(1,23) = 0.18, p = .678. In sum, these results suggest that extreme manipulations (i.e., the Forward versus Reverse conditions) impacted accuracy, but not latency.</p><p>Relationship between consistency and tracking performance. In an exploratory analysis, we determined whether consistency in scanning behaviour modulated the difference in tracking performance between the Forward and Reverse conditions. To do this, we first calculated the within-subject consistency (see Experiment 2, Correlation Analysis). Higher positive Fisher Z-transformed correlation values indicate greater consistency, while values closer to zero indicate more random gaze behaviour. The correlation analysis revealed a range of individual differences in consistency (Figure <ref type="figure">5A</ref>); across participants, correlation values ranged from 0.02 to 0.54.</p><p>Similarly, with regards to tracking performance, some participants had a larger reduction in tracking performance in the Reverse condition compared to the Forward manipulation than others. We investigated whether consistent individuals would have a larger decrement in tracking performance with a temporally reversed compared to unaltered scan path. We represented this susceptibility as a Forward -Reverse difference in tracking accuracy and latency, then correlated these values with their corresponding within-subject consistency values using a Pearson correlation. We found that within-subject consistency was associated with this difference in tracking latency, r(22) = -.60, p = .002 (Figure <ref type="figure">5C</ref>) such that more consistent participants showed a larger decrement in tracking latency for Forward compared to Reverse scan paths (i.e., they were faster in the Forward compared to the Reverse condition). However, there was no relationship between consistency and the difference in accuracy between the Forward and Reverse conditions, r(22) = .17, p = .42 (Figure <ref type="figure">5B</ref>). In sum, our correlation analysis suggests that people who are more internally consistent in their scanning behavior are more affected by the temporal reversal of their eye movement sequence, as evident by the latency benefit (negative latency values on the x-axis, Figure <ref type="figure">5C</ref>), while those who are less consistent seemed to be less affected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Individual variability in within-subject consistency. (A)</head><p>Within-subject consistency for each participant in Experiment 2. Each bar represents the mean within-subject consistency, calculated from the average of all ten spatial arrangements (Fisher Z-transformed correlation value). (B) Correlation between Forward -Reverse difference in tracking accuracy (x-axis) for all participants with their within-subject consistency (y-axis). Negative x-axis values mean that the participant performed better during the Reverse conditions, while positive values mean that they performed better during the Forward conditions. Scatter points represent individual participants. (C) Correlation comparing the Forward -Reverse difference in tracking latency (x-axis) with their within-subject consistency (y-axis). Negative x-axis values correspond to shorter latencies (i.e., faster tracking) in the Forward compared to the Reverse condition, while positive values correspond to shorter latencies in the Reverse compared to Forward condition. Scatter points represent individual participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Experiment 2 demonstrated that temporally reversing the eye movement sequence produced decrements in tracking accuracy, but did not affect latency. This could be explained by the fact that this manipulation flipped the direction of the eye movement sequence. For example, participants may retain some information about the direction in which they tend to scan the display (e.g., clockwise) and rely on this information during the brief segments when the replayed dot disappears, which would drive down the tracking accuracy measure.</p><p>We also examined whether people with more consistent scanning behaviour would be more affected by the Forward versus Reverse manipulation. We operationalized this susceptibility to the temporal reversal as a Forward -Reverse difference in accuracy and latency.</p><p>We found a negative correlation between susceptibility (in our latency measure) and withinsubject consistency, such that people who were more consistent had greater latency decrements in the Forward compared to the Reverse conditions. However, we did not see a similar relationship with tracking accuracy (i.e., within-subject consistency was not associated with the effect of the manipulation on tracking accuracy). Regardless, we demonstrated that with a sufficiently extreme change in the path of the tracked dot, participants' tracking performance decreased. This could indicate that the null result we observed in Experiment 1 may a consequence of using a more subtle manipulation (Self versus Other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3</head><p>Experiment 2 demonstrated that a highly dissimilar eye movement sequence (i.e., temporal reversal) is more difficult for participants to track than their own gaze sequence. These results are consistent with the idea that we have some information about our own typical gaze behaviour. This addresses one of the possible interpretations we had for the results in Experiment 1, supporting the idea that the Self versus Other manipulation in Experiment 1 may not have been large enough to produce differences in tracking performance. For example, it is possible that we observed no difference in tracking performance in Experiment 1 because highly similar people were paired by random chance. What remains uncertain is whether tracking performance could, in principle, be sensitive to individual differences in gaze behaviour.</p><p>To resolve this question, in Experiment 3, we had three conditions. In one, participants tracked their own previously recorded eye movements. We also had two different "other" conditions: one that replayed eye movements from a participant who was very similar to the current participant, and another that replayed eye movements from a participant who was very dissimilar. The tracked positions for these two "other" conditions ("Other-Most Similar" and "Other-Least Similar", respectively) were selected from participants who had previously completed Experiment 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Thirty-two participants were recruited from the paid participant pool at the University of Toronto Mississauga. Eligibility requirements and inclusion criteria were held constant from Experiments 1 and 2, with the addition that we excluded any participant who had done either of the two previous experiments to preserve task naivety. Following exclusions (see Experiment 1, Exclusion Criteria), the final sample consisted of twenty-four participants (age: M = 20, 6 males, 18 females). Six participants were excluded due to a high proportion of lost eye-tracking samples (&gt; 20%), one was excluded due to low accuracy (not significantly above chance based on a binomial test), and another was excluded due to a low median reaction time (less than 3.5 seconds). Four participants were compensated with course credit in accordance with their request, while the rest were compensated at $15 CAD/hour. In general, participants completed the experiment in 75 to 90 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Stimuli</head><p>Apart from the following changes, Experiment 3 followed an identical procedure to Experiment 2, including the same set of ten spatial arrangements.</p><p>Tracking conditions. To isolate the effects of scanpath similarity within our original design, we measured participants' tracking performance for their own gaze positions ("Self" condition) and compared this with two different "other" tracking conditions: "Other-Most Similar" and "Other-Least Similar". These were taken from the previously recorded gaze data from participants who completed Experiment 2, since the same set of spatial arrangements were used in both experiments. As shown in Figure <ref type="figure">6B</ref>, after each participant completed the search task, but prior to starting the tracking task, we correlated the x-and y-gaze positions between the current participant (in Experiment 3) and each participant from the previous set of 24 participants (Experiment 2). Pairwise correlations between the current participant and the Experiment 2 participants were first calculated separately for each matched spatial arrangement (i.e., layout), and then averaged across layouts to produce an overall measure of similarity between the current participant and each Experiment 2 participant.</p><p>Next, these resulting correlation values were used to determine the participants from Experiment 2 to use for the Other-Most and Other-Least replay conditions in Experiment 3. To minimize the odds of always selecting the same participant for the "Other-Most" and "Other-Least" conditions, we randomly selected one out of four of the most correlated participants for the Other-Most condition, and one out of the four least correlated participants for the Other-Least condition. In addition to these two "other" conditions, we included the original Self condition, wherein we replayed the current participant's own eye movements. Concisely, in this version, we tested three conditions in the second half of the experiment: Self, Other-Most, and Other-Least.</p><p>Unlike Experiments 1 and 2, every trial showed a replay of the gaze positions superimposed on the original stimulus array.</p><p>As shown in Figure <ref type="figure">6A</ref>, in the first half of the experiment, participants completed 30 practice trials and 150 experimental trials of the search task. In the second half (Tracking task), participants completed 30 practice trials followed by 150 tracking trials, which consisted of 50 experimental trials per condition, which were randomly interleaved. As in the previous experiments, participants were not informed of the different tracking conditions. Figure <ref type="figure">6</ref>. Design and procedure for Experiment 3. (A) Schematic of trial order. Participants completed 30 practice trials followed by 150 trials of the experiment proper for both tasks, with an option for breaks after every 40 trials. Between the two tasks, a longer break occurred in which we processed the eye-tracking data and prepared it for the tracking task. (B) Conditions for the tracking task, Experiment 3. The determination of the two "Other" conditions was based on pairwise comparisons between the current Experiment 3 participant's gaze data and previous participants from Experiment 2. Each bar represents the calculated Fisher Z-transformed correlation value between the current participant and each Experiment 2 participant (e.g., S18 or Subject 18, was the most correlated for this current participant). The dark blue bars represent the four most correlated Experiment 2 participants, and the light blue bars represent the four least correlated. From each of these, one was randomly selected for the Other-Most Similar ("Other-Most") and Other-Least Similar ("Other-Least) conditions, respectively (e.g., S23 for Other-Most and S25 for Other-Least). In a random order, participants were presented with the dot replaying eye movements from themselves (Self), from someone who was very similar to them (Other-Most), or from someone who was very dissimilar to them (Other-Least). The original stimulus array was present in all trials for Experiment 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Tracking accuracy. We repeated the same analyses from Experiments 1 and 2. A oneway repeated-measures ANOVA found no main effect of eye movement source on tracking accuracy, F(2,46) = 0.82, p = .446. As shown in Figure <ref type="figure" target="#fig_3">7A</ref>, on average, participants were no more accurate at tracking the red dot, regardless of whether it replayed their own eye movements (Self; M = 0.95, SD= 0.12), someone with similar scanning behaviour (Other-Most; M = 0.97, SD= 0.11), or someone else with very dissimilar scanning behaviour (Other-Least; M = 0.96, SD = 0.10).</p><p>Tracking latency. With respect to tracking latency (Figure <ref type="figure" target="#fig_3">7B</ref>), a separate one-way repeated-measures ANOVA found a main effect of eye movement source, F(2,46) = 6.88, p = .002, η 2 p = 0.230. Post-hoc Tukey's tests further revealed that, on average, participants had a lower latency when tracking replayed Other-Most eye movements compared to when they were tracking their own eye movements (Self), t(23) = -3.07, pTukey = .014. Similarly, participants had shorter tracking latencies for the Other-Most condition compared to the Other-Least condition, t(23) = 3.83, pTukey = .002. However, no difference in latency was found between Self and Other-Least conditions, t(23) = 0.51, pTukey = .866.</p><p>In sum, surprisingly, participants had a shorter tracking latency when tracking the eye movements of the participant from Exp 2 most similar to themselves (Other-Most; M = 164.74 ms, SD= 35.60 ms), compared to either themselves (Self; M= 189.57 ms, SD= 32.70 ms) or the participant they were least similar to <ref type="bibr">M = 194.62 ms,</ref><ref type="bibr">SD = 43.96 ms)</ref>.</p><p>Relationship between consistency and tracking performance. In an exploratory analysis, we repeated the analyses from Experiment 2 (see Correlation Analysis and Results) and calculated a within-subject consistency measure for each participant. In Figure <ref type="figure">8A</ref>, we observed a large range of differences in internal consistency across participants, which led us to examine whether this was related to the estimates of similarity and dissimilarity when assigning conditions (i.e., Other-Most and Other-Least). One possibility is that, for a participant with relatively random and less consistent gaze behaviour, that they might only be highly correlated with another participant with a relatively high within-subject consistency measure. In other words, the participant selected for the "Other-Most" condition would not only be similar to them, but also have a high degree of internal consistency. We asked whether this would explain the counterintuitive results we found in Experiment 3, specifically, the observation that participants had shorter latencies for the Other-Most condition compared to the Self condition. We postulated that instead of gaze behaviour similarity, consistency in scanning behaviour modulated these results.</p><p>To assess this, we first calculated each participant's consistency in Experiment 3, following the same analyses outlined for Experiment 2 (Figure <ref type="figure">8A</ref>). Next, we calculated the between-subject consistency difference (Other-Most -Self difference in within-subject consistency). We then calculated the observed latency difference between the Self and Other-Most conditions in Experiment 3, and then correlated it with the corresponding between-subject consistency difference using a Pearson correlation (Figure <ref type="figure">8B</ref>). We found a negative correlation between the observed latency difference and the between-subject consistency difference, r(22) = -.59, p = .003. This would indicate that tracking a consistent scanpath, regardless of the condition, resulted in shorter latencies. More precisely, when the current participant (Self) was less consistent than the Other-Most subject, tracking latency was lower for the Other-Most condition; and vice versa. To summarize, our correlation analysis suggests that people who are more internally consistent may be easier for another participant to track, as evident by the shorter latencies (negative latency values on the x-axis, Figure <ref type="figure">8C</ref>), while those who are more random in their gaze behaviour may be harder for another participant to track. However, we did not observe this relationship for tracking accuracy, r(22) = 0.90, p = .68 (Figure <ref type="figure">8B</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8. Within-subject consistency modulates the tracking latency difference between</head><p>Other-Most and Self. (A) Within-subject consistency for each participant in Experiment 3. Each bar represents the mean within-subject consistency, calculated from the average of all ten spatial arrangements (Fisher Z-transformed correlation value). Correlation values ranged from -0.0012 to 0.61 across all participants. (B) Correlation between the difference in internal consistency between the Other-Most and Self conditions (y-axis), and the tracking accuracy difference between these two conditions (x-axis). Negative values on the x-axis indicate that the participant had higher tracking accuracy in the Self condition compared to the Other-Most Condition, while positive values correspond to higher accuracy in the Other-Most compared to the Self condition. On the y-axis, negative values mean that the current participant had higher internal consistency, while positive values indicate that the Other-Most participant was more consistent internally. Scatter points represent individual participants. (C) Correlation between the difference in internal consistency between the Other-Most and Self conditions (y-axis), and the tracking latency difference between these two conditions (x-axis), following the same conventions as panel B. Negative values on the x-axis mean that the participant had shorter latencies (faster tracking) in the Other-Most compared to the Self condition, while positive values correspond to shorter latencies in the Self compared to the Other-Most condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Experiment 3 showed no difference in tracking accuracy between the three conditions:</p><p>Self, Other-Most, and Other-Least. However, we did find a significant latency advantage for Other-Most trials compared to the other two conditions. Intuitively, we expected that Other-Most and Self would have similar accuracy or latency advantages, based on the results of Experiment 2, which suggested that participants' tracking performance might be worse for scanpaths that are very different from their own. Surprisingly, our results showed that participants had significantly shorter latencies in the Other-Most condition compared to the Self condition. As shown by our correlation analysis (Figure <ref type="figure">8B</ref>), one factor that may account for this difference between Other-Most and Self may be a difference in the within-subject consistency.</p><p>Together, this would suggest that the difference in latency may be an artifact of the selection process for each condition, which would produce differences in internal consistency as well as between-subject similarity between conditions. By choosing Experiment 2 participants for the "Other-Most condition" based on their correlation with the current participant, this process may have unintentionally also selected highly consistent participants for the "Other-Most" condition relative to the other two conditions. A post-hoc t-test revealed significantly higher consistency values for the Other-Most compared to the Self condition, further supporting this possibility, t(23) = 3.42, p = .002. Similarly, the lack of latency difference between the Self and Other-Least conditions may be due to the randomness of the Self and Other-Least participant's gaze behaviour. That is, the Other-Least participants could be weakly correlated with the current participant either because their gaze behaviour was relatively random, or because their behaviour was consistent, but different from the current participant. We note that internal consistency was somewhat higher for the Self condition compared to the Other-Least conditions t(23) = -2.31, p = .03, supporting the former interpretation. In sum, Experiment 3 revealed that internal consistency modulated tracking latency differences, which also aligns with the results we observed in Experiment 2 (Figure <ref type="figure">5C</ref>). As the design of Experiment 3 did not allow us to fully separate similarity from consistency, we cannot determine the extent to which eye movement similarity underlies differences in tracking accuracy between conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>In this study, we investigated whether participants retain some information about their typical gaze behaviour using a novel tracking paradigm as an indirect probe. To briefly summarize the results of Experiment 1, we investigated whether participants would have better tracking performance (i.e., higher accuracy or shorter latency) when tracking their own eye movements compared to those of another person. We found no differences in tracking performance when participants tracked their own eye movements compared to those of another participant (Self versus Other conditions, respectively). However, we found a latency benefit with the presence of the original stimulus array. In Experiment 2, we demonstrated that reversing the temporal sequence of eye movements was sufficient to producing a tracking accuracy decrement, such that participants were less accurate at tracking temporally reversed scan paths compared to unaltered ones (Reverse versus Forward, respectively). Simultaneously, we observed a persistent latency benefit as well as a new accuracy benefit with the presence of the original stimulus array, which was largely consistent with our results from Experiment 1. This prompted us to revisit the Self versus Other comparison in Experiment 3, in which participants tracked a dot that replayed gaze positions in one of three conditions: participants' own previously recorded eye movements (Self), those of a participant highly correlated with themselves (Other-Most), or those of a participant poorly correlated with themselves (Other-Least). In Experiment 3, while we continued to find no differences in tracking accuracy across conditions, a surprising latency advantage was revealed in the Other-Most conditions compared to Self and Other-Least. This result could reflect differences in internal consistency, rather than similarity per se.</p><p>Our results, specifically the Self versus Other comparisons in Experiments 1 and 3, seemed contrary to previous literature on stable, large individual differences in gaze behaviour within visual search tasks. Gaze behaviour differences between individuals can be attributed to an amalgamation of high-level influences, like strategy <ref type="bibr" target="#b3">(Boot et al., 2009;</ref><ref type="bibr">Clarke et al., n.d., 2022;</ref><ref type="bibr" target="#b11">Gilchrist &amp; Harvey, 2006)</ref>, and low-level factors, like individual visual sensitivity <ref type="bibr" target="#b30">(Veríssimo et al., 2021)</ref>. These different factors have been known to impact the temporal order, spatial distribution, and number the eye movements we make. One possible explanation for the lack of tracking performance differences between Self and Other is due to having relatively poor knowledge of their own eye movements, which would be consistent with previous literature <ref type="bibr" target="#b8">(Clarke et al., 2017;</ref><ref type="bibr" target="#b10">Foulsham &amp; Kingstone, 2013;</ref><ref type="bibr" target="#b17">Kok et al., 2017;</ref><ref type="bibr" target="#b18">Marti et al., 2015;</ref><ref type="bibr" target="#b29">Van Wermeskerken et al., 2018;</ref><ref type="bibr" target="#b31">Võ et al., 2016)</ref>. Another possibility is that the task might impact the degree to which individuals exhibit differences in gaze behaviour. Some tasks could drive down consistent and reliable gaze behaviour differences between individuals, due to the visual information in the environment or the nature of the task, which would prevent participants from tracking their own eye movements better or faster those of another participant. Our results in Experiment 2 would be consistent with the latter assertion, such that when we induced extreme changes to the replayed eye movements (i.e., Forward versus Reverse), we were able to induce tracking performance differences.</p><p>Nevertheless, the threshold of our limited awareness remains unclear. How extreme must these differences be for us to reliably detect them? Further work using different stimuli to induce larger idiosyncratic behaviours compared to a search for Ts among Ls, may be required.</p><p>Regardless, at least in the context of the task tested here, the observed tracking differences in our results suggest that our awareness of our own gaze behaviour is relatively limited when probed with an implicit task.</p><p>A further consideration is whether this method captures the extent to which participants retain information (either implicitly or explicitly) about their own gaze behavior. For example, one possibility is that participants could achieve a high level of tracking accuracy, not by tracking the dot, but instead by simply be reproducing their own previous gaze behaviour (as might be observed if the dot were absent for the duration of the entire trial). However, we note that the latencies we observed are consistent with latencies of reflexive saccades to a target (on the order of 200 ms), suggesting that participants are indeed tracking the position of the dot.</p><p>Another interpretation of the result we observed in Experiment 2 is that participants might track the dot more accurately in the Forward compared to the Reverse condition due to differences in saccadic precision for different saccade directions. In other words, participants might naturally adopt particular saccade directions (i.e., in the Forward condition) because those directions tend to produce lower errors to begin with. This interpretation is unlikely, because scanning the search display requires making saccades in multiple directions, and finer spatial precision for one direction would be counteracted by lower precision in another. A final possibility is that differences between the Forward and Reverse conditions may reflect differences in search habits rather than participants' knowledge their own gaze behaviour. For example, participants may revert back to their typical scanning behaviour in the Reverse condition whenever the dot disappears, which would reduce tracking accuracy. Nonetheless, this interpretation would be consistent with the idea that participants retain some information, albeit implicitly, about typical scanning behaviour.</p><p>Together, the results of these experiments also indicate that a number of factors may influence performance in this tracking task, making it challenging to use as a method for probing awareness. In particular, both Experiments 2 and 3 support the idea that tracking latency decreases when participants are required to track a consistent scan path. In addition, Experiment 3 showed that in the span of thirty minutes to an hour (average time for the second task), participants were able to extract information about another participant's consistent gaze behaviour which may be a product of statistical learning. Statistical learning is learning based on the extraction of environmental patterns, regularities, and distribution <ref type="bibr" target="#b21">(Perruchet &amp; Pacton, 2006;</ref><ref type="bibr" target="#b25">Sznabel et al., 2023)</ref>, and may explain why participants were able to reproduce and track the eye movements of other people with shorter latencies when those individuals were highly consistent <ref type="bibr">(Figure 5C,</ref><ref type="bibr">8C)</ref>. Having more consistent scanning behaviour produced reliable patterns for individuals to extract and learn implicitly. This type of learning would be relevant to the growing interest in using gaze behaviour and modelling to guide trainees in radiology, airport baggage screening, and other real-world search tasks.</p><p>A further area for future studies is to examine how scanning behaviour would interact with search performance and target detection. In our study, we showed that even though participants' knowledge of their own gaze behaviour was limited, people were able to learn others' gaze behaviours and track them with a shorter delay. Future work might examine the question of consistency in more detail, specifically in the context of search performance, focusing on whether having a consistent scanpath would be more helpful or harmful to overall search performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Schematic of analysis using cross-correlation. Gaze positions collected from the first experimental block (red dotted line, showing horizontal position over time, in inset panel, replayed as a red dot) were cross-correlated with the gaze positions recorded (solid black line in inset panel) during the tracking task to calculate our dependent variables. The graph shows a sample cross-correlogram (CCG), which illustrates the correlation between original and tracked gaze positions as a function of time lag. Positive lags on the x-axis correspond to the original recording being shifted later, while negative lags correspond to the original recording being shifted earlier. The height of the peak represents the tracking accuracy, and the location of the peak represents the temporal lag between the two time series. A lag at a positive value indicates that the two time series are most correlated when the original recording is shifted later.</figDesc><graphic coords="15,114.00,135.74,383.65,272.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Tracking performance in Experiment 1. (A) Average tracking accuracy in each condition, calculated as the peak of the cross-correlation (Fisher Z-transformed correlation value). (B) Average tracking latency in each condition, calculated as the lag corresponding to the peak accuracy (i.e., the point on the xaxis of the CCG corresponding to the peak). Symbols represent individual participants, and error bars represent  1 standard error of the mean (SEM). Asterisks represent results of the 2×2 repeated-measures ANOVA, ***p &lt; .001.</figDesc><graphic coords="17,123.85,72.00,364.30,260.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Tracking performance in Experiment 2. (A) Average tracking accuracy in each condition, calculated as the peak of the cross-correlation (Fisher Z-transformed correlation value). (B) Average tracking latency in each condition, calculated as the lag corresponding to the peak accuracy (i.e., the point on the xaxis of the CCG corresponding to the peak). Symbols represent individual participants, and error bars represent  1 standard error of the mean (SEM). Asterisks represent results of the 2×2 repeated-measures ANOVA, ***p &lt; .001; **p &lt; .01.</figDesc><graphic coords="24,101.50,182.39,408.95,292.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Tracking performance in Experiment 3. (A) Average tracking accuracy in each condition, calculated as the peak of the cross-correlation (Fisher Z-transformed correlation value). (B) Average tracking latency in each condition, calculated as the lag corresponding to the peak accuracy (i.e., the point on the x-axis of the CCG corresponding to the peak). Symbols represent individual participants, and error bars represent  1 standard error of the mean (SEM). Asterisks represent results of the one-way ANOVA, **p &lt; .01.</figDesc><graphic coords="33,72.00,190.39,401.67,276.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="47,72.00,269.53,467.49,263.35" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div><head>Funding</head><p>This work was supported by an undergraduate research grant from the <rs type="funder">University of Toronto Mississauga</rs> to AHC, and an <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> <rs type="grantName">Discovery Grant</rs> to <rs type="person">Anna Kosovicheva</rs> (<rs type="grantNumber">RGPIN-2022-03131</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tE78hfS">
					<idno type="grant-number">RGPIN-2022-03131</idno>
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of Data and Materials</head><p>All data and materials are available on OSF [<ref type="url" target="https://osf.io/gw4uf/">https://osf.io/gw4uf/</ref>]. All experiments were preregistered on OSF at [<ref type="url" target="https://osf.io/gw4uf/">https://osf.io/gw4uf/</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>Code for stimulus presentation and analysis is available on OSF at [<ref type="url" target="https://osf.io/gw4uf/">https://osf.io/gw4uf/</ref>].</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>The authors have no conflicts of interest to declare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Approval</head><p>This research was approved by the Research Ethics Board at the University of Toronto (#41533).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to participate</head><p>All participants provided written informed consent prior to participating in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>Participants consented to the publication of their deidentified data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>AHC and AK conceived and programmed the experiments. AHC collected the data. AHC and AK analyzed the data, interpreted the results, and wrote the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials 898</head><p>Supplemental Figure <ref type="figure">1</ref>. Tracking performance analyzed using the full length of trials equal to or greater than 4000ms, across all three experiements. The entire length of the trial was included, as opposed to truncating each trial from 500 ms to 4000 ms, as done in the main analysis. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Programming saccadic eye movements</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jonides</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.14.3.428</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.14.3.428" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="428" to="443" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Coppola</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0042-6989(99)00019-X</idno>
		<ptr target="https://doi.org/10.1016/S0042-6989(99)00019-X" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2947" to="2953" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eye-tracking technology in medical education: A systematic review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sodergren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Merali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mylonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<idno type="DOI">10.1080/0142159X.2017.1391373</idno>
		<ptr target="https://doi.org/10.1080/0142159X.2017.1391373" />
	</analytic>
	<monogr>
		<title level="j">Medical Teacher</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="69" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stable individual differences in search strategy?: The effect of task demands and motivational factors on scanning strategy in visual search</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Boot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Becic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Psychophysics Toolbox</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Movements of the eyes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H S</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page">593</biblScope>
		</imprint>
	</monogr>
	<note>2nd rev. &amp; enlarged ed</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Pion</forename><surname>Limited</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stable individual differences in strategies within, but not between, visual search tasks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D F</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Irons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Leber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><surname>(n.D</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">People Are Unable to Recognize or Report on Their Own Eye Movements</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D F</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2016.1231208</idno>
		<ptr target="https://doi.org/10.1080/17470218.2016.1231208" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2251" to="2270" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual search habits and the spatial structure of scenes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D F</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nowakowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-022-02506-2</idno>
		<ptr target="https://doi.org/10.3758/s13414-022-02506-2" />
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1874" to="1885" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Where have eye been? Observers can recognise their own fixations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Foulsham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
		<idno type="DOI">10.1068/p7562</idno>
		<ptr target="https://doi.org/10.1068/p7562" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1085" to="1089" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evidence for a systematic component within scan paths in visual search</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gilchrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harvey</surname></persName>
		</author>
		<idno type="DOI">10.1080/13506280500193719</idno>
		<ptr target="https://doi.org/10.1080/13506280500193719" />
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4-8</biblScope>
			<biblScope unit="page" from="704" to="715" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The measurement of learners&apos; self-regulated cognitive and metacognitive processes while using computer-based learning environments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azevedo</surname></persName>
		</author>
		<idno type="DOI">10.1080/00461520.2010.515935</idno>
		<ptr target="https://doi.org/10.1080/00461520.2010.515935" />
	</analytic>
	<monogr>
		<title level="j">Educational Psychologist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stable individual differences in saccadic eye movements during reading, pseudoreading, scene viewing, and scene search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Luke</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0036330</idno>
		<ptr target="https://doi.org/10.1037/a0036330" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1390" to="1400" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0042-6989(99)00163-7</idno>
		<ptr target="https://doi.org/10.1016/s0042-6989(99)00163-7" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inhibitory tagging system facilitates visual search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1038/334430a0</idno>
		<ptr target="https://doi.org/10.1038/334430a0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="issue">6181</biblScope>
			<biblScope unit="page" from="430" to="431" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What&apos;s new in psychtoolbox-3</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Broussard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Even if I showed you where you looked, remembering where you just looked is hard</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Aizenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Võ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1167/17.12</idno>
		<ptr target="https://doi.org/10.1167/17.12" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Subjective report of eye fixations during serial search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bayet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.concog.2014.11.007</idno>
		<ptr target="https://doi.org/10.1016/j.concog.2014.11.007" />
	</analytic>
	<monogr>
		<title level="j">Consciousness and Cognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metamemory: A Theoretical Framework and New Findings</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0079-7421(08)60053-5</idno>
		<ptr target="https://doi.org/10.1016/S0079-7421(08)60053-5" />
	</analytic>
	<monogr>
		<title level="m">Psychology of Learning and Motivation</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Bower</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="125" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The VideoToolbox software for visual psychophysics: Transforming numbers into movies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Pelli</surname></persName>
		</author>
		<idno type="DOI">10.1163/156856897X00366</idno>
		<ptr target="https://doi.org/10.1163/156856897X00366" />
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="442" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Implicit learning and statistical learning: One phenomenon, two approaches</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perruchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pacton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2006.03.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2006.03.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="233" to="238" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Components of visual orienting</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention and Performance X: Control of Language Processes</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="531" to="556" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Eye movements during information processing tasks: Individual differences and cultural effects</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Well</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2007.05.007</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2007.05.007" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2714" to="2726" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Use of eye movements as feedforward training for a synthetic aircraft inspection task</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sadasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Greenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Gramopadhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<idno type="DOI">10.1145/1054972.1054993</idno>
		<ptr target="https://doi.org/10.1145/1054972.1054993" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The relation between implicit statistical learning and proactivity as revealed by EEG</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sznabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kral</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-023-42116-y</idno>
		<ptr target="https://doi.org/10.1038/s41598-023-42116-y" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15787</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Our Eyes Do Not Always Go Where We Want Them to Go: Capture of the Eyes by New Objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="385" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Influence of attentional capture on oculomotor control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<idno type="DOI">10.1037//0096-1523.25.6.1595</idno>
		<ptr target="https://doi.org/10.1037//0096-1523.25.6.1595" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1595" to="1608" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eye Tracking as a Tool to Study and Enhance Cognitive and Metacognitive Processes in Computer-Based Learning Environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jarodzka</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4419-5546-3_10</idno>
		<ptr target="https://doi.org/10.1007/978-1-4419-5546-3_10" />
	</analytic>
	<monogr>
		<title level="m">International Handbook of Metacognition and Learning Technologies</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What Am I Looking at? Interpreting Dynamic and Static Gaze Displays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Wermeskerken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Litchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gog</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12484</idno>
		<ptr target="https://doi.org/10.1111/cogs.12484" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="220" to="252" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Individual differences in crowding predict visual search performance</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Veríssimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hölsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N L</forename><surname>Olivers</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.21.5.29</idno>
		<ptr target="https://doi.org/10.1167/jov.21.5.29" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You think you know where you looked? You better look again</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Võ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Aizenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1037/xhp0000264</idno>
		<ptr target="https://doi.org/10.1037/xhp0000264" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1477" to="1481" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
