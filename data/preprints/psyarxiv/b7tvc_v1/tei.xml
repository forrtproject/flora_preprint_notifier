<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">binaryRL: Reinforcement Learning Modeling of Two-Alternative Forced Choice Decision Making in R -A Step-by-Step Tutorial</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengzhen</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>XiaoTian</roleName><forename type="first">X</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
							<email>xtwang@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Applied Psychology</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Dr</roleName><forename type="first">Wei</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><roleName>Dr</roleName><forename type="first">Guangyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ms</forename><surname>Xinyu</surname></persName>
						</author>
						<title level="a" type="main">binaryRL: Reinforcement Learning Modeling of Two-Alternative Forced Choice Decision Making in R -A Step-by-Step Tutorial</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B41829F7D0B1D148B474B67D718350BB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-18T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=true, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>cognitive modelling</term>
					<term>two-alternative forced choice task</term>
					<term>expected value</term>
					<term>R package</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The reinforcement learning framework offers valuable insights into people's decisionmaking processes in Two-Alternative Forced Choice (TAFC) tasks, providing a deeper understanding of how individuals adjust their choices in response to feedback. However, tools to build, simulate, and compare reinforcement learning models tailored for such tasks remain limited. To address this gap, we developed binaryRL, an R package designed to support the implementation and evaluation of reinforcement learning (RL) models in the context of TAFC tasks, following best-practice guidelines for model comparison in cognitive science. Using real-world open data, we present a tutorial that demonstrate the comprehensive modelling pipeline-from raw experimental input to the construction, parameterization, and validation of RL models. Our package offers a flexible and accessible platform for advancing both theoretical and empirical research in reinforcement learning and cognitive modeling of feedback-based decision-making.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding how humans make decisions is a central goal in psychological science, yet directly studying the mechanisms behind decision-making processes is inherently challenging <ref type="bibr" target="#b36">(Kantowitz et al., 2009)</ref>. As a result, researchers often rely on behavioral experiments to infer cognitive processes from observable behavior. While these approaches facilitate hypothesis testing, they typically offer descriptive insights and fall short of capturing the dynamic, latent mechanisms that drive adaptive behavior.</p><p>With the rise of computational approaches, cognitive modeling has become a crucial tool for bridging the gap between observable behavior and latent cognitive mechanisms <ref type="bibr" target="#b23">(Farrell &amp; Lewandowsky, 2010)</ref>. By constructing formal models that simulate human cognition, researchers can generate testable predictions and iteratively refine theoretical frameworks based on empirical data <ref type="bibr" target="#b10">(Bogacz et al., 2006;</ref><ref type="bibr" target="#b37">Krajbich &amp; Rangel, 2011;</ref><ref type="bibr" target="#b56">Ratcliff &amp; Smith, 2004)</ref>. Among these, reinforcement learning (RL) has emerged as a powerful framework for modeling how agents learn from feedback and update value expectations over time <ref type="bibr" target="#b39">(Lee et al., 2012;</ref><ref type="bibr" target="#b67">Subramanian et al., 2022;</ref><ref type="bibr" target="#b78">Zhang et al., 2020)</ref>. RL models are particularly well-suited for value-based decision-making tasks, where behavior unfolds over repeated choices and is shaped by cumulative outcomes.</p><p>Among the most common paradigms in value-based decision-making are Two-Alternative Forced Choice (TAFC) tasks, in which participants repeatedly choose between two options associated with varying magnitudes of reward and their probabilities (e.g., <ref type="bibr" target="#b11">Botvinick et al., 2019;</ref><ref type="bibr" target="#b60">Robbins, 1952)</ref>. These tasks provide structured environments ideal for trial-by-trial modeling of learning and decision-making <ref type="bibr" target="#b11">(Botvinick et al., 2019)</ref>.</p><p>Despite the increasing use of reinforcement learning in psychological research <ref type="bibr" target="#b20">(Eckstein et al., 2021)</ref>, its broader adoption remains limited due to two key challenges. First, RL is conceptually and computationally complex, involving multiple interrelated components-such as agents, states, actions, rewards, and policies-that require familiarity with advanced mathematical concepts (e.g., Markov Decision Processes) and computational tools. For psychologists without formal training in these areas, the steep learning curve can be a significant barrier. Second, machine learning applications (e.g., computer vision, natural language processing, and control systems)-such as OpenAI Gym <ref type="bibr" target="#b12">(Brockman et al., 2016)</ref>, Deepmind Acme <ref type="bibr" target="#b32">(Hoffman et al., 2022)</ref>, and Tsinghua Tianshou <ref type="bibr" target="#b75">(Weng et al., 2022)</ref> offer limited support for the task paradigms and experimental workflows common in psychological research, like paradigms focused on understanding human decision-making behaviors and workflows involving live interaction between human participants. Using these tools often demands substantial customization, which can be time-consuming and error-prone, especially for researchers with limited coding expertise. Similarly, while R packages developed for psychological applications, such as ReinforcementLearning <ref type="bibr" target="#b54">(Proellochs &amp; Feuerriegel, 2017)</ref>, hBayesDM <ref type="bibr" target="#b0">(Ahn et al., 2017), and</ref><ref type="bibr">twochoiceRL (Suthaharan et al., 2021)</ref>-which align more closely with psychological tasks -still have significant limitations. These packages often lack the flexibility needed for users to customize or extend the underlying reinforcement learning models, with some only supporting basic model architectures.</p><p>This gap highlights the need for accessible, domain-specific tools that support rigorous RL modeling in psychology.</p><p>To address these challenges and reduce barriers to the use of reinforcement learning models in psychology, we introduce binaryRL, an R package designed to facilitate the construction, estimation, and comparison of RL models in the context of TAFC tasks (see Figure <ref type="figure">1</ref>). Built with psychologists in mind, binaryRL integrates best practices in computational modeling <ref type="bibr" target="#b53">(Palminteri et al., 2017;</ref><ref type="bibr" target="#b76">Wilson &amp; Collins, 2019)</ref> into a streamlined and user-friendly framework.</p><p>This article provides a step-by-step tutorial for constructing reinforcement learning models, following the four-stage framework outlined by <ref type="bibr" target="#b76">Wilson &amp; Collins (2019)</ref>. We begin by introducing two-alternative forced-choice tasks and the RL models commonly used to analyze them. We then walk through the full modeling pipeline-(1) building reinforcement learning models, (2) conducting parameter recovery and model validation, (3) optimizing parameters for real data and conducting model comparisons, and finally, (4) replaying the experiment for latent variable analysis -accompanied by annotated R code. To enhance accessibility, especially for researchers new to RL or cognitive modeling, the tutorial starts from raw behavioral data. We illustrate the workflow using open data from the Gambling Paradigm <ref type="bibr" target="#b44">(Mason et al., 2024)</ref>, a representative TAFC task in the study of risky decision-making, though the approach generalizes to a broad range of TAFC designs (e.g., <ref type="bibr" target="#b17">Cools et al., 2002;</ref><ref type="bibr" target="#b22">Ellsberg, 1961;</ref><ref type="bibr" target="#b31">Hertwig et al., 2004;</ref><ref type="bibr" target="#b43">Ludvig &amp; Spetch, 2011;</ref><ref type="bibr" target="#b60">Robbins, 1952;</ref><ref type="bibr" target="#b77">Yechiam &amp; Busemeyer, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 The binaryRL Workflow: A Step-by-step Modelling Pipeline</head><p>Note: ACC = accuracy, proportion of RL model predictions matching human choices; LogL = Log-Likelihood; AIC = Akaike Information Criterion; BIC = Bayesian Information Criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAFC Tasks for Reinforcement Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure and Characteristics of TAFC Tasks</head><p>Two-alternative forced choice tasks are a class of experimental paradigms in which participants are presented with two options on each trial and are required to choose one <ref type="bibr" target="#b10">(Bogacz et al., 2006)</ref>. On each trial, participants select one of two available options, receive feedback (e.g., rewards or outcomes), and use this information to guide future decisions. This trial-by-trial structure makes TAFC tasks particularly well-suited for reinforcement learning models, which are designed to capture how agents update value estimates and action policies based on feedback.</p><p>However, not all TAFC tasks are equally appropriate for RL modeling. RL is best applied to tasks that require participants to learn from outcome contingencies, rather than tasks that simply measure perceptual discrimination or cognitive conflict. Canonical decision-focused paradigms, such as the Two-Armed Bandit Task <ref type="bibr" target="#b60">(Robbins, 1952)</ref>, Ellsberg Paradox <ref type="bibr" target="#b22">(Ellsberg, 1961)</ref>, Passive Avoidance Learning Task <ref type="bibr">(Newman &amp; Kosson, 1986)</ref>, and the Probabilistic Reversal Learning Task <ref type="bibr" target="#b17">(Cools et al., 2002)</ref>, fulfill these criteria. In these tasks, options typically differ in reward probabilities or values, and participants must learn to adapt their choices based on outcome histories. RL models offer a principled framework for characterizing these learning processes, including value updating, reward prediction error computation, and exploration-exploitation tradeoffs (e.g., <ref type="bibr" target="#b26">Gershman, 2015;</ref><ref type="bibr" target="#b40">Liu et al., 2019;</ref><ref type="bibr" target="#b48">Niv et al., 2012;</ref><ref type="bibr" target="#b49">Oba et al., 2021;</ref><ref type="bibr" target="#b62">Rosenbaum et al., 2022;</ref><ref type="bibr" target="#b74">Waltmann et al., 2023)</ref>.</p><p>In contrast to value-based TAFC tasks, certain TAFC tasks-such as the Simple Discrimination Task <ref type="bibr" target="#b30">(Henmon, 1911)</ref> and the Stroop task <ref type="bibr" target="#b66">(Stroop, 1935)</ref>-primarily emphasize the speed-accuracy tradeoff <ref type="bibr" target="#b29">(Heitz, 2014)</ref>. These paradigms typically do not incorporate value-based feedback; instead, they provide binary feedback indicating whether a response was correct or incorrect. As such, they are more appropriately modelled using evidence accumulation frameworks, such as the Drift Diffusion Model <ref type="bibr" target="#b10">(Bogacz et al., 2006)</ref>, than reinforcement learning models, which rely on trial-by-trial learning signals derived from outcome valence.</p><p>In summary, TAFC tasks that involve learning from reward outcomes-rather than simply responding based on perceptual accuracy-are well-suited to modeling within a reinforcement learning framework. One such paradigm, widely used to examine valuebased decision-making, is the Two-Armed Bandit task, which we introduce in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Two-Armed Bandit Task as a Canonical Example</head><p>The Two-Armed Bandit Task <ref type="bibr" target="#b60">(Robbins, 1952)</ref> is a foundational paradigm within the family of TAFC tasks and has been widely used to study learning and decision-making in reinforcement learning (e.g., <ref type="bibr" target="#b48">Niv et al., 2012;</ref><ref type="bibr" target="#b49">Oba et al., 2021;</ref><ref type="bibr" target="#b62">Rosenbaum et al., 2022)</ref>. In this task, participants repeatedly choose between two options that differ in reward structure. For instance, one option may deliver a fixed reward (e.g., 20 points with 100% certainty), while the other offers a probabilistic reward (e.g., 40 points with a 50% chance, otherwise 0) (see Figure <ref type="figure">2</ref>). Although the expected values may be equivalent, the outcome distributions differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 Schematic of Two Representative Trials of The Two-Arm Bandit Task</head><p>Critically, participants are not informed of the reward contingencies beforehand. Instead, individuals must learn through experience which option yields better outcomes over time. This trial-and-error process aligns closely with core reinforcement learning principles, particularly value updating via prediction error-a mechanism originally formalized in psychological models as the Rescorla-Wagner rule <ref type="bibr" target="#b57">(Rescorla &amp; Wagner, 1972)</ref> and later integrated into computational RL frameworks.</p><p>Beyond learning dynamics, the task also provides a flexible platform for examining broader decision-making theories. Variants of the task have been used to test predictions from models such as prospect theory <ref type="bibr" target="#b34">(Kahneman &amp; Tversky, 1979)</ref>, regret theory <ref type="bibr" target="#b41">(Loomes &amp; Sugden, 1982)</ref>, fuzzy-trace theory <ref type="bibr" target="#b58">(Reyna &amp; Brainerd, 1995)</ref>, and trireference point theory <ref type="bibr">(Wang &amp; Johnson, 2002)</ref>, each of which emphasizes distinct psychological constructs. What distinguishes RL models, however, is their ability to model the latent computational process by which individuals integrate past outcomes to guide future choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement Learning Models for Value-Based TAFC Tasks</head><p>To formally capture how participants adapt their choices in response to feedback in value-based TAFC tasks, we turn to reinforcement learning models. RL models describe how agents learn to optimize behavior through interactions with the environment <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>. In value-based decision tasks, at each trial t, the environment presents a state St (e.g., two slot machines, one blue and one green). The agent selects an option At (e.g., choosing the blue machine) according to its internal policy Ï€ (e.g., a rule that always chooses the option with the highest expected value). Before observing the outcome, the agent forms an expectation of the chosen option's value <ref type="bibr" target="#b15">(Cai et al., 2011;</ref><ref type="bibr" target="#b35">Kahnt et al., 2010;</ref><ref type="bibr" target="#b38">Lau &amp; Glimcher, 2008;</ref><ref type="bibr" target="#b51">Padoa-Schioppa &amp; Assad, 2006)</ref>. Upon receiving feedback, a prediction error (PE) is computed as the difference between the actual reward and the expected value <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>. This PE is used to update the expected value, scaled by a learning rate ğœ‚, which determines how strongly new outcomes revise current expectation and, in turn, influence future behavior. This process leads to a new state St+1 and a scalar reward Rt+1. Figure <ref type="figure">3</ref> illustrates this feedback loop in the two-armed bandit task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 Reinforcement Learning Loop in Two-Armed Bandit Task</head><p>Formally, this interaction is modeled as a Markov Decision Process (MDP), where state transitions and rewards depend only on the current state and action, rather than on the entire history <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-Free vs. Model-Based RL</head><p>Reinforcement learning models can be broadly categorized into model-free and model-based approaches <ref type="bibr" target="#b39">(Lee et al., 2012)</ref>. Model-free RL updates action values solely based on past reinforcement signals, without constructing an explicit representation of the task environment <ref type="bibr" target="#b3">(Barraclough et al., 2004;</ref><ref type="bibr" target="#b33">Ito &amp; Doya, 2009;</ref><ref type="bibr" target="#b70">Sutton &amp; Barto, 2018)</ref>. Learning is driven by experienced outcomes and their discrepancies from expectations (i.e., prediction errors).</p><p>Conversely, model-based reinforcement learning models allow inferences to be made about how the environment will behave <ref type="bibr" target="#b39">(Lee et al., 2012;</ref><ref type="bibr" target="#b67">Subramanian et al., 2022;</ref><ref type="bibr" target="#b68">Sutton, 1990)</ref>. For example, given a state and action, the model might predict the resultant next state and next reward. However, in many TAFC tasks, the primary interest lies not in simulating complex state transitions, as options typically don't change dynamically based on previous decisions (see Figure <ref type="figure">3</ref>). Instead, the core objective is to determine which of the given options to choose. For this goal, directly learning choice values from past reinforcement signals via model-free methods presents a simpler method for many TAFC tasks, despite model-based approaches excelling at providing a comprehensive understanding of the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core Learning Models</head><p>To understand how individuals learn from feedback in value-based decision-making, particularly within TAFC paradigms, this section introduces three foundational RL models: Temporal-Difference (TD) learning, Risk-Sensitive TD (RSTD), and Utility-Based learning <ref type="bibr" target="#b48">(Niv et al., 2012)</ref>. These models capture key mechanisms by which agents adjust expectations based on prediction errors and learning rates. While a number of alternative frameworks have been proposed-such as the accentuation-of-differences model <ref type="bibr" target="#b65">(Spektor et al., 2019)</ref> and models with adaptive learning rates (e.g., adaptive learning rates model <ref type="bibr" target="#b26">(Gershman, 2015)</ref>)-the three selected here are among the most widely used and offer a tractable starting point for RL modeling in behavioral research.</p><p>Temporal-Difference (TD) Learning. The TD model is a foundational RL approach <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>. After each choice, the agent updates the value of the chosen option ğ‘‰ ! " using:</p><formula xml:id="formula_0">ğ‘‰ ! " â† ğ‘‰ ! " + ğœ‚ â€¢ &amp;ğ‘Ÿ !#$ + ğ›¾ğ‘‰ !#$ " -ğ‘‰ ! " * (1)</formula><p>where ğœ‚ represents the learning rate and ğ›¾ is the discount factor. ğ‘‰ ! " denotes the expected value of option ğ‘˜ at trial ğ‘¡. The formula can be interpreted as updating the expectation for option ğ‘˜ in the next trial, based on the difference between the actual reward and the current trial's expectation for option ğ‘˜, scaled by the learning rate ğœ‚. This formulation allows the agent to update its expectations not only based on immediate outcomes but also anticipated future rewards.</p><p>In many TAFC tasks, which provide rewards immediately after each choice, the delay discount factor is omitted, reducing the model to the Rescorla-Wagner form <ref type="bibr" target="#b57">(Rescorla &amp; Wagner, 1972)</ref>:</p><formula xml:id="formula_1">ğ‘‰ ! " â† ğ‘‰ ! " + ğœ‚ â€¢ &amp;ğ‘Ÿ !#$ -ğ‘‰ ! " * (2)</formula><p>This simplified rule reflects incremental learning via prediction errors, a hallmark of reinforcement-based updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Risk-Sensitive Temporal Difference (RSTD) Model.</head><p>Standard TD learning assumes symmetric learning across all outcomes, but psychological research suggests that individuals often weigh gains and losses differently. The RSTD model addresses this gain-loss asymmetry by assigning separate learning rates for positive and negative prediction errors <ref type="bibr" target="#b46">(Mihatsch &amp; Neuneier, 2002)</ref>.</p><p>Specifically, the RSTD model assigns separate learning rates depending on the valence of the prediction error. When the actual reward exceeds the expected value-i.e., when the prediction error is positive-a positive learning rate (ğœ‚ # ) governs the update of the value estimate. Conversely, when the reward falls below expectations-i.e., a negative prediction error-a distinct, negative learning rate (ğœ‚ % ) is applied. This asymmetry enables the model to reflect differential sensitivity to favorable and unfavorable outcomes.</p><formula xml:id="formula_2">ğ‘‰ ! â† 1 ğ‘‰ ! + ğœ‚ # â€¢ (ğ‘… !#$ -ğ‘‰ ! ) ğ‘–ğ‘“ ğ‘… !#$ -ğ‘‰ ! &gt; 0 ğ‘‰ ! + ğœ‚ % â€¢ (ğ‘… !#$ -ğ‘‰ ! ) ğ‘–ğ‘“ ğ‘… !#$ -ğ‘‰ ! &lt; 0 (3)</formula><p>This asymmetry enables the model to capture individual differences in loss aversion or optimism bias, making it particularly relevant in studies of risk-sensitive or emotionally charged decision-making.</p><p>Utility-Based Learning Models. Traditional RL models assume that rewards are processed in their objective, numerical forms. However, human decision-making is based on subjective utility rather than raw outcomes <ref type="bibr" target="#b8">(Bernoulli, 1954)</ref>. A utility model thus transforms the objective reward ğ‘… !#$ into a nonlinear subjective value ğ‘ˆ(ğ‘… !#$ ), via, for instance, a power function <ref type="bibr" target="#b13">(Buchsbaum &amp; Stevens, 1971)</ref>:</p><formula xml:id="formula_3">ğ‘¢(ğ‘¥) = ğ‘¥ &amp; (4)</formula><p>The value update rule is then modified as:</p><formula xml:id="formula_4">ğ‘‰ ! â† ğ‘‰ ! + ğœ‚ â€¢ [ğ‘ˆ(ğ‘… !#$ ) -ğ‘‰ ! ]<label>(5)</label></formula><p>Here, ğœ‚ represents the learning rate, and ğ‘ˆ(ğ‘… !#$ ) is the subjectively transformed utility of the received reward. By incorporating nonlinear utility, this model reflects the subjective valuation process that underlies decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From Learning to Choice: Modeling Decision Behavior in TAFC Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice Selection via Exploration-Exploitation Tradeoff</head><p>While reinforcement learning models provide a formal account of how agents update value estimates based on feedback, a critical design consideration arises in translating these learned values into action: how to implement the exploration-exploitation tradeoff.</p><p>Using a value function, an agent estimates the expected reward associated with each action. Selecting the action with the highest estimated value-commonly referred to as the greedy action-constitutes exploitation of current knowledge. In contrast, choosing a lower-valued action-i.e., a nongreedy action-constitutes exploration, enabling the agent to refine its value estimates for less familiar alternatives. Because it is not possible to both exploit and explore within a single choice, this tradeoff governs how agents balance the pursuit of immediate reward with the need to gather information that may improve future decisions <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>.</p><p>One common solution for implementing the exploration-exploitation tradeoff is the softmax action selection rule, which converts estimated values into choice probabilities in a graded, probabilistic manner. This approach aligns well with empirical findings that human choices in Two-Alternative Forced Choice (TAFC) tasks are inherently stochastic: participants do not always select the option with the highest estimated value, but their likelihood of choosing an option increases with its relative value <ref type="bibr" target="#b39">(Lee et al., 2012)</ref>. The softmax function thereby captures both deterministic and exploratory elements of choice behavior.</p><p>Formally, for a TAFC task with two options-left (L) and right (R)-the probability of choosing each option is given by:</p><formula xml:id="formula_5">ğ‘ƒ ' = 1 1 + ğ‘’ %() ! %) " )â€¢, ; ğ‘ƒ -= 1 1 + ğ‘’ %() " %) ! )â€¢,<label>(6)</label></formula><p>where ğ‘‰ ' and ğ‘‰ -denote the estimated values of the left and right options, respectively; the inverse temperature parameter ğœ governs the degree of stochasticity: higher ğœ values produce more deterministic choices, while lower ğœ values lead to more random responding.</p><p>Although the softmax model is widely used due to its ability to balance exploration and exploitation seamlessly, other strategies (see our tutorial website for more details), such as the epsilon-greedy approach <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>, provide alternative ways to operationalize exploration. With a fixed probability Ïµ, the agent selects a random action regardless of value estimates (pure exploration). With probability 1-Ïµ, it chooses the option with the highest estimated value (pure exploitation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fitting and Likelihood-Based Evaluation</head><p>To evaluate how well a reinforcement learning model captures human choice behavior, it is essential to quantitatively compare the model's predicted choice probabilities against actual participant responses. Likelihood-based methods, particularly the log-likelihood metric, provide a principled approach for this purpose <ref type="bibr" target="#b28">(Hampton et al., 2006)</ref>.</p><p>In TAFC tasks, choices are binary, with each trial resulting in the selection of either the left or right option. The RL model generates predicted probabilities for each option using the softmax rule. The log-likelihood (LL) is calculated as the sum of the logprobabilities assigned to the participant's observed choices across trials:</p><formula xml:id="formula_6">ğ¿ğ¿ = H ğµ '. â€¢ ğ‘™ğ‘œğ‘” ğ‘ƒ '. / .0$ + H ğµ -. â€¢ ğ‘™ğ‘œğ‘” ğ‘ƒ -. / .0$ (7)</formula><p>where ğµ '. and ğµ -. are binary indicators (1 or 0) denoting whether the participant chose the left or right option on trial ğ‘–, and ğ‘ƒ '. and ğ‘ƒ -. are the corresponding model-predicted probabilities. Higher LL values indicate better alignment between the model's predictions and observed behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Criteria and Model Comparison</head><p>While the log-likelihood quantifies goodness of fit, it does not penalize model complexity. Models with many free parameters may fit idiosyncratic noise rather than meaningful psychological processes <ref type="bibr" target="#b76">(Wilson &amp; Collins, 2019)</ref>. To counter this, information-theoretic criteria such as the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) introduce complexity penalties, balancing fit and parsimony <ref type="bibr" target="#b1">(Akaike, 1974;</ref><ref type="bibr" target="#b63">Schwarz, 1978)</ref>:</p><formula xml:id="formula_7">ğ´ğ¼ğ¶ = -2ğ¿ğ¿ + 2ğ‘˜ (8) ğµğ¼ğ¶ = -2ğ¿ğ¿ + ğ‘˜ ğ‘™ğ‘œğ‘” ğ‘› (9)</formula><p>where ğ‘˜ is the number of free parameters and ğ‘› is the total number of trials. These criteria facilitate principled model selection by favoring models that generalize beyond the sample data, thus strengthening theoretical inferences drawn from model-based analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Structured Approach to RL Modeling: The Wilson &amp; Collins Framework</head><p>Building on the theoretical foundations of TAFC tasks and reinforcement learning models, this section briefly introduces the four-step modeling framework proposed by <ref type="bibr" target="#b76">Wilson and Collins (2019)</ref>, which has become a widely adopted standard in cognitive and computational modeling (e.g., <ref type="bibr" target="#b16">Ciranka et al., 2022;</ref><ref type="bibr" target="#b72">Valentin et al., 2024;</ref><ref type="bibr" target="#b73">van Baar et al., 2022)</ref>. This framework provides a systematic and transparent roadmap for model development and validation, guiding researchers through four essential stages: (1) constructing a model suited to the task structure, (2) conducting parameter and model recovery to ensure identifiability, (3) fitting the model to empirical data, and (4) reporting and interpreting results in a transparent and reproducible manner. We now introduce each of these steps in detail.</p><p>Step 1: Design an Experiment and Build RL Models. The first step in applying reinforcement learning models to behavioral data is to define an appropriate experimental paradigm and develop a set of candidate models capable of capturing the key experimental effects observed in the data. A central focus of this stage is the specification of the model's core computational components, such as learning rules, utility transformations, and choice functions.</p><p>To illustrate this step, we use a well-established experimental task-the Two-Armed Bandit task <ref type="bibr" target="#b60">(Robbins, 1952)</ref>, which is implemented in a publicly available dataset <ref type="bibr" target="#b44">(Mason et al., 2024</ref>, <ref type="url" target="https://osf.io/hy3q4/">https://osf.io/hy3q4/</ref>) and serves as an example in this tutorial. More details of the task design are provided in the coding tutorial section. Candidate RL models vary in how they formalize learning and decision-making-e.g., the Temporal Difference model with a fixed learning rate, the Risk-Sensitive TD model allowing asymmetric updates, and utility-based models incorporating nonlinear transformations. Action selection is modeled using the softmax (probabilistic choice) method.</p><p>Step 2: Parameter and Model Recovery. Parameter and model recovery are critical steps in evaluating candidate models. Before fitting models to real data, researchers should assess the stability and identifiability of each model by performing recovery analyses. This process involves generating synthetic datasets using a given model and then fitting these datasets with both the generating model and alternative models. Parameter recovery refers to the degree to which the parameters used to generate data can be accurately estimated (i.e., show high correlation) by fitting the model to that data. Model recovery, on the other hand, assesses whether a model can be correctly identified from simulated data that it itself generated, typically by showing that it has the lowest BIC compared to other candidate models. For more details, see <ref type="bibr" target="#b76">Wilson and Collins (2019)</ref>.</p><p>Only when a model demonstrates strong recoverability-accurately retrieving its own parameters and outperforming other models in fitting its own simulated data-can its performance on real data be meaningfully interpreted and compared to other candidate models.</p><p>Step 3: Fit Real Data and Model Comparison. Once parameter recovery and model recovery analyses have been successfully completed, providing confidence in the model's ability to estimate its parameters and distinguish between competing mechanisms reliably, researchers can proceed to fit these candidate computational models to real experimental behavioral data. This step involves finding the set of parameter values for each model that best accounts for the observed human behavior, often by maximizing the likelihood of the data given by the model and its parameters.</p><p>After fitting, the next step is to perform model comparison. This process aims to determine which of the predefined set of models is most likely to have generated the observed behavioral data. Various methods exist for model comparison, with approaches like the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) being commonly used. It is essential to ensure that the parameter range obtained from fitting the real data aligns with the ranges where parameter and model recovery were robust; otherwise, it may be necessary to revisit and adjust the simulation parameters. The ultimate goal is to select the "winning" model that offers the best balance of fitness and parsimony.</p><p>Step 4: Latent Variable Analysis and Report Results. Following model fitting and selection, the next step is to evaluate how well the best-fitting model captures key behavioral patterns. This is typically done through posterior predictive checks <ref type="bibr" target="#b25">(Gelman et al., 1996;</ref><ref type="bibr" target="#b61">Roecker, 1991)</ref>, where the model, using its fitted parameters, is used to simulate behavioral data. These simulated outcomes are then compared to empirical results to assess whether the model reproduces the core experimental effects of interest. This process strengthens confidence that the model reflects not just statistical patterns but meaningful psychological processes.</p><p>In addition to validating model performance, researchers can examine latent variables-unobservable internal quantities inferred from model parameters-to gain insight into underlying cognitive mechanisms. For example, fitted learning rates can be correlated with individual traits or external measures, enabling interpretation of how computational processes relate to broader psychological constructs (e.g., <ref type="bibr" target="#b18">Davidow et al., 2016;</ref><ref type="bibr" target="#b42">Louie, 2022;</ref><ref type="bibr" target="#b47">Neuser et al., 2023)</ref>. Such analyses help bridge the gap between model parameters and theoretical understanding of human behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building Reinforcement Learning Models with the binaryRL Pipeline</head><p>In this section, an openly available dataset from the Gambling Paradigm <ref type="bibr" target="#b44">(Mason et al., 2024)</ref> will be used as an example, along with annotated R code from our package, to facilitate a hands-on understanding of the complete four-step reinforcement learning modelling workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting Up the binaryRL Environment in R</head><p>The binaryRL package is written almost entirely in R (R Core Team, 2020) and Rcpp <ref type="bibr" target="#b21">(Eddelbuettel et al., 2008)</ref>. The parallel processing relies on the future <ref type="bibr" target="#b5">(Bengtsson, 2015)</ref>, doFuture <ref type="bibr" target="#b6">(Bengtsson, 2016)</ref> and foreach <ref type="bibr" target="#b45">(Microsoft &amp; Weston, 2009)</ref> packages, with doRNG <ref type="bibr" target="#b24">(Gaujoux, 2011)</ref> used to control the random number seeds during parallel execution and progressr <ref type="bibr" target="#b7">(Bengtsson, 2020)</ref> to display progress bars. Although the dependencies of these R packages have minimal R version requirements, we still recommend that users install our package using an R version higher than 4.0.0. Our package installation only includes R packages necessary for parallel processing and does not force the installation of algorithm packages. If users need to use algorithms other than L-BFGS-B, the software will prompt them to install the corresponding algorithm packages.</p><p>Step 1: Design an Experiment and Build an RL Model</p><p>The example experiment represents a variant of the two-armed bandit task introduced earlier, in which participants repeatedly choose between two options that differ in risk and reward. In the original mixed-design experiment, participants were divided into two between-subjects groups: Rare-extreme and Rare-non-extreme. A total of 250 participants were randomly assigned to these two groups, with 125 participants in each. The within-subjects factor was the choice domain (gain vs. loss).</p><p>The example data used in this tutorial is from the Rare-non-extreme group. Participants in the Rare-non-extreme group (binaryRL::Mason_2024_G2) faced two options in both gain and loss frames. In the gain frame, the risky option offered a 90% chance of receiving 40 points and a 10% chance of receiving 0 points, whereas the fixed option guaranteed 36 points. In the loss frame, the risky option carried a 90% chance of losing 40 points and a 10% chance of losing 0 points, while the fixed option resulted in a guaranteed loss of 36 points. For more details, refer to <ref type="bibr" target="#b44">Mason (2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Structure and Column Specifications</head><p>For application of the R code described in the next section, the dataset must be in long format (each row of data represents a single trial). Most experimental software (e.g., PsychoPy, jsPsych, E-Prime) outputs data in long format by default; if not, functions such as tidyr::pivot_longer() can transform wide-format datasets to long format. Additionally, it is essential for RL modelling that the dataset records the reward of the unchosen option for each trial. To facilitate visualization, Table <ref type="table" target="#tab_0">1</ref> presents the example dataset in long format. The essential columns in the dataset are Subject, Block, Trial, L_choice, R_choice, L_reward, R_reward, Sub_Choose. If users choose to rely solely on the built-in models, the 'reinforcement learning model construction' step can be omitted. However, the column names in the dataset must match those of the key columns in the example dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement Learning Model Construction</head><p>This sub-section details the most distinctive and technically involved component of this R package: constructing a custom reinforcement learning model. For users who prefer to use one of the three built-in RL models (TD, RSTD, Utility), this step can be skipped. These models can be invoked directly using the rcv_d and fit_p functions with the following code:</p><p>recovery &lt;-rcv_d( â€¦, simulate_models = list(TD, RSTD, Utility), fit_models = list(TD, RSTD, Utility), â€¦ ) comparison &lt;-fit_p( â€¦, fit_models = list(TD, RSTD, Utility), â€¦ )</p><p>To construct a custom model, the first step is to define an objective function compatible with the estimation algorithm. Here, we use the TD model as an example to demonstrate how to build its objective function. This function takes a single argument, params, which corresponds to the model's two free parameters: the learning rate, eta, and the inverse temperature, tau. The arguments in the run_m function-including data, id, n_trials, n_params, priors, mode, and policy-are automatically populated by subsequent functions such as rcv_d(), fit_p(), or rpl_e(). The final three lines of the function handle result assignment and environment variable transfer. Users are strongly advised not to modify these components. A detailed explanation of the primary arguments is provided below: name: Defaults to NA. The program automatically detects whether a built-in model (TD, RSTD, or Utility) is used based on the number of learning rates and the presence of gamma. Users defining custom models may modify this argument. data: The dataset currently being processed. id: The identifier of the participant being analyzed. May be of character or numeric type, but must correspond to an existing ID in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n_trials:</head><p>The total number of trials completed by each participant. This value is used in the calculation of AIC and BIC. n_params: The number of free parameters in the RL model. For instance, the basic TD model includes two free parameters: the learning rate eta and the inverse temperature parameter tau; hence , n_params = 2. priors: A list specifying the prior distributions for each parameter. Each element must be a density function. mode: Accepts one of three values: simulate, fit, or replay. These correspond to parameter/model recovery, model fitting to real data, and replaying the experiment using estimated parameters (steps 2, 3, and 4 in the workflow, respectively). policy: Determines the policy used during fitting. Off-policy methods use the participant's actual choices to update value estimates, whereas on-policy methods sample choices based on the model's current action probabilities and update values accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specify Your Column Names</head><p>When you begin creating a new reinforcement learning model, the first thing to consider is the column names. While binaryRL can run models with user-defined column names, these may not match the package's defaults. You must therefore inform the program (the run_m function) what your column names are by specifying the following arguments as character strings:</p><p>run_m( # column names sub = "Subject", time_line = c("Block", "Trial"), L_choice = "L_choice", R_choice = "R_choice", L_reward = "L_reward", R_reward = "R_reward", sub_choose = "Sub_Choose", var1 = "extra_Var1", var2 = "extra_Var2" â€¦ )</p><p>Each argument serves the following purpose: sub: Specifies the column containing participant identifiers. Values can be numeric or character. time_line: A character vector defining the hierarchical structure of the experiment timeline. For example, c("Block", "Trial") indicates that the experiment is organized into blocks, each containing multiple trials.</p><p>L_choice and R_choice: Specify the columns indicating the options available on the left and right sides, respectively, in each trial. L_reward and R_reward: Specify the columns containing the rewards associated with the left and right options, respectively. sub_choose: Indicates the column recording the actual choice made by the participant. The values should match those in L_choice and R_choice. var1 and var2: Allow incorporation of additional experimental variables beyond rewards and choices. For instance, setting var1 = "Frame" enables the model to access information about gain/loss framing. These parameters offer flexibility for models that require supplementary contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Free Parameters in Reinforcement Learning Models</head><p>The second part of constructing a custom reinforcement learning model is defining its free parameters, which are used to capture different cognitive and behavioral mechanisms. The run_m function supports multiple parameters that govern value updating, exploration-exploitation tradeoffs, and choice rules. Below is an example of how these parameters can be passed:</p><p>run_m( â€¦, # value function eta = c(params[â€¦], params[â€¦]), gamma = c(params[â€¦], params[â€¦]), # exploration-exploitation tradeoff initial_value = NA, threshold = 1, epsilon = c(params[â€¦]), lambda = c(params[â€¦]), # upper-confidence-bound pi = c(params[â€¦]), # soft-max tau = c(params[â€¦]), lapse = 0.02, # extra parameters alpha = c(params[â€¦], params[â€¦]), beta = c(params[â€¦], params[â€¦]), â€¦ )</p><p>Each of these parameters plays a distinct role in the model's behavior. The following sections provide a detailed explanation of their theoretical and practical implications. u Value Function utility function (gamma): Although named gamma, this parameter does not correspond to the temporal discounting factor in classical TD learning. Rather, it represents the exponent in Stevens' power law, capturing nonlinear relationships between objective rewards and their subjective utility. This package is built upon the Rescorla-Wagner framework, which does not incorporate future reward discounting, as outcomes are immediate within each trial. learning rate (eta): The learning rate (eta), often denoted as alpha in the literature, controls the extent to which prediction errors update value estimates. A higher value implies greater weight on recent prediction errors. While standard TD models use a single learning rate, the RSTD model incorporates two distinct rates: one for positive prediction errors (eta+) and another for negative ones (eta-). The name eta is used here to avoid namespace conflicts, reserving alpha for user-defined extensions. u Exploration-exploitation Tradeoff</p><p>Based on <ref type="bibr">Sutton and Barton (2018)</ref>, the package supports several strategies to balance exploration and exploitation: optimistic initial value (initial_value): Setting a high initial value for options encourages exploration of less-chosen alternatives. This strategy, known as "optimistic initialization" <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>, can influence the estimated asymmetry between positive and negative learning rates <ref type="bibr" target="#b52">(Palminteri &amp; Lebreton, 2022)</ref>. By default, initial_value = NA uses the first encountered reward as the initial value. Users may fix this to a specific value or treat it as a free parameter. epsilon-based strategies (threshold, epsilon, lambda): Epsilon-greedy policies choose the highest-value option with probability 1 -epsilon, and explore randomly otherwise. The package also supports: o epsilon-first: fully random exploration for the first threshold trials, then greedy exploitation. o epsilon-decreasing: exploration probability decays across trials, controlled by lambda. upper confidence bound (pi): This strategy promotes exploration of less-sampled options by adding a bonus proportional to the uncertainty in value estimates <ref type="bibr" target="#b70">(Sutton &amp; Barto, 2018)</ref>. The parameter pi (often denoted as c) controls the degree of exploration bias. By default, pi is set to NA, which ensures the model samples each option at least once. If the user sets it to 0, the bias_func will be completely disabled. softmax inverse temperature (tau): The parameter tau (commonly beta) regulates how deterministic choices follow value differences. Higher values increase sensitivity to value differences, leading to more exploitative behavior. Lower values produce more random choices. Along with eta, this is one of the most widely used parameters in RL modeling. u Lapse Rate lapse rate (lapse): To avoid numerical underflow when computing log-probabilities of choice, a small lapse rate is often incorporated <ref type="bibr" target="#b76">(Wilson &amp; Collins, 2019)</ref>. This represents the probability of a random choice error, ensuring that no option has exactly zero probability of being chosen. The default value of 0.02 ensures a minimum choice probability of 1% in two-alternative tasks. This parameter can also be treated as free-elevated lapse rates may indicate participant inattention, especially in catch trials where one option is clearly disadvantageous. u Extra Parameters alpha, beta: The names alpha and beta are reserved for user-defined parameters, allowing maximal flexibility for custom model extensions. This ensures that users can incorporate novel mechanisms without altering the core package structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core Functions for Custom Model Specification</head><p>Despite having the ability to define the number of free parameters to create variations of the built-in models, if users would like to build a completely new model, they must define the third part in run_m, the six core computational components: the utility function, learning rate function, exploration strategy, bias function, choice probability function, and loss function. If any of these functions are modified in a custom model, their names must be provided as a character vector in the funcs argument of rcv_d()or fit_p(), ensuring proper transmission to parallel computing environments. The following functions can be customized: Users can redefine this function to incorporate alternative utility transformations. rate_func (associated with eta): Determines the learning rate applied during value updating. The default implementation uses an if-else statement to distinguish between the TD and RSTD models: if a single learning rate is provided, it operates as in standard TD learning; if two rates are provided, it implements the RSTD rule by selecting eta <ref type="bibr">[1]</ref> when the reward exceeds the expectation and eta[2] otherwise. expl_func (associated with threshold, epsilon, lambda): Implements exploration strategies such as Îµ-greedy, Îµ-first, or Îµ-decreasing policies. The function uses conditional checks on the input parameters to determine which strategy to execute and returns an exploration flag (try) that influences subsequent choice selection. bias_func (associated with pi): Adds an exploration bonus to value estimates to encourage sampling of less-frequently chosen options. If pi = NA, the function ensures each option is selected at least once. If pi is a numeric value, it implements standard upper confidence bound (UCB) action selection. Users can modify this function to implement alternative exploration biases. prob_func (associated with tau): Converts value differences into choice probabilities. Receives the exploration flag (try) from expl_func: if try = 1, it returns uniform random choice probabilities; otherwise, it applies either greedy selection or softmax normalization based on the inverse temperature parameter tau. The lapse rate adjusts the final probabilities to prevent numerical underflow. loss_func: Computes the discrepancy between model predictions and actual choices. The default implementation calculates the negative log-likelihood of observed choices given model parameters. Users can redefine this function to implement alternative loss functions. These core functions provide comprehensive flexibility for implementing a wide range of reinforcement learning models beyond the built-in TD, RSTD, and Utility specifications. Users are encouraged to refer to the package documentation and source code for detailed examples of function implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 2: Parameter Recovery and Model Recovery</head><p>To assess model identifiability and robustness, researchers should conduct parameter recovery and model recovery analyses using the rcv_d function. In this example, synthetic datasets were generated by sampling parameters from three built-in RL models: TD, RSTD, and Utility models. By default, the learning rate (Î·) and utility (Î³) parameters were drawn from a uniform distribution ranging from 0 to 1, while the inverse temperature parameter (Ï„) for the softmax function was sampled from an exponential distribution. Each simulated dataset was then fitted using all three models to determine which model provided the best fit. The R code for this procedure is as follows:</p><p>recovery &lt;-rcv_d( policy = "on", data = Mason_2024_G2, model_names = c("TD", "RSTD", "Utility"), simulate_models = list(TD, RSTD, Utility), fit_models = list(TD, RSTD, Utility), lower = list(c(0, 0), c(0, 0, 0), c(0, 0, 0)), upper = list(c(1, 5), c(1, 1, 5), c(1, 1, 5)), iteration_s = 100, iteration_f = 100, nc = 4, algorithm = c("NLOPT_GN_MLSL", "NLOPT_LN_BOBYQA") )</p><p>The main arguments are described below. Arguments marked with * do not need to be set by the user, as default values are sufficient: policy: Determines the policy used during fitting. off-policy uses the participant's actual choices to update value estimates, whereas on-policy samples choices based on the model's current action probabilities and updates values accordingly. data: The dataset used for analysis. model_names: Names of the candidate models, entered as a character vector. simulate_models: Models used to generate simulated data, passed as a list of functions. fit_models: Models used to fit simulated data, passed as a list of functions. lower/ upper: Lower and upper bounds for free parameters in the fitting models. The class of these bounds must be numeric vector, and the entire argument is passed as a list. iteration_s: Numbers of simulated datasets. iteration_f: Maximum iterations for the fitting algorithm per simulated dataset. nc: Number of threads for computation. nc &gt; 1 enables parallel fitting of multiple datasets. algorithm: Choice of fitting algorithm. We provide eight algorithms for users: L-BFGS-B <ref type="bibr" target="#b14">(Byrd et al., 1995)</ref>, Simulated Annealing <ref type="bibr" target="#b27">(Gubian et al., 2011)</ref>, Genetic Algorithm <ref type="bibr" target="#b64">(Scrucca, 2012)</ref>, Differential Evolution <ref type="bibr" target="#b2">(Ardia et al., 2005)</ref>, Particle Swarm Optimization <ref type="bibr" target="#b4">(Bendtsen, 2010)</ref>, Bayesian Optimization <ref type="bibr" target="#b9">(Bischl et al., 2017)</ref>, Covariance Matrix Adapting Evolutionary Strategy <ref type="bibr" target="#b71">(Trautmann et al., 2010)</ref> and Nonlinear Optimization Library <ref type="bibr" target="#b78">(Ypma &amp; Johnson, 2011)</ref>. estimate*: Specifies the estimation method. The default is Maximum Likelihood Estimation (MLE), which does not incorporate prior distributions. Setting this to Maximum a Posteriori (MAP) uses the EM algorithm, following the approach in the sjgershm/mfit MATLAB toolbox. rfun*: A nested list of functions generates random parameter values for simulation. The elements of thelist must be a named list of functions, where each name corresponds to a model parameter and its value is the random number generation function. dfun*: A nested list that defines the probability density/mass functions (PDF/PMF) for each model's parameters. The top-level names of the list must match the model names. Each element must be another named list, where each name corresponds to a model parameter and its value is the probability density function.</p><p>id*: Defaults to NULL. The program randomly selects one participant as a template to generate simulated data, ensuring that the trial sequence matches the original experiment. This is important for parameter and model recovery. n_trials*: Defaults to NULL. The number of trials per participant is automatically determined. If participants experienced different paradigms with varying trial numbers, id and n_trials must be set manually. funcs*: For custom models that modify core functions and use parallel computing (nc &gt; 1), list the names of the custom functions to ensure they are accessible in the parallel environment. initial_params*: Initial parameter values for iterative algorithms. Defaults to NA, with each parameter initialized at lower + 0.01. It can be set manually in the form of a numeric vector if needed. initial_size*: For evolutionary algorithms, the initial population size can be specified (default to 50). Excessively large sizes increase computation time. seed*: Random seed to ensure reproducible results.</p><p>After defining the necessary arguments, users will obtain the model recovery output. Table <ref type="table" target="#tab_1">2</ref> shows a subset of the example output. Each row corresponds to a single recovery iteration, listing the model used to generate the data, the model used for fitting, key performance indices (accuracy, log-likelihood, AIC, BIC), and the true (input) versus recovered (output) parameter values. Differences between input and output parameters reflect the extent to which the fitting procedure successfully recovered the generative parameters, given the specified argument settings. </p><formula xml:id="formula_8">â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦</formula><p>The result table can be used for visualization. For example, Figure <ref type="figure">4</ref> illustrates the parameter recovery plots (scatterplots comparing input and recovered parameters), while Figures <ref type="figure" target="#fig_7">5a</ref> and <ref type="figure" target="#fig_7">5c</ref> show confusion matrices, and Figures <ref type="figure" target="#fig_7">5b</ref> and <ref type="figure" target="#fig_7">5d</ref> present inversion matrices. As plotting was not the primary focus of this work, the code for reproducing these plots is openly available at the package homepage (<ref type="url" target="https://yuki-961004.github.io/binaryRL/">https://yuki- 961004.github.io/binaryRL/</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>Parameter Recovery Plots under Different Inverse Temperature Settings NOTE: In Figures <ref type="figure">4a</ref> and <ref type="figure">4b</ref>, the inverse temperature parameter in the softmax function was set according to Ï„ ~ Exp(1). In this scenario, choice behavior exhibits a certain degree of randomness. Consequently, the inverse temperature parameter can be recovered with relatively high accuracy, whereas the learning rate parameter Î· in the TD model cannot. To facilitate Î· recovery, researchers may add 1 to all Ï„ values during data generation, which reduces choice randomness and results in a more accurate recovery of the learning rate Î·, but a less accurate recovery of the inverse temperature parameter Ï„ (see Figures <ref type="figure">4c</ref> and <ref type="figure">4d</ref>). For visualization purposes, in Figure <ref type="figure">4d</ref>, we subtracted 1 from Ï„ values when plotting, so that the figures reflect the original coordinate scale.</p><p>In Figure <ref type="figure" target="#fig_7">5</ref>, the confusion matrix (Figures <ref type="figure" target="#fig_7">5a</ref> and <ref type="figure" target="#fig_7">5c</ref>  <ref type="figure" target="#fig_7">5b</ref> and <ref type="figure" target="#fig_7">5d</ref>) shows P(simulated model | fit model), computed from the confusion matrix using Bayes' theorem. It represents the likelihood that a dataset was generated by a particular model (rows), given that a specific model was identified as the best fit (columns), which is particularly useful when the true generating model is unknown. NOTE: In Figures 5a and 5b, the inverse temperature parameter in the softmax function was set according to Ï„ ~ Exp(1). In this scenario, choice behavior exhibits a certain degree of randomness. Consequently, it is difficult for simulated data generated by Model A to be best fit by Model A itself. Similar to parameter recovery, increasing the inverse temperature Ï„ in the softmax function by 1 can improve the effect of model recovery.</p><p>It is crucial to clarify that the parameter and model recovery results presented in this section should be viewed as illustrative of the analytical workflow rather than as an evaluation of the robustness of our MDP construction. The main challenge lies in the inverse temperature parameter of the softmax function, which introduces choice stochasticity into the simulated data and interferes with the recovery of other parameters (Fig. <ref type="figure">4a</ref>). Our tests show that near-perfect recovery can be achieved using estimation methods that bypass the log-likelihood, such as Approximate Bayesian Computation (ABC) or Recurrent Neural Networks (RNN), suggesting that the issue stems from the inherent information loss of log-likelihood-based approaches.</p><p>This limitation is not unique to our work; it is consistent with findings from <ref type="bibr" target="#b76">Wilson and Collins (2019)</ref>. We not only replicated their results, showing that manually increasing the inverse temperature by 1 improves the recovery of other parameters (Figs. <ref type="figure">4a</ref>, <ref type="figure">4c</ref>) and model recovery (Fig. <ref type="figure" target="#fig_7">5</ref>), but also found that this adjustment comes at the cost of poor recovery of the inverse temperature itself (Figs. <ref type="figure">4b</ref>, <ref type="figure">4d</ref>). We suggest it may be an unavoidable limitation when combining log-likelihood-based estimation with RL models. <ref type="bibr" target="#b76">Wilson and Collins (2019)</ref> further proposed a tradeoff approach, in which introducing an unimportant parameter can improve the recovery of others. However, this also implies that without new constraints, one can achieve reliable recovery for either the learning rate (Î·) or the inverse temperature (Ï„), but not both.</p><p>These findings have an important implication for practical applications: when fitting real-world data with MLE, a low estimated inverse temperature should raise doubts about the reliability of the fitted learning rate. In addition, factors such as policy choice (on or off), algorithm package (e.g., GenSA, GA), and the number of iterations also affect recovery outcomes. Researchers are therefore encouraged to account for these factors and to consider alternative estimation methods, as demonstrated in our advanced tutorial, when aiming for more precise parameter recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 3: Fitting Real Data and Model Comparison</head><p>To evaluate model performance at the individual level, we employed the fit_p function, which simultaneously estimates parameters and model fit indices for each participant across multiple candidate models. In the example below, we compared three models-TD, RSTD, and Utility-using data from 125 participants. The R code for this procedure is as follows: The main arguments are described below. Arguments marked with an asterisk (*) are optional and do not need to be set by the user, as default values are sufficient: policy: Determines the policy used during fitting</p><p>. Off-policy methods use the participant's actual choices to update value estimates, whereas on-policy methods sample choices based on the model's current action probabilities and update values accordingly. data: The dataset to be analyzed. model_names: Character vector specifying the names of the models. fit_models: List of model-fitting functions. lower/ upper: Lower and upper bounds for free parameters in the fitting models. The class of these bounds must be numeric vector, and the entire argument is passed as a list. iteration_i: Maximum number of optimization iterations for fitting each participant's data. nc: Number of threads. Defaults to 1 (sequential computation). Values greater than 1 enable parallel computing. algorithm: Optimization algorithm. We provide eight algorithms for users. The fit_p function returns optimal parameters and key performance metrics, including accuracy, log-likelihood, Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). Table 3 shows a subset of the example output. estimate*: Specifies the estimation method. The default is Maximum Likelihood Estimation (MLE), which does not incorporate prior distributions. Setting this to Maximum a Posteriori (MAP) uses the EM algorithm, following the approach in the sjgershm/mfit MATLAB toolbox. iteration_g*: Maximum number of EM iterations in MAP estimation. priors* (Same as dfun in rcv_d): Prior probability density functions for parameters (only required when estimate = "MAP"). Uniform priors can be applied to all parameters, or different priors (e.g., an exponential distribution for the inverse temperature) can be specified. In MAP estimation, priors are reset after each iteration to a normal distribution. This assumption may be suboptimal (The resulting file will only include the log priority and log posterior if priors have been set.). tolerance*: Convergence criterion for the EM algorithm. Iterations stop when the change in log posterior between two iterations is less than 0.001. id*: Participant identifier(s). Defaults to NULL, meaning parameters are fitted for all participants (id = unique(data$subject)). n_trials*: Defaults to NULL. Automatically calculates the number of trials per participant. If the dataset contains multiple paradigms with different trial counts, both id and n_trials should be specified. funcs*: Required only if custom models modify one of the six core functions and nc &gt; 1 is set for parallel computing. The function names should be provided as a character vector so the parallel environment can access them. initial_params*: Initial parameter values for iterative algorithms. Defaults to NA, with each parameter initialized at lower + 0.01. It can be set manually in the form of a numeric vector if needed. initial_size*: Initial population size for evolutionary algorithms (default = 50). Larger values can increase computation time. seed*: Random seed for reproducibility.</p><p>After defining necessary arguments, users will obtain the model comparison output. Table <ref type="table" target="#tab_3">3</ref> shows a subset of the example output. Each row represents the optimal parameters for each participant, as well as the corresponding model fit metrics (e.g., ACC and logL), obtained by fitting different models. To facilitate interpretation, model performance across the four-evaluation metrics (ACC, -LogL, AIC, BIC) can be visualized using bar plots (Figure <ref type="figure">6</ref>). Detailed plotting scripts are available on the package's homepage (<ref type="url" target="https://github.com/yuki-961004/binaryRL">https://github.com/yuki- 961004/binaryRL</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head><p>Model Comparison n_trials*: Defaults to NULL. The function will automatically compute the number of trials per participant. If the dataset includes multiple paradigms with varying trial counts, the 'id' and 'n_trials ' should be specified explicitly.</p><p>The resulting output contains many columns, which include the original dataset's columns as well as information on the value updates for each option in every trial. Specifically, it includes the current values of the left and right options, the model's predicted probability of choosing either left or right, the human participant's actual choice (left or right), and additional information. Users can view this example output on the program's homepage (see example result).</p><p>The subsequent latent variable analysis, which is of interest to many researchers, can be conducted in various ways depending on the research questions. For instance, Figure <ref type="figure">7</ref> showed an example of whether the data generated by the model with the best-fitting parameters can reproduce the experimental effects (see example code).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replay the Experimental Effects</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Remark</head><p>This article introduces binaryRL, an R package that provides a tutorial-based framework for constructing and evaluating reinforcement learning models in twoalternative forced-choice tasks. Following the four-stage workflow of <ref type="bibr" target="#b76">Wilson and Collins (2019)</ref>, the package demonstrates how raw behavioral data can be transformed into parameterized models that can be validated, compared, and used for theoretical inference. Together with three canonical models (TD, RSTD, Utility) and a straightforward interface for custom functions, the package offers both a ready-to-use toolkit and a flexible foundation for innovation. In addition, by encapsulating the core RL components-value updating, prediction error signaling, and stochastic action selection-within user-friendly functions, binaryRL lowers the technical barrier for researchers without extensive machine learning expertise to build and test their own RL models. Although the tutorial illustrated the workflow using data from the Gambling Paradigm <ref type="bibr" target="#b44">(Mason et al., 2024)</ref>, the pipeline generalizes to a wide range of TAFC tasks in value-based decision-making, such as learning under uncertainty and cognitive control.</p><p>Despite these strengths, binaryRL has several limitations that users should be aware of. First, the core function (run_m) is limited to two user-specified covariates (var1 and var2) and two free parameters (alpha and beta). This design restricts the inclusion of additional predictors when defining custom models. Second, the core function (run_m) is fundamentally designed for tabular reinforcement learning problems. It is therefore not equipped to handle tasks with much more complex environmental dynamics, such as those involving procedural state transitions, which cannot be fully represented in a static table format (e.g., <ref type="bibr" target="#b19">Eckstein &amp; Collins, 2020;</ref><ref type="bibr" target="#b50">Otto et al., 2013)</ref>. Third, the core function (run_m) conceptually structures the TAFC paradigm into six core modeling functions: the utility function, learning rate function, exploration strategy, bias function, choice probability function, and loss function. A key limitation arises if a user's intended model cannot be constructed or adapted to fit within this six-function framework.</p><p>It is also crucial to recognize that multiple methodological factors influence the parameter estimation process for computational models. The results presented in this tutorial, generated using Maximum Likelihood Estimation (MLE) with specific optimization algorithms, serve primarily for demonstration. In practice, the final parameter estimates can be substantially affected by: (1) the choice of estimation method itself (e.g., MLE, MAP, MCMC, ABC, RNN), which dictates the approach to the loglikelihood or operates outside of it entirely; (2) whether the inference is simulation-based (e.g., on-policy vs. off-policy); (3) the specific optimization algorithm employed; and (4) the number of fitting iterations. As evidenced by <ref type="bibr" target="#b59">Rmus et al. (2024)</ref>, advanced methods such as Artificial Neural Networks (ANNs) offer a compelling alternative by bypassing traditional likelihood calculations and leveraging GPU acceleration for both speed and accuracy. In our tests, Approximate Bayesian Computation (ABC) also achieved superior parameter recovery on our example data. In summary, the binaryRL package provides a convenient foundation for MLE and MAP estimation. However, given the potential limitations of log-likelihood-based methods, we recommend that users also explore alternative approaches that bypass the log-likelihood, as demonstrated in the advanced tutorial on our website.</p><p>In sum, binaryRL is intended as a tutorialized entry point. Its main contribution lies in providing a transparent, reproducible workflow that lowers the barrier for applying reinforcement learning to TAFC tasks, while leaving ample space for researchers to extend, adapt, or embed the package within more elaborate frameworks. We hope that by bridging methodological rigor with practical accessibility, binaryRL will facilitate broader engagement with computational approaches in psychology and inspire future developments in reinforcement learning and cognitive modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Users can install the stable version from CRAN or the latest version from GitHub using the following code:# Install the stable version from CRAN install.packages("binaryRL") # Install the latest version from GitHub remotes::install_github("yuki-961004/binaryRL@*release")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>= "binaryRL.res", value = res, envir = binaryRL.env) loss &lt;-switch(EXPR = estimate, "MLE" = -res$ll, "MAP" = -res$lpo) switch(EXPR = mode, "fit" = loss, "simulate" = res, "replay" = res) }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>each function are provided below: util_func (associated with gamma): Transforms objective reward magnitudes into subjective utilities. The default implementation follows Stevens' power law: u(x) = x^Î³.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>) displays the P(fit model | simulated model), indicating how often data generated by a given model (columns) are best fitted by each candidate model (rows). High diagonal values reflect good model discriminability. The inversion matrix (Figures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 Confusion</head><label>5</label><figDesc>Figure 5 Confusion Matrix and Inversion Matrix under Different Inverse Temperature Settings</figDesc><graphic coords="26,90.00,175.20,432.00,324.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>dplyr::bind_rows(comparison) write.csv(result, "./result_comparison.csv", row.names = FALSE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="7,122.80,105.57,383.15,198.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,151.27,229.65,330.42,199.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="30,90.00,78.70,432.00,142.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="31,90.00,351.70,432.00,142.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Example Dataset from<ref type="bibr" target="#b44">Mason et al.(2024)</ref> </figDesc><table><row><cell cols="4">Subject Block Trial L_choice</cell><cell>R_choice</cell><cell cols="5">L_reward R_reward Sub_Choose Frame NetWorth</cell><cell>RT</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>A</cell><cell>B</cell><cell>36</cell><cell>40</cell><cell>A</cell><cell>Gain</cell><cell>36</cell><cell>1000</cell></row><row><cell>1</cell><cell>1</cell><cell>3</cell><cell>B</cell><cell>A</cell><cell>0</cell><cell>36</cell><cell>B</cell><cell>Gain</cell><cell>36</cell><cell>1100</cell></row><row><cell>1</cell><cell>1</cell><cell>2</cell><cell>C</cell><cell>D</cell><cell>-36</cell><cell>-40</cell><cell>C</cell><cell>Loss</cell><cell>0</cell><cell>1200</cell></row><row><cell>1</cell><cell>1</cell><cell>4</cell><cell>D</cell><cell>C</cell><cell>0</cell><cell>-36</cell><cell>D</cell><cell>Loss</cell><cell>0</cell><cell>1300</cell></row><row><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell></row></table><note><p>Note. L_choice and R_choice indicate the options presented on the left and right, respectively; L_reward and R_reward correspond to the reward values associated with these options; Sub_Choose denotes the participant's selection; Frame indicates the choice context (gain vs. loss); NetWorth is the cumulative reward at the end of the trial; RT represents response time in milliseconds.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Example Output of Model Recovery</figDesc><table><row><cell>simulate_</cell><cell>fit_</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>input_</cell><cell>output_</cell><cell>input_</cell><cell>output_</cell><cell>input_</cell><cell>output_</cell></row><row><cell></cell><cell></cell><cell>iteration</cell><cell>ACC</cell><cell>LL</cell><cell>AIC</cell><cell>BIC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>param_1</cell><cell>param_1</cell><cell>param_2</cell><cell>param_2</cell><cell>param_3</cell><cell>param_3</cell></row><row><cell>TD</cell><cell>TD</cell><cell>1.00</cell><cell>92.50</cell><cell>-81.65</cell><cell>167.3</cell><cell>175.07</cell><cell>0.64</cell><cell>0.76</cell><cell>1.83</cell><cell>1.00</cell><cell>NA</cell><cell>NA</cell></row><row><cell>TD</cell><cell>TD</cell><cell>2.00</cell><cell>93.06</cell><cell>-74.62</cell><cell>153.24</cell><cell>161.01</cell><cell>0.72</cell><cell>0.75</cell><cell>2.44</cell><cell>1.00</cell><cell>NA</cell><cell>NA</cell></row><row><cell>TD</cell><cell>TD</cell><cell>3.00</cell><cell>83.06</cell><cell>-146.57</cell><cell>297.14</cell><cell>304.91</cell><cell>0.95</cell><cell>0.01</cell><cell>3.04</cell><cell>1.01</cell><cell>NA</cell><cell>NA</cell></row><row><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell></row><row><cell>RSTD</cell><cell>RSTD</cell><cell>1.00</cell><cell>93.89</cell><cell>-72.61</cell><cell>151.22</cell><cell>162.88</cell><cell>0.29</cell><cell>0.13</cell><cell>0.72</cell><cell>0.36</cell><cell>2.44</cell><cell>1.69</cell></row><row><cell>RSTD</cell><cell>RSTD</cell><cell>2.00</cell><cell>93.61</cell><cell>-70.17</cell><cell>146.34</cell><cell>158</cell><cell>0.95</cell><cell>0.42</cell><cell>0.21</cell><cell>0.14</cell><cell>1.36</cell><cell>1.45</cell></row><row><cell>RSTD</cell><cell>RSTD</cell><cell>3.00</cell><cell>95.28</cell><cell>-65.47</cell><cell>136.94</cell><cell>148.6</cell><cell>0.54</cell><cell>0.64</cell><cell>0.20</cell><cell>0.19</cell><cell>2.01</cell><cell>4.79</cell></row><row><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell></row><row><cell>Utility</cell><cell>Utility</cell><cell>1.00</cell><cell>62.78</cell><cell>-190.72</cell><cell>387.44</cell><cell>399.1</cell><cell>0.29</cell><cell>0.01</cell><cell>0.72</cell><cell>0.38</cell><cell>2.44</cell><cell>6.00</cell></row><row><cell>Utility</cell><cell>Utility</cell><cell>2.00</cell><cell>60.56</cell><cell>-209.00</cell><cell>424.00</cell><cell>435.66</cell><cell>0.95</cell><cell>0.34</cell><cell>0.21</cell><cell>0.00</cell><cell>1.36</cell><cell>2.94</cell></row><row><cell>Utility</cell><cell>Utility</cell><cell>3.00</cell><cell>60.56</cell><cell>-209.74</cell><cell>425.48</cell><cell>437.14</cell><cell>0.54</cell><cell>0.22</cell><cell>0.20</cell><cell>0.01</cell><cell>2.01</cell><cell>2.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Example Results of Model Comparison</figDesc><table><row><cell cols="4">fit_model Subject ACC LogL</cell><cell>AIC</cell><cell>BIC</cell><cell cols="2">param_1 param_2 param_3</cell></row><row><cell>TD</cell><cell cols="5">1.00 76.74 125.29 254.58 261.91</cell><cell>0.01</cell><cell>0.04 NA</cell></row><row><cell>TD</cell><cell cols="5">2.00 88.89 79.26 162.52 169.85</cell><cell>0.71</cell><cell>0.11 NA</cell></row><row><cell>TD</cell><cell cols="5">3.00 73.61 186.28 376.56 383.89</cell><cell>0.65</cell><cell>0.04 NA</cell></row><row><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦ â€¦</cell></row><row><cell>RSTD</cell><cell cols="5">1.00 78.12 126.09 258.18 269.17</cell><cell>0.14</cell><cell>0.07 0.05</cell></row><row><cell>RSTD</cell><cell cols="5">2.00 94.79 42.05 90.10 101.09</cell><cell>0.66</cell><cell>0.05 0.18</cell></row><row><cell>RSTD</cell><cell cols="5">3.00 77.78 180.16 366.32 377.31</cell><cell>0.95</cell><cell>0.18 0.04</cell></row><row><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦ â€¦</cell></row><row><cell>Utility</cell><cell cols="5">1.00 76.74 125.47 256.94 267.93</cell><cell>0.00</cell><cell>0.36 0.37</cell></row><row><cell>Utility</cell><cell cols="5">2.00 88.89 77.73 161.46 172.45</cell><cell>0.76</cell><cell>0.75 0.26</cell></row><row><cell>Utility</cell><cell cols="5">3.00 75.69 186.31 378.62 389.61</cell><cell>0.68</cell><cell>0.46 0.23</cell></row><row><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦ â€¦</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Note: Figure <ref type="figure">6a</ref> shows the absolute values for accuracy (ACC), negative log-likelihood (-LogL), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC) across all participants. Figure <ref type="figure">6a</ref> normalized the results using a min-max scaling procedure, specifically (Value -Minimum) / (Maximum -Minimum), to transform these into relative scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 4: Latent Variable Analysis and Report Results</head><p>In this step, we demonstrate how to use the rpl_e function to re-insert optimal parameters into a model and example result suitable for plotting risk preferences across different frames. Here, we use the replication of the TD model as an example. The same approach can be applied to the RSTD and Utility models: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1162/CPSY_a_00002</idno>
		<ptr target="https://doi.org/10.1162/CPSY_a_00002" />
	</analytic>
	<monogr>
		<title level="j">Computational Psychiatry</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahn, W.-Y., Haines, N., &amp; Zhang, L. (2017). Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package. Computational Psychiatry, 1(0), 24. https://doi.org/10.1162/CPSY_a_00002</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.1974.1100705</idno>
		<ptr target="https://doi.org/10.1109/TAC.1974.1100705" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716-723. https://doi.org/10.1109/TAC.1974.1100705</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DEoptim: Global Optimization by Differential Evolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ulrich</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=DEoptim" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="2" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ardia, D., Mullen, K., Peterson, B., &amp; Ulrich, J. (2005). DEoptim: Global Optimization by Differential Evolution (p. 2.2-8). https://CRAN.R-project.org/package=DEoptim</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prefrontal cortex and decision making in a mixed-strategy game</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barraclough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1209</idno>
		<ptr target="https://doi.org/10.1038/nn1209" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="404" to="410" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barraclough, D. J., Conroy, M. L., &amp; Lee, D. (2004). Prefrontal cortex and decision making in a mixed-strategy game. Nature Neuroscience, 7(4), 404-410. https://doi.org/10.1038/nn1209</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bendtsen</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=pso" />
		<title level="m">pso: Particle Swarm Optimization</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bendtsen, C. (2010). pso: Particle Swarm Optimization (p. 1.0.4). https://CRAN.R- project.org/package=pso</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">future: Unified Parallel and Distributed Processing in R for Everyone</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bengtsson</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=future" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bengtsson, H. (2015). future: Unified Parallel and Distributed Processing in R for Everyone (p. 1.67.0). https://CRAN.R-project.org/package=future</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">doFuture: Use Foreach to Parallelize via the Future Framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bengtsson</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=doFuture" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bengtsson, H. (2016). doFuture: Use Foreach to Parallelize via the Future Framework (p. 1.1.2). https://CRAN.R-project.org/package=doFuture</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">progressr: An Inclusive, Unifying API for Progress Updates</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bengtsson</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=progressr" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bengtsson, H. (2020). progressr: An Inclusive, Unifying API for Progress Updates (p. 0.15.1). https://CRAN.R-project.org/package=progressr</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exposition of a New Theory on the Measurement of Risk</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bernoulli</surname></persName>
		</author>
		<idno type="DOI">10.2307/1909829</idno>
		<ptr target="https://doi.org/10.2307/1909829" />
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bernoulli, D. (1954). Exposition of a New Theory on the Measurement of Risk. Econometrica, 22(1), 23. https://doi.org/10.2307/1909829</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">mlrMBO: Bayesian Optimization and Model-Based Optimization of Expensive Black-Box Functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bossek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=mlrMBO" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bischl, B., Richter, J., Bossek, J., Horn, D., Lang, M., &amp; Thomas, J. (2017). mlrMBO: Bayesian Optimization and Model-Based Optimization of Expensive Black-Box Functions (p. 1.1.5.1). https://CRAN.R-project.org/package=mlrMBO</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Physics of Optimal Decision Making: A Formal Analysis of Models of Performance in Two-Alternative Forced-Choice Tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moehlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bogacz, R., Brown, E., Moehlis, J., Holmes, P., &amp; Cohen, J. D. (2006). The Physics of Optimal Decision Making: A Formal Analysis of Models of Performance in Two- Alternative Forced-Choice Tasks. Psychological Review.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement Learning, Fast and Slow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2019.02.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2019.02.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="408" to="422" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Botvinick, M., Ritter, S., Wang, J. X., Kurth-Nelson, Z., Blundell, C., &amp; Hassabis, D. (2019). Reinforcement Learning, Fast and Slow. Trends in Cognitive Sciences, 23(5), 408-422. https://doi.org/10.1016/j.tics.2019.02.006</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1606.01540</idno>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1606.01540" />
		<title level="m">OpenAI Gym</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., &amp; Zaremba, W. (2016). OpenAI Gym (No. arXiv:1606.01540). arXiv. https://doi.org/10.48550/arXiv.1606.01540</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Events and Psychophysical Law</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buchsbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.172.3982.502</idno>
		<ptr target="https://doi.org/10.1126/science.172.3982.502" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="502" to="502" />
			<date type="published" when="1971">1971. 3982</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Buchsbaum, M., &amp; Stevens, S. S. (1971). Neural Events and Psychophysical Law. Science, 172(3982), 502-502. https://doi.org/10.1126/science.172.3982.502</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Limited Memory Algorithm for Bound Constrained Optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1137/0916069</idno>
		<ptr target="https://doi.org/10.1137/0916069" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Byrd, R. H., Lu, P., Nocedal, J., &amp; Zhu, C. (1995). A Limited Memory Algorithm for Bound Constrained Optimization. SIAM Journal on Scientific Computing, 16(5), 1190-1208. https://doi.org/10.1137/0916069</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Heterogeneous Coding of Temporally Discounted Values in the Dorsal and Ventral Striatum during Intertemporal Choice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2010.11.041</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2010.11.041" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="170" to="182" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cai, X., Kim, S., &amp; Lee, D. (2011). Heterogeneous Coding of Temporally Discounted Values in the Dorsal and Ventral Striatum during Intertemporal Choice. Neuron, 69(1), 170-182. https://doi.org/10.1016/j.neuron.2010.11.041</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asymmetric reinforcement learning facilitates human inference of transitive relations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ciranka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Linde-Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Padezhki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wicharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spitzer</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-021-01263-w</idno>
		<ptr target="https://doi.org/10.1038/s41562-021-01263-w" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="564" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ciranka, S., Linde-Domingo, J., Padezhki, I., Wicharz, C., Wu, C. M., &amp; Spitzer, B. (2022). Asymmetric reinforcement learning facilitates human inference of transitive relations. Nature Human Behaviour, 6(4), 555-564. https://doi.org/10.1038/s41562- 021-01263-w</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Defining the Neural Mechanisms of Probabilistic Reversal Learning Using Event-Related Functional Magnetic Resonance Imaging</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cools</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Robbins</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.22-11-04563.2002</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.22-11-04563.2002" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4563" to="4567" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cools, R., Clark, L., Owen, A. M., &amp; Robbins, T. W. (2002). Defining the Neural Mechanisms of Probabilistic Reversal Learning Using Event-Related Functional Magnetic Resonance Imaging. The Journal of Neuroscience, 22(11), 4563-4567. https://doi.org/10.1523/JNEUROSCI.22-11-04563.2002</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Upside to Reward Sensitivity: The Hippocampus Supports Enhanced Reinforcement Learning in Adolescence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Davidow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Foerde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>GalvÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shohamy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016.08.031</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2016.08.031" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="99" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Davidow, J. Y., Foerde, K., GalvÃ¡n, A., &amp; Shohamy, D. (2016). An Upside to Reward Sensitivity: The Hippocampus Supports Enhanced Reinforcement Learning in Adolescence. Neuron, 92(1), 93-99. https://doi.org/10.1016/j.neuron.2016.08.031</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computational evidence for hierarchically structured reinforcement learning in humans</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1912330117</idno>
		<ptr target="https://doi.org/10.1073/pnas.1912330117" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">47</biblScope>
			<biblScope unit="page" from="29381" to="29389" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eckstein, M. K., &amp; Collins, A. G. E. (2020). Computational evidence for hierarchically structured reinforcement learning in humans. Proceedings of the National Academy of Sciences, 117(47), 29381-29389. https://doi.org/10.1073/pnas.1912330117</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What do reinforcement learning models measure? Interpreting model parameters in cognition and neuroscience</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wilbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2021.06.004</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2021.06.004" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="128" to="137" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eckstein, M. K., Wilbrecht, L., &amp; Collins, A. G. E. (2021). What do reinforcement learning models measure? Interpreting model parameters in cognition and neuroscience. Current Opinion in Behavioral Sciences, 41, 128-137. https://doi.org/10.1016/j.cobeha.2021.06.004</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rcpp: Seamless R and C++ Integration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eddelbuettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ushey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=Rcpp" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eddelbuettel, D., Francois, R., Allaire, J., Ushey, K., Kou, Q., Russell, N., Ucar, I., Bates, D., &amp; Chambers, J. (2008). Rcpp: Seamless R and C++ Integration (p. 1.1.0). https://CRAN.R-project.org/package=Rcpp</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Risk, Ambiguity, and the Savage Axioms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ellsberg</surname></persName>
		</author>
		<idno type="DOI">10.2307/1884324</idno>
		<ptr target="https://doi.org/10.2307/1884324" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="669" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ellsberg, D. (1961). Risk, Ambiguity, and the Savage Axioms. The Quarterly Journal of Economics, 75(4), 643-669. https://doi.org/10.2307/1884324</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computational Models as Aids to Better Reasoning in Psychology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lewandowsky</surname></persName>
		</author>
		<idno type="DOI">10.1177/0963721410386677</idno>
		<ptr target="https://doi.org/10.1177/0963721410386677" />
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="329" to="335" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Farrell, S., &amp; Lewandowsky, S. (2010). Computational Models as Aids to Better Reasoning in Psychology. Current Directions in Psychological Science, 19(5), 329- 335. https://doi.org/10.1177/0963721410386677</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">doRNG: Generic Reproducible Parallel Backend for &quot;foreach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaujoux</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=doRNG" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>1.8.6.2</note>
	<note type="raw_reference">Gaujoux, R. (2011). doRNG: Generic Reproducible Parallel Backend for &quot;foreach&quot; Loops (p. 1.8.6.2). https://CRAN.R-project.org/package=doRNG</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Posterior predictive assessment of model fitness via realized discrepancies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="733" to="760" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gelman, A., Meng, X.-L., &amp; Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. Statistica Sinica, 6, 733-760.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do learning rates adapt to the distribution of rewards</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-014-0790-3</idno>
		<ptr target="https://doi.org/10.3758/s13423-014-0790-3" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1320" to="1327" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gershman, S. J. (2015). Do learning rates adapt to the distribution of rewards? Psychonomic Bulletin &amp; Review, 22(5), 1320-1327. https://doi.org/10.3758/s13423- 014-0790-3</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GenSA: R Functions for Generalized Simulated Annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gubian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sa</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=GenSA" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gubian, S., Xiang, Y., Brian, S., Hoeng, J., &amp; SA, P. (2011). GenSA: R Functions for Generalized Simulated Annealing (p. 1.1.14.1). https://CRAN.R- project.org/package=GenSA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Role of the Ventromedial Prefrontal Cortex in Abstract State-Based Inference during Decision Making in Humans</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bossaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1010-06.2006</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.1010-06.2006" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page" from="8360" to="8367" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hampton, A. N., Bossaerts, P., &amp; O&apos;Doherty, J. P. (2006). The Role of the Ventromedial Prefrontal Cortex in Abstract State-Based Inference during Decision Making in Humans. The Journal of Neuroscience, 26(32), 8360-8367. https://doi.org/10.1523/JNEUROSCI.1010-06.2006</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The speed-accuracy tradeoff: History, physiology, methodology, and behavior</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Heitz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2014.00150</idno>
		<ptr target="https://doi.org/10.3389/fnins.2014.00150" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Heitz, R. P. (2014). The speed-accuracy tradeoff: History, physiology, methodology, and behavior. Frontiers in Neuroscience, 8. https://doi.org/10.3389/fnins.2014.00150</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The relation of the time of a judgment to its accuracy</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A C</forename><surname>Henmon</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0074579</idno>
		<ptr target="https://doi.org/10.1037/h0074579" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="201" />
			<date type="published" when="1911">1911</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Henmon, V. A. C. (1911). The relation of the time of a judgment to its accuracy. Psychological Review, 18(3), 186-201. https://doi.org/10.1037/h0074579</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decisions From Experience and the Effect of Rare Events in Risky Choice</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">U</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="534" to="539" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hertwig, R., Barron, G., Weber, E. U., &amp; Erev, I. (2004). Decisions From Experience and the Effect of Rare Events in Risky Choice. Psychological Science, 15(8), 534-539.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Acme: A Research Framework for Distributed Reinforcement Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Momchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sinopalnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>StaÅ„czyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jacq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2006.00979</idno>
		<idno type="arXiv">arXiv:2006.00979</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2006.00979" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hoffman, M. W., Shahriari, B., Aslanides, J., Barth-Maron, G., Momchev, N., Sinopalnikov, D., StaÅ„czyk, P., Ramos, S., Raichuk, A., Vincent, D., Hussenot, L., Dadashi, R., Dulac-Arnold, G., Orsini, M., Jacq, A., Ferret, J., Vieillard, N., Ghasemipour, S. K. S., Girgin, S., â€¦ Freitas, N. de. (2022). Acme: A Research Framework for Distributed Reinforcement Learning (No. arXiv:2006.00979). arXiv. https://doi.org/10.48550/arXiv.2006.00979</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Validation of Decision-Making Models and Analysis of Decision Variables in the Rat Basal Ganglia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.6157-08.2009</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.6157-08.2009" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="9861" to="9874" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ito, M., &amp; Doya, K. (2009). Validation of Decision-Making Models and Analysis of Decision Variables in the Rat Basal Ganglia. The Journal of Neuroscience, 29(31), 9861-9874. https://doi.org/10.1523/JNEUROSCI.6157-08.2009</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prospect Theory: An Analysis of Decision Under Risk</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<idno type="DOI">10.1142/9789814417358_0006</idno>
		<ptr target="https://www.worldscientific.com/doi/abs/10.1142/9789814417358_0006" />
	</analytic>
	<monogr>
		<title level="m">World Scientific Handbook in Financial Economics Series</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Maclean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Ziemba</surname></persName>
		</editor>
		<imprint>
			<publisher>WORLD SCIENTIFIC</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="99" to="127" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kahneman, D., &amp; Tversky, A. (1979). Prospect Theory: An Analysis of Decision Under Risk. In L. C. MacLean &amp; W. T. Ziemba, World Scientific Handbook in Financial Economics Series (Vol. 4, pp. 99-127). WORLD SCIENTIFIC. https://www.worldscientific.com/doi/abs/10.1142/9789814417358_0006</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The neural code of reward anticipation in human orbitofrontal cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kahnt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0912838107</idno>
		<ptr target="https://doi.org/10.1073/pnas.0912838107" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="6010" to="6015" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kahnt, T., Heinzle, J., Park, S. Q., &amp; Haynes, J.-D. (2010). The neural code of reward anticipation in human orbitofrontal cortex. Proceedings of the National Academy of Sciences, 107(13), 6010-6015. https://doi.org/10.1073/pnas.0912838107</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Experimental psychology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Kantowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Elmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Wadsworth, Cengage Learning</publisher>
		</imprint>
	</monogr>
	<note>9th ed</note>
	<note type="raw_reference">Kantowitz, B. H., Roediger, H. L., &amp; Elmes, D. G. (2009). Experimental psychology (9th ed). Wadsworth, Cengage Learning.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Krajbich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1101328108</idno>
		<ptr target="https://doi.org/10.1073/pnas.1101328108" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page" from="13852" to="13857" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krajbich, I., &amp; Rangel, A. (2011). Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions. Proceedings of the National Academy of Sciences, 108(33), 13852-13857. https://doi.org/10.1073/pnas.1101328108</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Value Representations in the Primate Striatum during Matching Behavior</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2008.02.021</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2008.02.021" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="463" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lau, B., &amp; Glimcher, P. W. (2008). Value Representations in the Primate Striatum during Matching Behavior. Neuron, 58(3), 451-463. https://doi.org/10.1016/j.neuron.2008.02.021</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural Basis of Reinforcement Learning and Decision Making</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-neuro-062111-150512</idno>
		<ptr target="https://doi.org/10.1146/annurev-neuro-062111-150512" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee, D., Seo, H., &amp; Jung, M. W. (2012). Neural Basis of Reinforcement Learning and Decision Making. Annual Review of Neuroscience, 35(1), 287-308. https://doi.org/10.1146/annurev-neuro-062111-150512</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Modeling and Interpreting Real-world Human Risk Decision Making with Inverse Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1906.05803</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1906.05803" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, Q., Wu, H., &amp; Liu, A. (2019). Modeling and Interpreting Real-world Human Risk Decision Making with Inverse Reinforcement Learning. arXiv. https://doi.org/10.48550/ARXIV.1906.05803</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regret Theory: An Alternative Theory of Rational Choice Under Uncertainty</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sugden</surname></persName>
		</author>
		<idno type="DOI">10.2307/2232669</idno>
		<ptr target="https://doi.org/10.2307/2232669" />
	</analytic>
	<monogr>
		<title level="j">The Economic Journal</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">368</biblScope>
			<biblScope unit="page">805</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Loomes, G., &amp; Sugden, R. (1982). Regret Theory: An Alternative Theory of Rational Choice Under Uncertainty. The Economic Journal, 92(368), 805. https://doi.org/10.2307/2232669</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Asymmetric and adaptive reward coding via normalized reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Louie</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1010350</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1010350" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1010350</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Louie, K. (2022). Asymmetric and adaptive reward coding via normalized reinforcement learning. PLOS Computational Biology, 18(7), e1010350. https://doi.org/10.1371/journal.pcbi.1010350</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Of Black Swans and Tossed Coins: Is the Description-Experience Gap in Risky Choice Limited to Rare Events?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Spetch</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0020262</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0020262" />
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">20262</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ludvig, E. A., &amp; Spetch, M. L. (2011). Of Black Swans and Tossed Coins: Is the Description-Experience Gap in Risky Choice Limited to Rare Events? PLoS ONE, 6(6), e20262. https://doi.org/10.1371/journal.pone.0020262</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rare and extreme outcomes in risky choice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Spetch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Madan</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-023-02415-x</idno>
		<ptr target="https://doi.org/10.3758/s13423-023-02415-x" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mason, A., Ludvig, E. A., Spetch, M. L., &amp; Madan, C. R. (2024). Rare and extreme outcomes in risky choice. Psychonomic Bulletin &amp; Review. https://doi.org/10.3758/s13423-023-02415-x</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">foreach: Provides Foreach Looping Construct</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=foreach" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Microsoft, &amp; Weston, S. (2009). foreach: Provides Foreach Looping Construct (p. 1.5.2). https://CRAN.R-project.org/package=foreach</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Risk-Sensitive Reinforcement Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mihatsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neuneier</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1017940631555</idno>
		<ptr target="https://doi.org/10.1023/A:1017940631555" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mihatsch, O., &amp; Neuneier, R. (2002). Risk-Sensitive Reinforcement Learning. https://doi.org/10.1023/A:1017940631555</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reliability of gamified reinforcement learning in densely sampled longitudinal assessments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Neuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>KÃ¼hnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>KrÃ¤utlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Teckentrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Kroemer</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pdig.0000330</idno>
		<ptr target="https://doi.org/10.1371/journal.pdig.0000330" />
	</analytic>
	<monogr>
		<title level="j">PLOS Digital Health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Neuser, M. P., KÃ¼hnel, A., KrÃ¤utlein, F., Teckentrup, V., Svaldi, J., &amp; Kroemer, N. B. (2023). Reliability of gamified reinforcement learning in densely sampled longitudinal assessments. PLOS Digital Health, 2(9), e0000330. https://doi.org/10.1371/journal.pdig.0000330</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural Prediction Errors Reveal a Risk-Sensitive Reinforcement-Learning Process in the Human Brain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.5498-10.2012</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.5498-10.2012" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="551" to="562" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Niv, Y., Edlund, J. A., Dayan, P., &amp; O&apos;Doherty, J. P. (2012). Neural Prediction Errors Reveal a Risk-Sensitive Reinforcement-Learning Process in the Human Brain. The Journal of Neuroscience, 32(2), 551-562. https://doi.org/10.1523/JNEUROSCI.5498-10.2012</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A learning mechanism shaping risk preferences and a preliminary test of its relationship with psychopathic traits</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohira</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-00358-8</idno>
		<ptr target="https://doi.org/10.1038/s41598-021-00358-8" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20853</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oba, T., Katahira, K., &amp; Ohira, H. (2021). A learning mechanism shaping risk preferences and a preliminary test of its relationship with psychopathic traits. Scientific Reports, 11(1), 20853. https://doi.org/10.1038/s41598-021-00358-8</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Workingmemory capacity protects model-based learning from stress</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Raio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1312011110</idno>
		<ptr target="https://doi.org/10.1073/pnas.1312011110" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="20941" to="20946" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Otto, A. R., Raio, C. M., Chiang, A., Phelps, E. A., &amp; Daw, N. D. (2013). Working- memory capacity protects model-based learning from stress. Proceedings of the National Academy of Sciences, 110(52), 20941-20946. https://doi.org/10.1073/pnas.1312011110</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neurons in the orbitofrontal cortex encode economic value</title>
		<author>
			<persName><forename type="first">C</forename><surname>Padoa-Schioppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Assad</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature04676</idno>
		<ptr target="https://doi.org/10.1038/nature04676" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="page" from="223" to="226" />
			<date type="published" when="2006">2006. 7090</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Padoa-Schioppa, C., &amp; Assad, J. A. (2006). Neurons in the orbitofrontal cortex encode economic value. Nature, 441(7090), 223-226. https://doi.org/10.1038/nature04676</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The computational roots of positivity and confirmation biases in reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebreton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2022.04.005</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2022.04.005" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="607" to="621" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Palminteri, S., &amp; Lebreton, M. (2022). The computational roots of positivity and confirmation biases in reinforcement learning. Trends in Cognitive Sciences, 26(7), 607-621. https://doi.org/10.1016/j.tics.2022.04.005</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Importance of Falsification in Computational Cognitive Modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koechlin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2017.03.011</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2017.03.011" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="425" to="433" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Palminteri, S., Wyart, V., &amp; Koechlin, E. (2017). The Importance of Falsification in Computational Cognitive Modeling. Trends in Cognitive Sciences, 21(6), 425-433. https://doi.org/10.1016/j.tics.2017.03.011</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">ReinforcementLearning: Model-Free Reinforcement Learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Proellochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feuerriegel</surname></persName>
		</author>
		<idno type="DOI">10.32614/CRAN.package</idno>
		<ptr target="https://doi.org/10.32614/CRAN.package" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Proellochs, N., &amp; Feuerriegel, S. (2017). ReinforcementLearning: Model-Free Reinforcement Learning. https://doi.org/10.32614/CRAN.package.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<orgName type="collaboration">ReinforcementLearning R Core Team</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">ReinforcementLearning R Core Team. (2020). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Comparison of Sequential Sampling Models for Two-Choice Reaction Time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.111.2.333</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.111.2.333" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="367" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ratcliff, R., &amp; Smith, P. L. (2004). A Comparison of Sequential Sampling Models for Two-Choice Reaction Time. Psychological Review, 111(2), 333-367. https://doi.org/10.1037/0033-295X.111.2.333</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and non-reinforcement</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rescorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Classical Conditioning, Current Research and Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="64" to="69" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rescorla, R. A., &amp; Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and non-reinforcement. Classical Conditioning, Current Research and Theory, 2, 64--69.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fuzzy-trace theory: An interim synthesis</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brainerd</surname></persName>
		</author>
		<idno type="DOI">10.1016/1041-6080(95)90031-4</idno>
		<ptr target="https://doi.org/10.1016/1041-6080(95)90031-4" />
	</analytic>
	<monogr>
		<title level="j">Learning and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="75" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Reyna, V. F., &amp; Brainerd, C. J. (1995). Fuzzy-trace theory: An interim synthesis. Learning and Individual Differences, 7(1), 1-75. https://doi.org/10.1016/1041- 6080(95)90031-4</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Artificial neural networks for model identification and parameter estimation in computational cognitive models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1012119</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1012119" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1012119</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rmus, M., Pan, T.-F., Xia, L., &amp; Collins, A. G. E. (2024). Artificial neural networks for model identification and parameter estimation in computational cognitive models. PLOS Computational Biology, 20(5), e1012119. https://doi.org/10.1371/journal.pcbi.1012119</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Some Aspects Of the Sequential Design of Experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<idno type="DOI">10.1090/S0002-9904-1952-09620-8</idno>
		<ptr target="https://doi.org/10.1090/S0002-9904-1952-09620-8" />
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="527" to="535" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Robbins, H. (1952). Some Aspects Of the Sequential Design of Experiments. Bulletin of the American Mathematical Society, 58, 527-535. https://doi.org/10.1090/S0002- 9904-1952-09620-8</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Prediction Error and Its Estimation for Subset-Selected Models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Roecker</surname></persName>
		</author>
		<idno type="DOI">10.1080/00401706.1991.10484873</idno>
		<ptr target="https://doi.org/10.1080/00401706.1991.10484873" />
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Roecker, E. B. (1991). Prediction Error and Its Estimation for Subset-Selected Models. Technometrics, 33(4), 459-468. https://doi.org/10.1080/00401706.1991.10484873</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Grassie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hartley</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.64620</idno>
		<ptr target="https://doi.org/10.7554/eLife.64620" />
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">64620</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rosenbaum, G. M., Grassie, H. L., &amp; Hartley, C. A. (2022). Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory. eLife, 11, e64620. https://doi.org/10.7554/eLife.64620</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Estimating the Dimension of a Model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
		<ptr target="https://doi.org/10.1214/aos/1176344136" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schwarz, G. (1978). Estimating the Dimension of a Model. The Annals of Statistics, 6(2). https://doi.org/10.1214/aos/1176344136</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">GA: Genetic Algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Scrucca</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=GA" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Scrucca, L. (2012). GA: Genetic Algorithms (p. 3.2.4). https://CRAN.R- project.org/package=GA</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">How similarity between choice options affects decisions from experience: The accentuation-of-differences model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Spektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fontanesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000122</idno>
		<ptr target="https://doi.org/10.1037/rev0000122" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="88" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Spektor, M. S., Gluth, S., Fontanesi, L., &amp; Rieskamp, J. (2019). How similarity between choice options affects decisions from experience: The accentuation-of-differences model. Psychological Review, 126(1), 52-88. https://doi.org/10.1037/rev0000122</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Studies of interference in serial verbal reactions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Stroop</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0054651</idno>
		<ptr target="https://doi.org/10.1037/h0054651" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="662" />
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Stroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental Psychology, 18(6), 643-662. https://doi.org/10.1037/h0054651</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Reinforcement Learning and its Connections with Neuroscience and Psychology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitlangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Baths</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2021.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2021.10.003" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="271" to="287" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Subramanian, A., Chitlangia, S., &amp; Baths, V. (2022). Reinforcement Learning and its Connections with Neuroscience and Psychology. Neural Networks, 145, 271-287. https://doi.org/10.1016/j.neunet.2021.10.003</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/B9781558601413500304" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings 1990</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton, R. S. (1990). Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. In Machine Learning Proceedings 1990 (pp. 216-224). Elsevier. https://linkinghub.elsevier.com/retrieve/pii/B9781558601413500304</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton, R. S., &amp; Barto, A. G. (1998). Reinforcement learning: An introduction. MIT Press.</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Second edition</note>
	<note type="raw_reference">Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">cmaes: Covariance Matrix Adapting Evolutionary Strategy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mersmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arnu</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=cmaes" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="0" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Trautmann, H., Mersmann, O., &amp; Arnu, D. (2010). cmaes: Covariance Matrix Adapting Evolutionary Strategy (p. 1.0-12). https://CRAN.R-project.org/package=cmaes</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Designing optimal behavioral experiments using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kleinegesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>SeriÃ¨s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lucas</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.86224</idno>
		<ptr target="https://doi.org/10.7554/eLife.86224" />
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">86224</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Valentin, S., Kleinegesse, S., Bramley, N. R., SeriÃ¨s, P., Gutmann, M. U., &amp; Lucas, C. G. (2024). Designing optimal behavioral experiments using machine learning. eLife, 13, e86224. https://doi.org/10.7554/eLife.86224</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Latent motives guide structure learning during adaptive social choice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Feldmanhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="404" to="414" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">van Baar, J. M., Nassar, M. R., Deng, W., &amp; FeldmanHall, O. (2022). Latent motives guide structure learning during adaptive social choice. Nature Human Behaviour, 6, 404-414.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Diminished reinforcement sensitivity in adolescence is associated with enhanced response switching and reduced coding of choice probability in the medial frontal pole</title>
		<author>
			<persName><forename type="first">M</forename><surname>Waltmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M F</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deserno</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dcn.2023.101226</idno>
		<ptr target="https://doi.org/10.1016/j.dcn.2023.101226" />
	</analytic>
	<monogr>
		<title level="j">Developmental Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101226</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Waltmann, M., Herzog, N., Reiter, A. M. F., Villringer, A., Horstmann, A., &amp; Deserno, L. (2023). Diminished reinforcement sensitivity in adolescence is associated with enhanced response switching and reduced coding of choice probability in the medial frontal pole. Developmental Cognitive Neuroscience, 60, 101226. https://doi.org/10.1016/j.dcn.2023.101226</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Tianshou: A Highly Modularized Deep Reinforcement Learning Library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duburcq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang, M., Su, Y., Su, H., &amp; Zhu, J. (2022). Tianshou: A Highly Modularized Deep Reinforcement Learning Library.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Ten simple rules for the computational modeling of behavioral data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.49547</idno>
		<ptr target="https://doi.org/10.7554/eLife.49547" />
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">49547</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wilson, R. C., &amp; Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, e49547. https://doi.org/10.7554/eLife.49547</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The effect of foregone payoffs on underweighting small probability events</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yechiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.509</idno>
		<ptr target="https://doi.org/10.1002/bdm.509" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yechiam, E., &amp; Busemeyer, J. R. (2006). The effect of foregone payoffs on underweighting small probability events. Journal of Behavioral Decision Making, 19(1), 1-16. https://doi.org/10.1002/bdm.509</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Using reinforcement learning models in social neuroscience: Frameworks, pitfalls and suggestions of best practices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ypma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lengersdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mikus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>GlÃ¤scher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lamm</surname></persName>
		</author>
		<idno type="DOI">10.1093/scan/nsaa089</idno>
		<ptr target="https://doi.org/10.1093/scan/nsaa089" />
	</analytic>
	<monogr>
		<title level="j">Social Cognitive and Affective Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="695" to="707" />
			<date type="published" when="2011">2011. 2020</date>
		</imprint>
	</monogr>
	<note>nloptr: R Interface to NLopt (p. 2.2.1)</note>
	<note type="raw_reference">Ypma, J., &amp; Johnson, S. G. (2011). nloptr: R Interface to NLopt (p. 2.2.1). https://CRAN.R-project.org/package=nloptr Zhang, L., Lengersdorff, L., Mikus, N., GlÃ¤scher, J., &amp; Lamm, C. (2020). Using reinforcement learning models in social neuroscience: Frameworks, pitfalls and suggestions of best practices. Social Cognitive and Affective Neuroscience, 15(6), 695-707. https://doi.org/10.1093/scan/nsaa089</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
