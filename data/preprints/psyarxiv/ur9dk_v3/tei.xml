<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The ABC of Heuristics: Rules of Thumb as Likelihood-free Approximate Bayesian Computation</title>
				<funder ref="#_cpNHkar">
					<orgName type="full">British Academy</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianwei</forename><surname>Gong</surname></persName>
							<email>t-gong@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Valentin</surname></persName>
							<email>s.valentin@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
							<email>neil.bramley@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">G</forename><surname>Lucas</surname></persName>
							<email>c.lucas@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The ABC of Heuristics: Rules of Thumb as Likelihood-free Approximate Bayesian Computation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5DC83F966200FC5B51B1617CE5A7DF77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-13T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Approximate Bayesian computation</term>
					<term>Heuristics</term>
					<term>likelihood-free inference</term>
					<term>rejection sampling</term>
					<term>bounded rationality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heuristics have long been viewed as simple decision rules that sacrifice optimality for cognitive efficiency. However, this perspective fails to fully account for their effectiveness in complex, real-world environments. This paper proposes a reinterpretation of heuristics as realizations of a universal algorithm for likelihood-free approximation to Bayesian inference, bridging the gap between heuristic and normative approaches to decision-making. We argue that many heuristic choice patterns are the consequence of a robust and adaptive inference process that involves simulating from one's prior and using low-dimensional summary statistics (cues) to compare with observed data. This process sidesteps the typically-intractable Bayesian likelihood calculation, while still approximating the optimum conditional on the effectiveness of chosen summary statistics. We illustrate how this allows heuristics to be situated within a probabilistic framework via this process. Our proposal addresses several key puzzles in the field: It relaxes strong assumptions about cue-target relationships in heuristic models, accounts for human stochasticity in heuristic decision-making, and invites a more appropriate benchmark for evaluating heuristics' performance. We provide two illustrative examples -a classic cue-based decision task and an ecological causal learning task, to demonstrate the practical applicability of our framework.</p><p>By unifying heuristics and Bayesian inference through the lens of likelihood-free methods, this work provides a more nuanced understanding of human inference and decision-making.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heuristics are defined as "rules of thumb", or "mental shortcuts", that allow people to make timely inferences or advantageous choices in familiar settings without expending excessive cognitive resources <ref type="bibr" target="#b107">(Simon, 1945)</ref>. For the past half century, a long and seemingly ever-growing list of heuristics has been proposed. These heuristics have typically been held as explanations for patterns of human behavior in tasks that resemble familiar kinds of real-world inference or decision making. A common theme is that they reduce effort and work surprisingly well in some settings, while they can also be pushed to failure by careful experimental manipulations <ref type="bibr" target="#b40">(Gigerenzer &amp; Todd, 1999;</ref><ref type="bibr" target="#b104">Shah &amp; Oppenheimer, 2008;</ref><ref type="bibr" target="#b107">Simon, 1945;</ref><ref type="bibr" target="#b119">Tversky &amp; Kahneman, 1974)</ref>. Human empirical data have been aligned with both the successes and failures of heuristics, showing evidence that many proposed heuristics have explanatory power that goes beyond that of the normative models they are held up as alternatives to.</p><p>Shortcuts, or cognitively economical inference strategies, are arguably essential; reality often requires us to solve complex problems in real time <ref type="bibr" target="#b19">(Christiansen &amp; Chater, 2016)</ref> while relying on incomplete information <ref type="bibr" target="#b92">(Oaksford &amp; Chater, 2007)</ref>. This has long been a challenge to rational, normative characterizations of those problems, which are typically very computationally expensive <ref type="bibr" target="#b59">(Jones &amp; Love, 2011;</ref><ref type="bibr" target="#b82">Marr, 1982)</ref>. In fact, for many everyday problems that we solve heuristically, optimal solutions are not merely costly to compute but intractable in practice <ref type="bibr" target="#b107">(Simon, 1945;</ref><ref type="bibr" target="#b125">Van Rooij, 2008)</ref>. <ref type="foot" target="#foot_0">1</ref>Even though the theory of heuristics provides a good explanation for the human ability to make rapid and reasonably accurate inferences, it says little about another hallmark of human cognition: flexibility. People have a remarkable ability to adapt their strategies when existing methods prove ineffective. When pushed far enough, people can abandon a malfunctioning heuristic and find a better approach <ref type="bibr" target="#b90">(Newell, Weston, &amp; Shanks, 2003)</ref>. The heuristics literature, nonetheless, often focuses on fixed, inflexible strategies for making inferences and decisions. At the same same time, this adaptiveness is not fully explained by a rational Bayesian perspective. Proposals about optimal strategy selection via rational meta-reasoning <ref type="bibr" target="#b74">(Lieder &amp; Griffiths, 2017</ref><ref type="bibr">, 2020)</ref> only go so far because they tend to assume that developmental or evolutionary-scale process produce strategies and an apparatus for understanding when to apply which one. This leaves us with a similar static toolkit to what is suggested by the heuristics literature, and seems at once reasonable and incomplete when it comes to the human ability to give snap answers to unfamiliar questions.</p><p>In this paper, we propose a new interpretation of heuristics by highlighting a deep commonality between the abstraction to simple indirect cues -the hallmark of heuristics literature -and the mathematical principles of likelihood-free inference. In particular, we focus on how, beyond the "lamplight" of small-world tasks and closedform models, effective inferences about different complex domains can be flexibly made by comparing observations with the outputs of mental simulations based on abstracted simple, low-dimensional cues. This mathematical approach is known as Approximate Bayesian Computation (ABC), and we will address how the use of heuristics in the cognitive literature aligns with it. Ours is not the first attempt to unify the literature on heuristics with notions of approximate rationality (cf. <ref type="bibr" target="#b6">Binz, Gershman, Schulz, &amp; Endres, 2022;</ref><ref type="bibr" target="#b24">Dasgupta, Schulz, Tenenbaum, &amp; Gershman, 2020;</ref><ref type="bibr" target="#b74">Lieder &amp; Griffiths, 2017)</ref>. However, we specifically advance the perspective that heuristics can be viewed as a form of likelihood-free inference, and that the ABC framework can be used to reinterpret the process of employing heuristics. We argue that the heuristics-aslikelihood-free-inference perspective offers a unique degree of generality and unlocks new insights into the functioning of the mind.</p><p>To foreshadow the advantages in using ABC to reinterpret heuristics, we highlight three challenges to previous research into heuristics that potentially undermine their power to explain human behavior: First, heuristic decision-making and inference -as typically conceived -relies on strong assumptions about the relation between cues (observable features used to make judgments) and targets (outcomes or variables being predicted). This limits their applicability to real-world problems, as it is unclear how the validity of these assumptions can be established and monitored in dynamic, complex environments where relationships may shift. Second, many standard accounts of heuristic decision-making and inference do not explicitly model either the systematic stochasticity at the aggregate level or uncertainty at the individual level. This stands in contrast to evidence showing systematic patterns of stochasticity in human responses, and people's explicit and implicit consideration of uncertainty when combining cues and making risky decisions. The nature of this uncertainty representation in heuristic reasoning remains subject to ongoing debate <ref type="bibr" target="#b102">(Sanborn &amp; Chater, 2016;</ref><ref type="bibr" target="#b115">Szollosi, Donkin, &amp; Newell, 2023)</ref>. Third, the "optimal" benchmarks to which many heuristics have traditionally been compared -weighted attribute averaging or linear regression -provide only a limited standard. A fairer evaluation would compare heuristics against models that are plausibly Bayes-optimal and grounded in reasonable priors.</p><p>Our perspective will preserve the central idea of heuristics while providing a means to address these challenges. Specifically, we will show heuristics are not independent rules, but particular summary statistics that, when combined with an ABC procedure, enable solutions to difficult or intractable inference problems, that can boundedly rational if the summary statistics are selected and used appropriately. The strong assumption about cue-target relationships can then be relaxed by viewing heuristics as flexible approximations to complex Bayesian computations. Besides, this offers an account of implicit uncertainty representation via the probabilistic framework of inference-by-sampling. As we will show later, sampling procedures within ABC can naturally capture the variability of human responses, without the need to impose an additional decision rule (e.g., the softmax rule). Finally, by understanding heuristics under the approximate Bayesian approach, it becomes easier and more straightforward to use rational Bayesian inference as the benchmark for comparison. By reconceptualizing heuristics as likelihood-free approximations to Bayesian inference, we aim to provide a more holistic framework for understanding human decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristics via the Lens of Cognitive Economy</head><p>From shortcuts to strategies Heuristics, defined as the "shortcuts" to more elaborate solutions <ref type="bibr" target="#b109">(Simon et al., 1990)</ref>, can lead people to make imperfect inferences or decisions. Tversky and Kahneman's work on "heuristics and biases" documented cases in which human decision making is suboptimal but consistent with simple heuristics <ref type="bibr" target="#b61">(Kahneman, 2011;</ref><ref type="bibr" target="#b64">Kahneman &amp; Tversky, 1973;</ref><ref type="bibr" target="#b119">Tversky &amp; Kahneman, 1974)</ref>. This includes examples such as favoring options that are more easily brought to mind (availability heuristic), more recently encountered (recency bias), or closer to an arbitrary anchor value (anchoring), categorizing someone based on how closely their description matches a category's salient features, while overlooking other possibilities (representativeness heuristic). When people are asked to make decisions in artificial scenarios based on a bounded amount of newly learned evidence, they are found to combine multiple sources of evidence, or cues, with apparent neglect to their relative strengths (Dawe's rule, <ref type="bibr" target="#b27">Dawes &amp; Corrigan, 1974)</ref> or by completely neglecting all but the strongest one cue (take-the-best, <ref type="bibr" target="#b38">Gigerenzer, Hoffrage, &amp; Kleinbölting, 1991)</ref>. Experimental manipulations of option sets can lead followers of such heuristics to systematically sub-optimal judgments and decisions, a fact that a generation of marketing, business, bookmaking and political groups have exploited <ref type="bibr" target="#b58">(John, Caldwell, McCoy, &amp; Braganza, 2024;</ref><ref type="bibr" target="#b88">Newall, 2019;</ref><ref type="bibr" target="#b117">Thaler &amp; Sunstein, 2009)</ref>.</p><p>While the first wave of heuristics research primarily highlighted the potential pitfalls arising from humans' reliance on heuristics, it also revealed how deeply such reasoning patterns are embedded in human cognition. As pointed out by Herbert Simon, heuristics serve to provide satisfactory solutions in the face of limited resources <ref type="bibr" target="#b109">(Simon et al., 1990)</ref>. Accordingly, the second wave of research emphasized that heuristics, when applied in their natural contexts, can be "fast and frugal" and "make us smart" <ref type="bibr" target="#b37">(Gigerenzer &amp; Goldstein, 1996;</ref><ref type="bibr" target="#b40">Gigerenzer &amp; Todd, 1999)</ref>. Behind these broad arguments were several more specific claims about heuristics. For example, they encode ecologically appropriate inductive biases <ref type="bibr" target="#b11">(Brighton &amp; Gigerenzer, 2012;</ref><ref type="bibr" target="#b36">Gigerenzer &amp; Brighton, 2009;</ref><ref type="bibr" target="#b118">Todd &amp; Gigerenzer, 2012)</ref> and embody forms of expert knowledge by distilling what truly matters and ignoring irrelevant or redundant information <ref type="bibr" target="#b83">(Martignon, Vitouch, Takezawa, &amp; Forster, 2003)</ref>. At its zenith, research sometimes argued that heuristics could actually produce more accurate and more robust decisions than more complex, complete statistical inference benchmarks <ref type="bibr" target="#b42">(Goldstein &amp; Gigerenzer, 2008)</ref>. For instance, the "take-the-best" heuristic, which makes decisions based solely on the most discriminative cue, was argued to outperform regression in predicting real-world features such as which of two cities is larger <ref type="bibr" target="#b37">(Gigerenzer &amp; Goldstein, 1996)</ref>. Adopting an 1/N distribution of investments (allocating 1/N of wealth to each of the N available assets) was argued to match or outperform more sophisticated investment strategies on average <ref type="bibr" target="#b29">(DeMiguel, Garlappi, &amp; Uppal, 2009;</ref><ref type="bibr" target="#b53">Hertwig &amp; Hoffrage, 2013)</ref>. Subsequent re-analyses have revealed that these "less-ismore" effects are due to either heuristics introducing good biases, i.e., encoding prior information unavailable to the statistical comparison model, or avoiding over-fitting to noise <ref type="bibr" target="#b94">(Parpart, Jones, &amp; Love, 2018)</ref>. However, the legacy of the fast and frugal heuristics program has been a field-wide recognition of the adaptive value, and indeed the necessity, of approximation strategies and inductive biases, now often discussed under the headings of learning to learn, meta-learning, or amortization (essentially forms of pre-computation and reuse) strategies <ref type="bibr" target="#b5">(Binz et al., 2023;</ref><ref type="bibr" target="#b24">Dasgupta et al., 2020;</ref><ref type="bibr" target="#b66">Kemp, Goodman, &amp; Tenenbaum, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cues (attribute substitution) in heuristics</head><p>At the core of heuristic decision-making is the concept of "cues" -features that guide our inferences, judgments and choices. Importantly, our use of these cues often involves what is called "attribute substitution", assessing an easier or more salient attribute in place of a more difficult target one <ref type="bibr" target="#b58">(John et al., 2024;</ref><ref type="bibr" target="#b62">Kahneman, Frederick, et al., 2002)</ref>: Rather than using raw sensory inputs, these cues often represent abstractions from our perceptual experiences, distilling complex information into more manageable forms.</p><p>Consider how we navigate our everyday environments. When choosing a restaurant in an unfamiliar city, we don't process every detail about each establishment. Instead, we might focus on a few key cues: how busy the restaurant is and its exterior appearance. Each of these cues is a coarse-grained reflection of a wealth of underlying information. Similarly, when judging the quality of a yogurt brand, we might use its price as a cue. The price, in this case, serves as a summary of a collection of attributes, potentially encoding information about ingredients, production processes, and market positioning. Or when predicting which team will win a baseball match, a powerful approach has proven to be the calculation of the batters' run creation average: (Hits + Walks) × Total Bases <ref type="bibr" target="#b73">(Lewis, 2004)</ref>, as a cue that effectively summarizes a complex history of performances into a single, easily digestible statistic.</p><p>The way we abstract and use cues can vary widely depending on the context and our prior experiences. Sometimes, we primarily use a single, highly predictive cue (as in the "take-the-best" heuristic). Other times, we might combine multiple cues, either equally (as in <ref type="bibr">Dawes' rule;</ref><ref type="bibr" target="#b27">Dawes &amp; Corrigan, 1974)</ref> or with simplified weighting schemes <ref type="bibr" target="#b135">(Zhang, 2004)</ref>. Notably, the way we apply these heuristic features outside of toy cases may rarely be a matter of a monotonic "more is better". We might pick whatever wine falls closest to the price we had anticipated paying, go to the restaurant that is busy but not too busy, or predict the winning team to be the one whose characteristics are, overall, closer to those we imagine a winning team having (e.g. strikeout rates for pitchers or swing aggressiveness for batters). This non-monotonic relationship between cues and decisions distinguishes sophisticated heuristics from simple rule-based decision-making. The complexity of cue usage in real-world scenarios necessitates strategies for reducing cognitive effort, which we will explore next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effort reduction</head><p>Given their effort-reducing nature <ref type="bibr" target="#b104">(Shah &amp; Oppenheimer, 2008)</ref>, cue-based heuristics offer several advantages over alternative approaches to inference and inductive reasoning. They promote efficiency and adaptability in three main areas: (1) computational complexity: by focusing on a subset of available information or using simplified rules, heuristics can dramatically decrease the time required to reach a decision; (2) memory requirements: given the notorious limitations of human working memory <ref type="bibr" target="#b22">(Cowan, 2010;</ref><ref type="bibr" target="#b86">Miller, 1956)</ref>, heuristics provide a way to make effective use of available information while only storing or processing a subset of it; and (3) solution feasibility: heuristics are particularly valuable in enabling decision-making in situations where comprehensive approaches fail entirely, providing a way to satisfice -to navigate complex problems and arrive at satisfactory, though not provably optimal, solutions <ref type="bibr" target="#b107">(Simon, 1945)</ref>.</p><p>In the simplest cases, discrete-choice decisions may require nothing more than choosing options that maximize a heuristic cue. This direct mapping from features to decisions occurs without additional computational steps, contrary to the monotonic value-based processing often assumed in the heuristics and biases literature. This approach aligns with the well-established understanding that humans are selective in what they track and attend to <ref type="bibr" target="#b20">(Chun, Golomb, &amp; Turk-Browne, 2011)</ref>. Such selectivity is not a limitation, but a feature of our cognitive systems, allowing us to function effectively in complex, information-rich environments. Returning to our earlier examples, one might immediately select the busier-looking restaurant for dinner, using the crowd size as a direct proxy for quality without further deliberation, or instinctively choose the more expensive yogurt brand, assuming price directly indexes quality.</p><p>By exploiting regularities of the environment, heuristics thus provide "shortcuts" to more elaborate reasoning and exhaustive consideration of alternatives. However, this efficiency can come at the cost of accuracy when there is a mismatch between the assumptions driving the heuristic and the environment one is actually in. The effectiveness of a heuristic cue ultimately depends on how reliably it correlates with the property we're interested in across different contexts. Later, we will explore what makes certain heuristic cues more robust and universally applicable than others, providing insights into both the persistence of existing heuristics and the development of new ones.</p><p>Independent of commitments to how heuristics reduce effort, their definition as mechanisms of effort reduction implies the existence of an alternative, more effortful, normative benchmark. This benchmark typically involves more comprehensive information processing and complex computations, often based on principles of rationality or optimality. Understanding this benchmark is crucial for evaluating the effectiveness and limitations of heuristics in various decision-making contexts. We next look more closely at what that benchmark should be and how it relates to our understanding of heuristic decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Models as the Normative Benchmark</head><p>We will begin by discussing models that have been held up as higher-effort or fullinformation alternatives to heuristics in the past. We follow this with a brief summary of Bayesian inference as a standard for rational or optimal inferences and decisions, with comparisons to those earlier benchmarks, before turning to the challenges in approximating Bayesian inferences in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Past benchmarks: weighted additive rules</head><p>In evaluating heuristics -both in establishing their accuracy and their match with human behavior -it is crucial to compare them against appropriate normative benchmarks. Historically, "full-information" models like weighted additive decision (WAD) rules have often been used for this purpose <ref type="bibr" target="#b104">(Shah &amp; Oppenheimer, 2008)</ref>.</p><formula xml:id="formula_0">V (i) = N n=1 w n v i,n ,<label>(1)</label></formula><p>where V (i) is the overall value of alternative i, N is the number of cues, w n is the weight of cue n, and v i,n is the value of cue n for alternative i.</p><p>Despite their historical use, WAD models fall short of capturing the complexities of optimal inference. Given their assumptions of linearity, independence, and the absence of prior information, they fail to account for cases where the ground truth includes complex dependencies or non-linear relationships, or where prior knowledge would make a difference. From this perspective, WAD models are better regarded as another class of heuristics rather than as normative standards. In contrast, fully Bayesian models -when made sufficiently expressive -are, in theory, able to capture dependencies and non-linear patterns, and to incorporate appropriate priors for the problem domain. They thus provide a more suitable normative benchmark for rational reasoning and decision-making under uncertainty (e.g., <ref type="bibr" target="#b0">Anderson &amp; Matessa, 1992;</ref><ref type="bibr" target="#b16">Chater &amp; Oaksford, 2008;</ref><ref type="bibr" target="#b21">Colombo &amp; Seriès, 2012;</ref><ref type="bibr" target="#b31">Friston, 2010;</ref><ref type="bibr">Gong, Pacer, Griffiths, &amp; Bramley, in press;</ref><ref type="bibr" target="#b46">Gopnik &amp; Tenenbaum, 2007;</ref><ref type="bibr" target="#b49">Griffiths &amp; Tenenbaum, 2006;</ref><ref type="bibr" target="#b56">Jaynes, 2003;</ref><ref type="bibr" target="#b67">Kemp, Perfors, &amp; Tenenbaum, 2007;</ref><ref type="bibr" target="#b68">Körding &amp; Wolpert, 2004;</ref><ref type="bibr" target="#b71">Lee, 2011;</ref><ref type="bibr" target="#b80">Lucas &amp; Griffiths, 2010;</ref><ref type="bibr" target="#b96">Ray, King-Casas, Montague, &amp; Dayan, 2008;</ref><ref type="bibr" target="#b111">Socher et al., 2009;</ref><ref type="bibr" target="#b122">Ullman &amp; Tenenbaum, 2020;</ref><ref type="bibr" target="#b133">Xu &amp; Tenenbaum, 2007;</ref><ref type="bibr" target="#b136">Zhao, Lucas, &amp; Bramley, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian inference and its intractability</head><p>Bayesian accounts offer a principled description of computational problems that involve integrating prior knowledge with new evidence, providing a formal framework for optimal inference under uncertainty. In Bayesian inference, the posterior distribution p(θ|y) of model parameters θ given observed data y is proportional to the product of the likelihood p(y|θ) and the prior p(θ), normalized by the evidence p(y):</p><formula xml:id="formula_1">p(θ|y) = p(y|θ)p(θ) p(y)<label>(2)</label></formula><p>Despite its simple mathematical form and its capability for optimal inference, a direct application of Bayesian inference can often be challenging. To illustrate it, consider an example that encapsulates many of the computational difficulties faced in everyday reasoning: Identifying a distant moving light. Let θ represent the identity of the light source (e.g., airplane, helicopter, cyclist, satellite, flashlight) and y represent the light's appearance over time. The Bayesian inference task is to compute P (θ|y):</p><formula xml:id="formula_2">P (θ|y) = p(θ)p(y|θ) θ ′ ∈θ p(θ ′ )p(y|θ ′ )<label>(3)</label></formula><p>The process involves two major challenges: 1. Intractable Marginal Likelihood (Denominator): The denominator involves summing over all possible light sources θ, which could be enormous or even infinite.</p><p>2. Intractable Conditional Likelihood: The conditional likelihood p(y|θ) itself is often intractable, depending on numerous nuisance parameters η:</p><formula xml:id="formula_3">p(y|θ) = p(y|η, θ)p(η|θ)dη<label>(4)</label></formula><p>These parameters might include the object's direction, distance, plausible paths, and many other factors -numerous dimensions that cannot be easily factorized. The intractability of this likelihood is the key motivation for likelihood-free inference methods, which perform inference without explicitly computing these complex likelihoods.</p><p>The intractability problem renders the full Bayesian inference impractical for realtime decision-making in complex, real-world scenarios. The computational demands far exceed human cognitive capacities <ref type="bibr" target="#b75">(Lieder &amp; Griffiths, 2020;</ref><ref type="bibr" target="#b126">Van Rooij, Blokpoel, Kwisthout, &amp; Wareham, 2019)</ref>, and can quickly reach the limits of computational tractability altogether. As such, it stands to reason that both human cognition and artificial systems must rely on efficient approximations or heuristics. However, understanding these approximations in relation to the full Bayesian ideal allows us to characterize how they trade off optimality for computational efficiency. Heuristics, in this light, can be viewed as computationally efficient, likelihood-free approximations to Bayesian inference, tailored to specific environmental structures and computational constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Bayesian Computation (ABC)</head><p>Likelihood-free inference (LFI) addresses a class of problems where direct evaluation of the likelihood function is not feasible, but we have a generative model of the domain. The key innovation of LFI is to bypass likelihood evaluation by working directly with simulations from the model. LFI had early roots in population genetics <ref type="bibr" target="#b116">(Tavaré, Balding, Griffiths, &amp; Donnelly, 1997)</ref> and quickly gained popularity in other scientific fields. Recent years have seen a surge of interest in likelihood-free inference methods with the development of increasingly sophisticated techniques (for reviews, see <ref type="bibr" target="#b23">Cranmer, Brehmer, &amp; Louppe, 2020;</ref><ref type="bibr" target="#b78">Lintusaari, Gutmann, Dutta, Kaski, &amp; Corander, 2017)</ref> such as sequential Monte Carlo likelihood-free methods <ref type="bibr" target="#b110">(Sisson, Fan, &amp; Tanaka, 2007)</ref>, synthetic likelihood methods <ref type="bibr" target="#b131">(Wood, 2010)</ref> and neural density estimation <ref type="bibr" target="#b93">(Papamakarios, Sterratt, &amp; Murray, 2019)</ref>.</p><p>A particularly prominent LFI method we focus on here is Approximate Bayesian Computation (ABC), which implements the broader concept of inference by comparison (see Section 26.2 in <ref type="bibr" target="#b87">Murphy, 2023)</ref>. Intuitively, ABC can be thought of as a "guess and check" approach to inference. Its simplest form, rejection ABC, repeatedly samples possible parameter values from a prior distribution, simulates data using these guesses, and checks how well the simulated data matches the observed data. If the match is close enough, the guessed parameters are accepted as plausible explanations for the observed data. This process is repeated many times to build up a collection of plausible (i.e., accepted) parameter values, which approximate the posterior distribution. The "closeness" of the match is typically judged using a distance function over summary statistics of the data rather than the raw data itself, making the comparison more efficient and often more meaningful. By leveraging this simulation-based approach, ABC opens up new possibilities for cognitive modeling, particularly in scenarios involving complex, high-dimensional processes that resist traditional analytical approaches or approximate methods that require access to a likelihood function.</p><p>Algorithm 1 ABC Rejection Sampling with Summary Statistics</p><formula xml:id="formula_4">1: procedure ABCRejection(p(θ), p(y|θ), y obs , s, d, ϵ, N ) 2: S ← ∅ 3: s obs ← s(y obs ) 4: while |S| &lt; N do 5: θ * ∼ p(θ) 6: y * ∼ p(y|θ * ) 7: s * ← s(y * ) 8: if d(s * , s obs ) ≤ ϵ then</formula><p>9: S ← S ∪ {θ * } 10: end if 11: end while 12:</p><p>return S 13: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rejection sampling</head><p>A more formal understanding of rejection ABC is provided by the pseudocode of this approach in Algorithm 1, which contains the key elements of the choice of summary statistic function s(•), distance metric d, the sample size N , and tolerance threshold ϵ.</p><p>We illustrate ABC using an example in Figure <ref type="figure">1</ref>. Suppose an observer is considering three hypotheses to explain a moving light in the sky: a spotlight, a helicopter, and an unidentified flying object (UFO). These hypotheses differ in their prior probabilities (Figure <ref type="figure">1</ref>). For instance, the prior probability of a UFO might be low, while that of a spotlight could be relatively high, especially if assuming there was a forecast for a light show that night. To perform inference, a data point is simulated from each hypothesis, with sampling frequency proportional to the prior. If the distance between the summary statistics of the simulated data and the observed data falls below a tolerance threshold ϵ, the sample is accepted; otherwise, it is rejected. This process continues until N samples (set to 10 in the figure) are accepted. The final decision is made by selecting the hypothesis that appears most frequently among the accepted samples. As ϵ → 0, the ABC posterior approaches the true posterior distribution. The approximation is asymptotically exact if the summary statistics are sufficient, but non-sufficient statistics can still yield good approximations in practice <ref type="bibr" target="#b8">(Blum, Nunes, Prangle, &amp; Sisson, 2013;</ref><ref type="bibr" target="#b60">Joyce &amp; Marjoram, 2008;</ref><ref type="bibr" target="#b78">Lintusaari et al., 2017;</ref><ref type="bibr" target="#b87">Murphy, 2023;</ref><ref type="bibr" target="#b114">Sunnåker et al., 2013)</ref>. However, the acceptance rate decreases as ϵ does, making inference more costly. Conversely, as ϵ → ∞, the ABC posterior approaches the prior distribution, since all simulated data points are accepted regardless of their similarity to the observed data.</p><p>We focus on rejection ABC due to its simplicity and interpretability, without committing to rejection sampling as a process-level model. While alternative ABC methods, including MCMC-ABC <ref type="bibr" target="#b87">(Murphy, 2023)</ref> can offer better efficiency or accuracy and align with ongoing research into "inference by sampling" <ref type="bibr" target="#b103">(Sanborn, Griffiths, &amp; Navarro, 2010;</ref><ref type="bibr" target="#b138">Zhu, Sanborn, &amp; Chater, 2020)</ref>, we leave a detailed reconciliation of that research and our ideas to future work; the simple rejection sampling procedure provides new insights into how decisions are made with a minimum of auxiliary hypotheses.</p><p>In using rejection ABC to predict responses, we assume that people make decisions by choosing the best or most-likely option, without adding any variability to that already created by the sampling process. For discrete options, this amounts to picking the most frequent θ in S. Given different tolerance thresholds ϵ and total sample sizes N , the procedure can capture a spectrum of responses -ranging from falling back to the prior, to behavior that looks like probability matching, to maximizing under the true posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary statistics</head><p>Summary statistics are an essential part of ABC and central to our account of how heuristics are usefully understood in terms of ABC. They provide a way to compress complex, high-dimensional data into lower-dimensional representations that ideally retain the most relevant information for the inference task at hand. In our example of identifying a moving light in the darkness, the summary statistics could include the light's average warmth (hue) and the average speed of its movement (Figure <ref type="figure">1</ref>). Formally, a summary statistic s(y) is a function of observed or simulated data y, occupying a lower-dimensional space. If y ∈ Y is the observed data, then s : Y → S where typically dim(S) &lt; dim(Y).</p><p>A key concept in the theory of summary statistics is sufficiency. A statistic s(y) is sufficient for parameter θ if it captures all relevant information about θ contained in y. From an information theoretic perspective, if s(y) is sufficient for θ, then:</p><formula xml:id="formula_5">I(s(y); θ) = I(y; θ)<label>(5)</label></formula><p>where I(•; •) denotes mutual information. In Bayesian inference, using sufficient statistics simplifies the computation of the posterior distribution. This means we can perform Bayesian inference using only the sufficient statistic s(y) instead of the full data y, without any loss of information:</p><formula xml:id="formula_6">p(θ|y) = p(θ|s(y)) ∝ p(s(y)|θ)p(θ)<label>(6)</label></formula><p>While sufficient statistics are well understood for many simple parametric distributions (for example, we are all familiar with the mean and standard deviation as sufficient statistics for a Gaussian distribution), finding strictly sufficient statistics for complex simulator models is difficult, and low-dimensional sufficient statistics need not exist for any particular model. In practice, ABC often works with summary statistics that are not strictly sufficient but capture enough of the relevant information for the modeler's purposes. For instance, it often assumes that the behavior of some phenomenon is close enough to Gaussian that we can summarize it with a mean and a standard deviation.</p><p>Instead of comparing observed data y directly with simulated data y sim , ABC compares the summary statistics of observed data to the summary statistics of simulated data:</p><formula xml:id="formula_7">d(s(y sim ), s(y)) ≤ ϵ (7)</formula><p>where d is a distance function (e.g. the Euclidean distance) and ϵ is a tolerance threshold. Given the importance and challenge of choosing good summary statistics, there has been growing interest in methods for learning summary statistics from data. These approaches often use neural networks <ref type="bibr" target="#b18">(Chen, Zhang, Gutmann, Courville, &amp; Zhu, 2020)</ref> or other flexible function approximators to learn mappings from the data to lower-dimensional representations that are informative for the inference task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristics as Approximate Bayesian Computation</head><p>While the theoretical developments of ABC and human heuristics were independent, there are intriguing similarities in their approaches to complex inference problems. We review recent cognitive research suggesting at least four connections between the two fields: (1) the motivation for resource-rational approximation, (2) the reliance on sampling-based processes, (3) the ability to simulate observations, and (4) the similarity between cues (attribute substitution) and summary statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource rationality</head><p>Resource rationality <ref type="bibr" target="#b4">(Bhui, Lai, &amp; Gershman, 2021;</ref><ref type="bibr" target="#b75">Lieder &amp; Griffiths, 2020)</ref> is a current framework in cognitive science that offers a new view for understanding heuristics within the broader context of Bayesian cognition. Moving one step beyond earlier Average Speed Average Warmth approaches such as bounded rationality <ref type="bibr" target="#b108">(Simon, 1955)</ref> and fast and frugal heuristics <ref type="bibr" target="#b40">(Gigerenzer &amp; Todd, 1999)</ref>, resource rationality explicitly formalizes the trade-off between accuracy and computational cost. It posits that a rational agent optimizes the total benefit after considering both the utility of accurate inferences and the costs of computation <ref type="bibr" target="#b48">(Griffiths, Lieder, &amp; Goodman, 2015)</ref>. Resource rationality differs from its predecessors in its emphasis on deriving process-level models from first principles. While bounded rationality and fast and frugal heuristics often propose computationally efficient strategies and then justify their effectiveness post hoc, resource rationality starts by formalizing the total loss function, including both accuracy and computational costs <ref type="bibr" target="#b76">(Lieder, Griffiths, &amp; Hsu, 2018)</ref>. This formalization can then be used to derive specific cognitive processes or heuristics that are optimal under given resource constraints and environmental structures <ref type="bibr" target="#b32">(Gershman, Horvitz, &amp; Tenenbaum, 2015)</ref>.</p><p>From this viewpoint, heuristics are supposed to be seen as potentially resourcerational solutions to the challenges posed by real-world complexity and the computational constraints of human cognition. Although some research has attempted to reframe heuristics, such as the anchoring effect, within the resource-rational framework <ref type="bibr" target="#b77">(Lieder, Griffiths, M. Huys, &amp; Goodman, 2018)</ref>, the relationship between heuristics based on attribute substitution and fast, approximately normative inferences has not previously been given a systematic, formal foundation. Connecting heuristics to ABC can clarify how computational costs are quantitatively reduced and the price one pays in faithfulness to exact posterior distributions, measured by the sample size in the sampling process and the trade-off between the quantity and quality of the summary statistics used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision by sampling</head><p>The ABC approach, as well as other approximate techniques (e.g., Monte Carlo methods) used when the Bayesian prior is intractable, relies on generating samples to estimate probabilities or make decisions. This aligns with the observation that human judgments often exhibit variability consistent with sampling-based processes <ref type="bibr" target="#b10">(Bramley &amp; Xu, 2023;</ref><ref type="bibr" target="#b26">Daw &amp; Courville, 2008;</ref><ref type="bibr" target="#b50">Griffiths, Vul, &amp; Sanborn, 2012;</ref><ref type="bibr" target="#b102">Sanborn &amp; Chater, 2016;</ref><ref type="bibr" target="#b127">Vul, Goodman, Griffiths, &amp; Tenenbaum, 2014;</ref><ref type="bibr">Zhu, Sundh, Spicer, Chater, &amp; Sanborn, 2023)</ref>, especially when assuming a small sample size <ref type="bibr" target="#b127">(Vul et al., 2014)</ref>. The success of sampling-based models extends across various cognitive domains, from categorization to causal reasoning, and shows compatibility with neural data <ref type="bibr" target="#b51">(Haefner, Berkes, &amp; Fiser, 2016)</ref>. Additionally, sequential models like drift diffusion models (DDMs) align with the idea that decision accuracy improves with evidence accumulation over time, interpretable as generating more samples from an internal probability distribution <ref type="bibr" target="#b7">(Bitzer, Park, Blankenburg, &amp; Kiebel, 2014;</ref><ref type="bibr" target="#b95">Ratcliff, Smith, Brown, &amp; McKoon, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mental simulation</head><p>The ABC approach relies on a generative model to sample potential observations, which brings us to another relevant line of cognitive research: mental simulation. Mental simulation has often been proposed as a key mechanism in human reasoning, particularly in domains like intuitive physics. When reasoning about physical systems, it has been shown that humans may simulate the behavior of objects in their minds without explicitly computing probabilities <ref type="bibr" target="#b2">(Battaglia, Hamrick, &amp; Tenenbaum, 2013;</ref><ref type="bibr" target="#b9">Bramley, Gerstenberg, Tenenbaum, &amp; Gureckis, 2018;</ref><ref type="bibr" target="#b33">Gerstenberg, Goodman, Lagnado, &amp; Tenenbaum, 2021;</ref><ref type="bibr" target="#b52">Hamrick, Battaglia, Griffiths, &amp; Tenenbaum, 2016;</ref><ref type="bibr" target="#b121">Ullman, Stuhlmüller, Goodman, &amp; Tenenbaum, 2018)</ref>. This can be understood as a form of sampling trajectories from an intuitive model, where the mind generates plausible trajectories or outcomes based on its knowledge of principles and constraints in a given domain <ref type="bibr" target="#b129">(Wang et al., 2024)</ref>. By running multiple simulations and aggregating the results, people might make probabilistic inferences and predictions about complex systems without explicitly computing likelihoods. Recent studies demonstrate simulation's potential to be drive inference wherever we have imagination-supporting domain knowledge, including the familiar Newtonian dynamics of the physical world but also familiar social situations <ref type="bibr" target="#b1">(Baker, Jara-Ettinger, Saxe, &amp; Tenenbaum, 2017;</ref><ref type="bibr" target="#b34">Gerstenberg &amp; Tenenbaum, 2017;</ref><ref type="bibr" target="#b55">Ho, Saxe, &amp; Cushman, 2022)</ref>. Mental simulation provides a route to inference that does not require explicit computation or representation of likelihoods. Instead, it relies on a generic capacity to generate concrete realizations of a model of the world, which directly links to the likelihood-free idea in ABC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cues and summary statistics in cognition</head><p>As discussed in the "cues (attribute substitution) in heuristics" section, cues play a central role in accounts of heuristic reasoning. After walking through the framework of rejection ABC, we can see the idea of cues, or attribute substitution, satisfy the definition of summary statistics, as both articulate the reduction of evidence by abstracting the useful information. There is ongoing debate about the level of granularity at which cues can and should be understood. The notion of natural assessments <ref type="bibr" target="#b120">(Tversky &amp; Kahneman, 1983)</ref> suggests that people often treat combinations of basic information as single units due to frequent exposure to these information patterns. For instance, when judging a restaurant's quality, "ambiance" might be considered a single cue, despite combining multiple sensory inputs and prior experiences. <ref type="bibr" target="#b104">Shah and Oppenheimer (2008)</ref> point out the ambiguity in determining whether a heuristic that considers only one cue is actually a natural assessment or a combination of multiple cues. This ambiguity contributes to the challenge of defining "basic" cues and the risk of reducing cognitive processes to absurdly granular levels. The parallel between how humans process environmental cues and how statistical methods reduce high-dimensional data to summary statistics is exactly what we are pointing out here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applying ABC to Heuristic Paradigms</head><p>We here provide two practical examples to illustrate how the ABC approach can be applied to tasks that study heuristics. Notably, the purpose of this paper is to provide a new interpretation of heuristics rather than a new approach to outperform them. As such, we do not expect our modeling results to surpass those proposed in previous literature. Instead, we focus on how the approach can (1) align with previous predictions of heuristics, (2) explain the systematic stochasticity patterns in human judgment that are missing from traditional heuristic accounts, (3) approximate the normative Bayesian benchmark, while (4) better capturing human performance than the normative Bayesian benchmark. We use the first task to illustrate points (1), (2), and (3), and the second task to illustrate points (3) and (4).</p><p>The first example is a classic binary comparison task, in which the decision-maker chooses between two alternatives based on a set of cues. This has been the core task in the literature on classic heuristic research <ref type="bibr" target="#b3">(Bergert &amp; Nosofsky, 2007;</ref><ref type="bibr" target="#b38">Gigerenzer et al., 1991;</ref><ref type="bibr" target="#b72">Lee &amp; Cummins, 2004;</ref><ref type="bibr" target="#b84">Mata, von Helversen, &amp; Rieskamp, 2010;</ref><ref type="bibr" target="#b90">Newell et al., 2003;</ref><ref type="bibr" target="#b94">Parpart et al., 2018;</ref><ref type="bibr" target="#b98">Rieskamp &amp; Otto, 2006;</ref><ref type="bibr" target="#b118">Todd &amp; Gigerenzer, 2012)</ref>, especially for demonstrating the take-the-best and tallying heuristics (see below).</p><p>The second example is an ecological causal learning task, in which learners infer the causal structure between events based on how the events unfold over time. This represents a current line of research that investigates how people can learn about or interact with the complex world within limited time. It extends the simple decision-making task by focusing more on heuristics in situations where people must extract useful information from rich perceptual observations in complex everyday tasks including causal learning <ref type="bibr">(Gong &amp; Bramley, 2023;</ref><ref type="bibr" target="#b44">Gong, Gerstenberg, Mayrhofer, &amp; Bramley, 2023;</ref><ref type="bibr" target="#b97">Rehder, Davis, &amp; Bramley, 2022)</ref>, physical learning <ref type="bibr" target="#b9">(Bramley et al., 2018;</ref><ref type="bibr" target="#b121">Ullman et al., 2018)</ref>, decision making <ref type="bibr" target="#b13">(Callaway et al., 2022;</ref><ref type="bibr" target="#b112">Song, Bnaya, &amp; Ma, 2019)</ref>, and multi-step planning <ref type="bibr" target="#b124">(Van Opheusden et al., 2023)</ref>. Similar to traditional decisionmaking paradigms, numerous domain-specific heuristics have been proposed in these studies. However, because researchers tend to highlight the flexibility of human cognition, individual heuristics often fail to capture human performance. Nevertheless, combinations of cues have frequently been shown to outperform the Bayesian benchmark in accounting for human behavior <ref type="bibr">(Gong &amp; Bramley, 2023;</ref><ref type="bibr" target="#b112">Song et al., 2019;</ref><ref type="bibr" target="#b124">Van Opheusden et al., 2023)</ref>. Here, we do not aim to develop a domain-general method for combining heuristics, but rather to show that any combination of cues can be systemically nested within the ABC approach, which may yield predictions that better capture human judgments than the Bayesian benchmark in similar ways to non-ABC cue combination methods used in previous papers.</p><p>Code of the modeling process in this paper can be found at <ref type="url" target="https://github.com/tianweigong/ABCHeuristics">https://github.com/tianweigong/ABCHeuristics</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary comparison</head><p>Take-the-best is one of the most well-known heuristics that have been proposed. It applies to situations where a decision maker needs to choose an option from a consideration set. In research settings, it is often simplified into a binary comparison procedure, where the decision-maker chooses the better of two alternatives (e.g., which city has a larger population) based on a set of binary cues (e.g., whether the city has an underground system; whether it has a university). The importance of cues can be provided directly or learned through a training procedure in which examples are accompanied by feedback indicating which option is better. This importance is typically quantified as cue validity, which measures how often a correct decision is made when the decision maker relies solely on the state of the cue (i.e., one stimulus has the cue, and the other does not). Cue validity is calculated as: Ncorrect N total , where N correct is the number of correct decisions made using the cue, and N total is the total number of decisions for which the cue discriminated between the options. A Bayesian version was later proposed to account for the influence of base rates (i.e., how often the cue is present in the training examples), in which cue validity is calculated as Ncorrect+1 N total +2 (Lee &amp; Cummins, 2004). Nevertheless, two calculations yield the same validity ranking of cues as long as each cue occurs a similar number of times.</p><p>As a "one-reason" decision strategy, take-the-best relies on the single most valid cue and ignores the rest. If this cue discriminates between the options, the favored one is chosen. If not, the next most valid cue is considered, and so on, until a decision is made. Tallying demonstrates another heuristic approach that uses additive decision rule. It chooses the option supported by a larger number of cues, treating each cue as equally important. Formally, both heuristics consider positive cues (where presence indicates a better option) and negative cues (where absence indicates a better option), with the calculation rule flipped for negative cues. However, many studies focus only on positive cues, or even just the most recognizable ones, and make random choices when cues do not discriminate, which is better aligned with the theoretical assumption that heuristics should remain simple and cognitively bounded.</p><p>Both take-the-best and tallying have been framed as deterministic decision rules in heuristics literature, meaning they do not account for the randomness in participants' decisions. We will here aim to replicate the previous phenomena by framing them as summary statistics under the ABC approach, meanwhile providing a way to explain the randomness in responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Task</head><p>We perform simulations based on the task in <ref type="bibr" target="#b72">Lee and Cummins (2004)</ref> and <ref type="bibr" target="#b3">Bergert and Nosofsky (2007)</ref>, who used the same stimuli with different cover stories. Using <ref type="bibr" target="#b3">Bergert and Nosofsky (2007)</ref> for demonstration here, the task asks to decide which of two insects is more poisonous, based on six binary body features (body, eyes, legs, antennae, fangs, and tail). The training phase provided feedback on pairwise comparisons of 16 insects, whose rankings were predetermined by the experimenters. The testing phase asked to choose between pairs of novel insects.</p><p>This task setup is ideal for our illustration here because (1) given the artificial context, numerous environments can be created by randomly swapping the rankings of the 16 training objects; and (2) all participants go through the same training phase, ensuring they receive the same amount of evidence, which is unlike other tasks that require actively searching for information (e.g., <ref type="bibr" target="#b90">Newell et al., 2003)</ref>.</p><p>We later created 100 environments by swapping the rankings of 16 training objects from <ref type="bibr" target="#b72">Lee and Cummins (2004)</ref> and <ref type="bibr" target="#b3">Bergert and Nosofsky (2007)</ref>. In the training phase, all 120 pairwise comparisons of 16 training objects were shown to the observers. In the testing phase, observers were asked to make judgments on the remaining 1896 possible pairs (2 6 -120). As in the original research, we focus on environments in which all cues are positive (i.e., the presence of a cue indicates a better option, ceteris paribus). <ref type="foot" target="#foot_1">2</ref>We focus on two key phenomena identified in previous research. First, the heuristics take-the-best and tallying often yield correct decisions across a wide range of situations. While many earlier simulations used real-world environments -where the correct answer is determined by actual ground truths -we demonstrate that these heuristics also perform well in artificial environments. Specifically, when Bayesian rationality is used as the normative benchmark, both the original heuristic algorithms and the ABC versions show strong performance as the approximation.</p><p>Second, the applicability of heuristics depends on the environments. For instance, when cues have similar levels of importance, relying on a single cue may be insufficient for identifying the best option. In such cases, the effectiveness of the take-the-best heuristic diminishes as cue validities converge. Conversely, when cue importance varies greatly, disregarding cue rankings may lead to suboptimal decisions. In such cases, the effectiveness of the tallying heuristic decreases as cue validities diverge. We will show that the ABC version of take-the-best and tallying replicates these observations.</p><p>Apart from general ecological characteristics, performance could also vary across the testing trials under some specific environments. For example, the cues can be noncompensatory, where the most important cue carries more importance than the sum of all other cues, or compensatory, where the combined importance of the less important cues exceed that of the best cue. In compensatory environments, take-the-best may make errors in certain trials-specifically when the unchosen option would have been better if all cues had been taken into account. We will show that the ABC version of take-the-best will generally replicate this observation while retaining its accuracy in certain special cases due to its consideration of uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Benchmark</head><p>In general, the probability that alternative A is better than B, based all cues F can be written as</p><formula xml:id="formula_8">p(A &gt; B|F ) ∝ p(F |A &gt; B)p(A &gt; B)<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">A &gt; B = 1[V [A] &gt; V [B]].</formula><p>Assuming the alternatives have the same prior, and the cues contribute independently to the likelihood function, the Bayesian benchmark decision simplifies to the log-odds that one alternative is better <ref type="bibr" target="#b72">(Lee &amp; Cummins, 2004)</ref>:</p><formula xml:id="formula_10">L AB = i∈F A ln( vi 1 -vi ) - i∈F B ln( vi 1 -vi ) (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where vi is the estimated validity of cue i given the training evidence, F A contains the features that are present in A but absent in B, and F B contains the features that are present in B but absent in A. The benchmark answer will be set as A when L AB &gt; 0 and B when L AB &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary Statistics</head><p>We consider take-the-best and tallying as two different types of summary statistics, both of which can be applied within the ABC framework. Take-the-best corresponds to a summary statistic that first sorts the cues in descending order of cue validity, then returns the value difference of the first discriminating cue (i.e., a cue that appears in one alternative but not the other). Given that all cues are usable in the current set up, the take-the-best summary statistic will take on one of two values, {1, -1}, indicating whether A or B wins, respectively.<ref type="foot" target="#foot_2">foot_2</ref> tallying, on the other hand, corresponds to the difference in the count of positive cues (i.e., cues with v &gt; 0.5) for both A and B, and calculates the difference between the two counts. It will range from -6 to 6 in the current set up. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Because the possible values of the take-the-best and tallying summary statistics are limited and discrete, we fix the acceptance threshold to 0 for take-the-best ABC and 1 for tallying ABC. This means that the value must match exactly for take-the-best, and can differ by at most 1 for tallying summary statistics. We here use samples of N = {20, 100} to capture randomness in the response. 100 ABC observers were simulated for each environment. We first simulated 100 environments and divided them into low or high cue validity groups based on the median of the average validity of six cues. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, the heuristic algorithms achieved high accuracies (i.e., closely aligned with the Bayesian benchmark choices), significantly above random decision-making (0.5). Accuracy was influenced by cue validity variation: The accuracy of take-the-best increased as the variation increased (i.e. when one cue was more likely to dominate the diagnosis), whereas the accuracy of tallying increased as the variation decreased. Importantly, the ABC versions of take-the-best and tallying closely tracked the performance of their original deterministic counterparts. That is, they align with the deterministic versions when the ABC observer makes decisions based on a large sample size (N = 100), but exhibit more stochasticity when the sample size is small (N = 20). This stochasticity increases when cue validity is generally low and variation is minimal (top-left panel, and the left side within the panel) -environments under which even the Bayesian benchmark struggles to make accurate decisions. This suggests that ABC heuristics not only replicate the behaviors of traditional heuristics, but also make further predictions about how the responses become uncertain in</p><p>Low Cue Validity High Cue Validity Low Cue Variation High Cue Variation 0.5 0.6 0.7 0.8 0.9 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 Bayesian Benchmark Posterior ABC TTB Choosing Proportion Compensatory Environments Validity Cue 1 0.60 Cue 2 0.56 Cue 3 0.55 Cue 4 0.54 Cue 5 0.52 Cue 6 0.52 e.g., Compensatory Environment #186 (Small cue validity. Low cue variation)</p><p>1 0 0 0 0 0 0 1 1 1 1 1 vs .</p><p>Fig. <ref type="figure">3</ref>: Scatterplots showing the proportion that the ABC take-the-best algorithm selected the compensatory answer in compensatory environments. The examples on the left illustrate a non-compensatory choice (top) versus a compensatory choice (bottom, highlighted in the brown box) in a compensatory environment where a combination of non-best cues can outweigh the absence of the best cue. particular situations. These predictions are also consistent with the human tendency to behave more stochastically in highly noisy environments as suggested in previous research <ref type="bibr" target="#b89">(Newell, Koehler, James, Rakow, &amp; Van Ravenzwaaij, 2013;</ref><ref type="bibr" target="#b105">Shanks, Tunney, &amp; McCarthy, 2002;</ref><ref type="bibr" target="#b128">Vulkan, 2000;</ref><ref type="bibr" target="#b132">Wozny, Beierholm, &amp; Shams, 2010)</ref>.</p><p>One type of trial used to diagnose whether an observer follows Bayesian rational decision-making or the take-the-best heuristic involves situations where the option containing the best cue is actually worse than the alternative, which lacks the best cue but includes other informative cues. This scenario can arise in compensatory environments, where a combination of non-best cues can outweigh the absence of the best cue. We simulate 300 such compensatory environments to evaluate how the ABCtake-the-best model (ABC-TTB) performs. We focus on trials in which Option A (the upper option in Figure <ref type="figure">3</ref>) is positive only on the best cue, whereas Option B (the lower option in Figure <ref type="figure">3</ref>) is positive on all cues except the best one. In these trials, the take-the-best heuristic will always select Option A. In contrast, a Bayesian decisionmaker will consistently favor Option B, although the posterior probability will vary depending on the specific environment.</p><p>As shown in Figure <ref type="figure">3</ref>, the ABC take-the-best observer often replicates the classic take-the-best heuristic by not choosing Option B -the the Bayesian benchmark's preference. However, there remains a small probability for it to Option B -especially in situations where cue validity and cue variation are both low, indicating a high degree of uncertainty even from the Bayesian perspective. These results highlight that the ABC take-the-best model captures not only the deterministic behavior of the takethe-best heuristic but also predicts stochasticity reflecting rational uncertainty in the responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous-time causal learning</head><p>In the second case example, we move from the traditional, discrete-trial-based paradigm, to a more ecological paradigm that requires people to process rich, real-time evidence to make judgments. This approach emerges from recent studies examining people's intuitive understanding of the world, such as how they infer the physical properties and relationships of objects <ref type="bibr" target="#b9">(Bramley et al., 2018;</ref><ref type="bibr" target="#b121">Ullman et al., 2018)</ref>, the mental states and social relationships of agents <ref type="bibr" target="#b1">(Baker et al., 2017;</ref><ref type="bibr" target="#b34">Gerstenberg &amp; Tenenbaum, 2017;</ref><ref type="bibr" target="#b79">Liu, Ullman, Tenenbaum, &amp; Spelke, 2017)</ref>, and the causal structure of events in general <ref type="bibr">(Btesh, Bramley, Speekenbrink, &amp; Lagnado, in press;</ref><ref type="bibr" target="#b25">Davis, Bramley, &amp; Rehder, 2020;</ref><ref type="bibr">Gong &amp; Bramley, 2023;</ref><ref type="bibr">Gong et al., 2023)</ref>. Given the complexity of the task structure, a single heuristic can seldom capture human responses <ref type="bibr" target="#b13">(Callaway et al., 2022;</ref><ref type="bibr" target="#b33">Gerstenberg et al., 2021)</ref>. However, research also shows that rational Bayesian inference does not capture human responses well either <ref type="bibr">(Gong &amp; Bramley, 2023;</ref><ref type="bibr" target="#b112">Song et al., 2019;</ref><ref type="bibr" target="#b124">Van Opheusden et al., 2023)</ref>.</p><p>A key feature of these tasks is that, given a candidate structural hypothesis, one can fairly easily imagine a scene or sequence of events it might produce. However, there exists an infinity of such possible scenes or sequences, with much of the underlying generative process remaining unobserved. It creates a situation in which the likelihood is extremely difficult or impossible to calculate, providing another fertile ground for studying heuristic decision-making strategies. Here, we use <ref type="bibr">Gong and Bramley (2023)</ref> as a representative example of this line of research and reanalyze its data under the ABC approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Task</head><p>In the task, participants observed causal systems that produce series of point events in continuous time, and then judged the causal relationships linking control components to a target component based on these temporal patterns of events. Each causal system included two control components, A and B, which were intervened upon by a preset program, and one target component, E (Figure <ref type="figure" target="#fig_3">4a</ref>). Participants were asked to use the available temporal information to whether the control component A or B was a generative cause (G) or a preventative cause (P) of the target component, or non-causal (N, see Figure <ref type="figure" target="#fig_3">4b</ref> for stimulus examples).</p><p>Participants were instructed on E's base rate and how a generative or preventative component would work. For instance, a generative component, when activated, would produce one extra E activation with a delay of around 1.5 seconds. A preventative component, when activated, would block all E's activations for around 3 seconds. Even though the rules were simple, the evidence could involve complex dynamics: An effect event expected to be generated by one cause might be prevented by another cause. An effect event in fact generated by one cause might appear after the intervention of another cause. These possibilities dramatically compound the complexity of calculating the exact likelihood that a causal structure produces a particular observation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Benchmark</head><p>A normative Bayesian analysis here would rely on enumerating and marginalizing over all possible token-level causation (i.e., which particular event actually caused which particular effect), to explain all effects observed, as well as all effects that were expected but not observed (e.g., due to the existence of a preventative cause; see <ref type="bibr">Gong &amp; Bramley, 2023</ref>, for modeling details). Given that any effect event could potentially be attributed to any cause event that happened before, the computational complexity grows rapidly with observation duration, and quickly become intractable once the observation period lasts longer than the particular experimental settings used in <ref type="bibr">Gong and Bramley (2023)</ref>.</p><p>randomness inherent in the sampling process. <ref type="foot" target="#foot_3">4</ref> Given that the study involved stimuli with three interventions to each component A and B, there are different ways to summarize the data. We here test which one of the two ways could better capture people's judgments. One is to use the "Full Vector", which includes delay and count features for all six interventions -A 1 , A 2 , A 3 , B 1 , B 2 , and B 3 (see Figure <ref type="figure" target="#fig_3">4c</ref>). The other uses the "Mean Vector" which averages each feature across components, resulting in a reduced total of four features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compared our Full Vector and Mean Vector ABC models with the Bayesian benchmark, examining which can better explain participants' response. Each model contains one fit parameter. A tolerance threshold parameter was fit for the ABC models and a softmax temperature parameter was fit for the Bayesian benchmark. We use the Wasserstein distance as the metric for model comparison for the ability to capture both the magnitude and distribution of differences between predicted and observed outcomes.</p><p>Results are shown in Figure <ref type="figure" target="#fig_4">5</ref>. The lowest Wasserstein distance is achieved by the ABC Full Feature model (0.050), compared to the Bayesian normative benchmark (0.062) and the ABC Mean Feature model (0.053). Qualitatively, the full normative benchmark model also provides a considerably poorer explanation of human inferences compared to the more heuristic accounts, as the latter better capture participants' responses-both in terms of the degree to which they chose the correct structure (upper right) and the degree to which they made specific errors (bottom left). This helps demonstrate the idea that, instead of following the highly demanding rational Bayesian inference, people tend to rely on simple features in event streams, the heuristics, to make judgments. The advantage of using the ABC framework here also lies in its ability to predict the distribution of human responses beyond simple point estimates (Figure <ref type="figure" target="#fig_4">5</ref>), something that cannot be easily achieved under the classic heuristic approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>In this paper, we have proposed that reliance on simple heuristics can be framed as a resource-rational approximation to a more complete Bayesian inference. By drawing parallels between heuristics and likelihood-free inference methods, particularly ABC, we offer a new insights into the nature and adaptive value of heuristics. It maintains the normative Bayesian framework while acknowledging and working within real-world computational constraints, providing a bridge between ideal observer models and the heuristics employed in human cognition. As briefly mentioned in the introduction, it addresses three key challenges in understanding human decision-making: applicability, uncertainty representation, and relationship with optimality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applicability of heuristics</head><p>The ABC framework significantly extends the applicability of heuristic models to complex, real-world scenarios by relaxing strong assumptions about cue-target relationships. For example, the reanalysis of <ref type="bibr">Gong and Bramley (2023)</ref> demonstrates this enhanced applicability in at least two ways: First, our ABC model successfully captured human judgments in a continuous-time causal inference task characterized by intricate temporal dynamics, which presents a challenge for traditional heuristic models due to its complex cue-outcome mappings. By employing summary statistics within an ABC framework, we demonstrated how ABC can extend heuristic modeling beyond the typical domain of static cue structures. Second, our approach exhibited considerable flexibility in cue selection. By exploring various summary statistics, including features per action and overall mean features, we illustrated how ABC facilitates adaptive cue selection without requiring strong a priori assumptions about cue validity. This flexibility allows heuristic models to adapt to diverse task structures, addressing a key limitation of traditional approaches that often rely on fixed, predefined cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty representation</head><p>A common criticism of heuristic models is their limited capacity to represent uncertainty. Our ABC framework naturally provides predictions about how variability in human responses tracks uncertainty, given the stochastic nature of the sampling procedure. In the first illustrative example, we captured the phenomenon that humans tend to behave more stochastically when environmental uncertainty is high <ref type="bibr" target="#b89">(Newell et al., 2013;</ref><ref type="bibr" target="#b105">Shanks et al., 2002;</ref><ref type="bibr" target="#b128">Vulkan, 2000;</ref><ref type="bibr" target="#b132">Wozny et al., 2010)</ref>. As shown in the simulation (Figure <ref type="figure">3</ref>), this behavior ultimately leads to better payoffs than the traditional take-the-best strategy in environments characterized by low cue validity and low cue variation. In the second example, it demonstrated the ability not only to capture the most frequent human judgments but also to reflect the systematic error patterns in responses (Figure <ref type="figure" target="#fig_4">5</ref>).This suggests that our LFI approach is modeling the full distribution of human judgments, not merely the average behavior indicated by heuristic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship with optimality</head><p>The LFI framework provides a nuanced perspective on the relationship between heuristics and optimal inference. Our ABC model can approximate normative inferences (given sufficient statistics, large numbers of samples, and ϵ → 0), but anticipates different kinds of departures from Bayesian norms, due to insufficient summary statistics, larger ϵ, and fewer samples. The performance of our computationally simpler ABC model over the normative model exemplifies the principle of computational rationality. By framing heuristics as LFI, we establish a formal link between heuristic strategies and Bayesian inference, positioning heuristics not as departures from Bayesian rationality, but as rational approximations to likelihood calculations under computational constraints. Moreover, the varying performance of different summary statistics in our ABC model provides a formal framework for understanding why certain heuristics are more effective or widely used than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards process-level explanations</head><p>One promising line of work is given by integrating the ideas of likelihood-free inference with the literature on inference-by-sampling <ref type="bibr" target="#b14">(Castillo, León-Villagrá, Chater, &amp; Sanborn, 2024;</ref><ref type="bibr" target="#b16">Chater &amp; Oaksford, 2008;</ref><ref type="bibr" target="#b26">Daw &amp; Courville, 2008;</ref><ref type="bibr" target="#b50">Griffiths et al., 2012;</ref><ref type="bibr" target="#b101">Sanborn, 2017;</ref><ref type="bibr" target="#b102">Sanborn &amp; Chater, 2016;</ref><ref type="bibr" target="#b103">Sanborn et al., 2010;</ref><ref type="bibr" target="#b106">Shi &amp; Griffiths, 2009;</ref><ref type="bibr" target="#b113">Stewart, Chater, &amp; Brown, 2006;</ref><ref type="bibr" target="#b134">Zednik &amp; Jäkel, 2016;</ref><ref type="bibr">Zhu, Chater, et al., 2023;</ref><ref type="bibr" target="#b138">Zhu et al., 2020;</ref><ref type="bibr">Zhu, Sundh, et al., 2023)</ref>. In common with the inference-bysampling (IBS) accounts of human cognition, we argue that while the generative model is the true latent representation of one's belief we generically have to rely on samples to access our beliefs and do inferences that respect them. The crucial distinction lies in how these samples are generated and evaluated: standard IBS approaches typically assume the mind can calculate unnormalized likelihoods to determine whether to accept or reject samples. In contrast, we contend that likelihoods, even unnormalized ones, are generally intractable in complex, real-world scenarios. By eliminating the need for explicit likelihood calculations, the ABC approach offers a more realistic model of how mental samples can be evaluated. At the same time, our proposal is compatible with recent IBS models: To implement ABC for a particular choice of summary statistics and distance function, models like the autocorrelated Bayesian sampler <ref type="bibr">(Zhu, Sundh, et al., 2023)</ref> generally need only replace their unnormalized likelihoods with 1(d(s(y sim ), s(y)) ≤ ϵ) indicator functions.</p><p>The ABC framework can be naturally extended to incorporate key features of inference-by-sampling models, enhancing its explanatory power and cognitive plausibility. Rejection sampling is one of the simplest methods available for likelihood-free inference. We implement this basic form in ABC, but there are several alternatives that could be implemented and may serve as cognitively plausible sampling procedures.</p><p>Future work may benefit from investigating different versions of ABC, examining subtle differences in the responses, and exploring which variant better explains people's behavior. For example, by adapting our approach to use a Markov Chain Monte Carlo (MCMC) sampler with a local proposal distribution, instead of sampling directly from the prior, we can create a more efficient model than rejection-ABC <ref type="bibr" target="#b81">(Marjoram, Molitor, Plagnol, &amp; Tavaré, 2003)</ref>. This extension would not only maintain the core strength of our likelihood-free approach but also capture patterns of autocorrelation and anchoring observed in human learning. This extended ABC model would predict a range of phenomena already associated with IBS models, such as autocorrelation effects, speed-accuracy trade-offs, and calibration effects under speeded judgment. Moreover, it could generate novel predictions unique to our likelihood-free framework. For instance, our model anticipates differences in speed-accuracy tradeoffs based on the surprisingness of observations. Unlike tractable-likelihood models, which are indifferent to how unusual evidence is in absolute terms, our ABC approach predicts that surprising examples would require more processing time. This is because unusual observations would be less likely to match the summary statistics of simulated data, necessitating more simulations to achieve a successful match. This distinctive prediction offers a testable hypothesis that could differentiate our framework from traditional IBS approaches.</p><p>In this paper, we assume that the reasoner simulates concrete samples and extracts summary statistics from them. However, at the process level, the simulator itself may be more abstract, neglecting nuisance variables and focusing on what is more relevant to the task <ref type="bibr" target="#b54">(Ho, Abel, et al., 2022)</ref>. This links back to the core idea of the classic representativeness heuristic, which argues that people judge the probability of an event by how similar it is to their mental prototype <ref type="bibr" target="#b35">(Gigerenzer, 1991;</ref><ref type="bibr" target="#b47">Griffiths, Daniels, Austerweil, &amp; Tenenbaum, 2018;</ref><ref type="bibr" target="#b62">Kahneman et al., 2002;</ref><ref type="bibr" target="#b63">Kahneman &amp; Tversky, 1972)</ref>. Within the ABC framework, the prototype can be understood as an imperfect simulator that encodes only the most salient features of a category. When a characteristic is atypical for a category, the simulator may fail to represent it and thus never generate it, making it absent in samples from that category. In contrast, categories for which the characteristic is stereotypical will faithfully include it in their samples, giving them an advantage in perceived similarity to the observation. Future work could relax the constraints that are still considered normative (e.g., a normative simulator) in the ABC framework to examine whether this better captures human behavior.</p><p>Where do summary statistics come from?</p><p>The origin and selection of summary statistics are crucial aspects of this framework, as summary statistics underpin heuristics in likelihood-free inference. Although we do not propose a general solution here, we identify several sources and mechanisms by which summary statistics can arise, each offering insights into human cognition and decision-making.</p><p>In traditional statistical approaches, summary statistics derive from domain knowledge and the mathematical structure the data-generating process. For instance, in Gaussian settings, the sample mean and variance are natural choices due to their sufficiency and simplicity. Likewise, exponential family distributions use natural sufficient statistics. These approaches are effective in well-understood domains but struggle with complex, high-dimensional data where relevant features are not obvious.</p><p>This limitation shifts the challenge of discovering useful summary statistics to earlier stages of learning, e.g., through iterative trial-and-error of domain knowledge within and across generations (see next section). A more general, data-driven approach, which is common in modern machine learning, learns summary statistics directly from data via deep neural networks <ref type="bibr" target="#b17">(Chen, Gutmann, &amp; Weller, 2023;</ref><ref type="bibr" target="#b18">Chen et al., 2020;</ref><ref type="bibr" target="#b57">Jiang, Wu, Zheng, &amp; Wong, 2017)</ref>. Such methods can capture information, non-obvious features while discarding irrelevant or redundant ones.</p><p>Humans, too, seem to learn summary statistics through experience and exposure to data. This can be viewed as amortized inference: the effort of identifying relevant features is distributed across multiple encounters with similar problems. For example, medical professionals learn to quickly identify key symptoms and risk factors for particular diseases (see, e.g., <ref type="bibr" target="#b41">Gobet, 2017)</ref>, effectively constructing internal mappings from observations to summary statistics -analogous to the abstraction process in ABC.</p><p>In addition to learning from real-world experiences, humans can also learn summary statistics through mental simulation (e.g., <ref type="bibr" target="#b130">Wittkuhn, Chien, Hall-McMaster, &amp; Schuck, 2021)</ref>. By generating data from internal models of the world, individuals can train their recognition systems, offline, to extract informative features based on replay of past experiences. This could be related to a presumed core feature of cognition: the wake-sleep cycle, in which offline replay during rest and sleep facilitates semanticisation and abstraction <ref type="bibr" target="#b28">(Dayan, Hinton, Neal, &amp; Zemel, 1995;</ref><ref type="bibr" target="#b30">Ellis et al., 2023;</ref><ref type="bibr" target="#b85">Mattar &amp; Daw, 2018;</ref><ref type="bibr" target="#b99">Sablé-Meyer, Ellis, Tenenbaum, &amp; Dehaene, 2022)</ref>.</p><p>The acquisition of summary statistics can occur across various timescales. Over developmental time, children learn to extract increasingly sophisticated statistical regularities, as demonstrated by infants' ability to use transitional probabilities for speech segmentation <ref type="bibr" target="#b100">(Saffran, Aslin, &amp; Newport, 1996)</ref> and older children's capacity to infer causal structures from data <ref type="bibr" target="#b45">(Gopnik &amp; Schulz, 2004)</ref>. Throughout a lifetime, individuals accumulate domain-specific expertise, refining their ability to identify relevant features in familiar contexts, as seen in chess masters' rapid pattern recognition <ref type="bibr" target="#b15">(Chase &amp; Simon, 1973)</ref> or radiologists' efficient identification of diagnostic features <ref type="bibr" target="#b70">(Kundel, Nodine, Conant, &amp; Weinstein, 2007)</ref>. On the shortest timescale, individuals can rapidly adapt to their current task through trial-and-error or instruction, as evidenced by studies on human categorization showing quick learning of task-relevant stimulus dimensions <ref type="bibr" target="#b91">(Nosofsky, 1986)</ref>.</p><p>This multi-timescale view helps highlight both the supreme flexibility of human cognition in its ability to adapt to momentarily novel situations and the remarkable efficiency of expert intuition in familiar domains. The origin of summary statistics in human cognition likely involves a complex interplay between innate predispositions, individual learning experiences, and cultural transmission of knowledge. The ABC framework suggests that heuristics, viewed as applications of summary statistics, are best thought of as a general purpose learning architecture rather than a set of fixed strategies, allowing individuals to adapt to new situations and accumulate experience in a complex environment that cannot be represented exactly. This perspective inverts the classical heuristics literature's view of the mind as containing an "adaptive toolbox" filled with situation-specific heuristics <ref type="bibr" target="#b39">(Gigerenzer &amp; Selten, 2002)</ref>. The adaptive toolbox view notoriously leads to the meta-cognitive challenges of selecting, switching between strategies <ref type="bibr" target="#b74">(Lieder &amp; Griffiths, 2017)</ref> and not to mention inventing new ones <ref type="bibr" target="#b69">(Krueger, Callaway, Gul, Griffiths, &amp; Lieder, 2024)</ref>. While likelihood-free view does not make the problem of generalization go away, nor does it imply that the mind solves it by storing and selecting from proliferating set of fixed strategies (i.e. the ever-growing list of heuristics documented by psychologists). It also makes heuristic invention less mysterious, by associating this with a generic cognitive process of abstraction of inputs to lower dimensional representations. We illustrate this conceptual inversion with an analogy to the problem of provisioning a toolbox for adaptivity flexibility (Figure <ref type="figure" target="#fig_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amortized inference and heuristics</head><p>The concept of amortized inference provides a complementary perspective on how heuristics might be learned and applied in complex real-world domains. This idea aligns naturally with our proposal and connects to previous accounts of heuristics, though it partially focuses on different aspects than what we explore in this work. We suggest that a mixed strategy combining amortization and likelihood-free inferenceby-sampling is both plausible and cognitively efficient. <ref type="bibr" target="#b6">Binz et al. (2022)</ref> propose Bounded Meta-Learned Inference (BMLI), an account of how people discover and select heuristics in decision-making, which addresses the challenges implied by the adaptive toolbox metaphor above. Their work suggests that through meta-learning, individuals exploit environment structure to select heuristics while managing computational resources. Different heuristics emerge as resource-rational strategies depending on available information: one-reason decisionmaking when feature rankings are known, equal weighting when feature directions are known, and weighted combinations when neither are known. In contrast to our work, BMLI assumes that people optimize for future predictions, rather than approximating Bayesian inference. Related to this, <ref type="bibr" target="#b24">Dasgupta et al. (2020)</ref> propose a theory of "learning to infer", to explain systematic deviations from Bayesian optimality. Their model uses a recognition network that maps queries to probability distributions. It allocates computational resources (as implemented by restricted the size of the network) preferentially to frequently encountered queries, leading to characteristic biases observed in human inference.</p><p>In the context of our framework, these approaches could be reconciled by considering the mechanisms they postulate for learning summary statistics. The notion of focusing computational effort on the most informative aspects of data aligns closely with our proposal that people select appropriate summary statistics for the task at hand. Further, for frequently encountered problems, people may not simulate a number of explicit possibilities, but rather perform a direct mapping to an inference through pattern-matching, or perform a small number of very informed simulations.</p><p>It is important, however, to distinguish between these variational-optimization approaches and our sampling-based framework. While both offer explanations for how the mind might approximate Bayesian inference, they differ in their underlying mechanisms and constraints <ref type="bibr" target="#b101">(Sanborn, 2017)</ref>, and we hope the present proposal provides a useful framework for investigating these differences further.</p><p>This raises the question of why we do not simply amortize everything, given the efficiency gains of amortization. Several considerations argue against this. First, amortized inference can struggle with novel or out-of-distribution situations, as it generalizes poorly beyond its training domain. In contrast, Approximate Bayesian Computation (ABC) methods are often more robust under such conditions <ref type="bibr" target="#b65">(Kelly et al., 2025)</ref>. Second, amortization requires substantial upfront computational and memory costs (analogous to "buying many gauged wrenches" in Figure <ref type="figure" target="#fig_5">6</ref>). For rare or rapidly changing environments, these costs may outweigh the benefits, especially considering memory costs. Maintaining some capacity for online inference based on rich simulator models allows for handling unexpected scenarios more effectively. Finally, pure amortization would lead to overfitting specific problem structures, reducing capacity to generalize to new situations (Figure <ref type="figure" target="#fig_5">6b</ref>), especially when there are only a few examples to generalize from.</p><p>In light of these considerations, a mixed strategy of amortization and inference-bysampling seems like a sensible trade off. It would balance the computational efficiency of amortized inference with the flexibility and expressiveness of simulation-based inference. In our framework, people might perform mental simulations using a simulator model and subsequently applying summary statistics learned through amortization to these simulations. Adaptation could involve adjusting which summary statistics are used, how many samples are generated, or the acceptance threshold. Furthermore, parts of the simulation process itself might be amortized, with full simulation engaged only when necessary, as a form of multi-fidelity simulation. It aligns well with the notion of learning heuristics or intuitive theories through experience: As people encounter various inferential problems within a domain, they may gradually learn to identify the most informative regularities that can guide their mental simulations and support rapid, approximate inferences. For instance, an experienced chef might develop heuristics for cooking decisions, such as adding a dash of soy sauce to bland dishes or reducing sautéing time by two minutes for smaller onions. These learned heuristics can then be generalized to novel but related problems, allowing for efficient generation of plausible hypotheses or predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we developed a unifying framework that views heuristics as resourcerational approximations to Bayesian inference, drawing connections between cognitive shortcuts, mental simulation, and likelihood-free inference methods. This perspective can improve our understanding of how heuristics operate and also illuminate why they emerge and how they relate to broader principles of cognition. Our approach reconciles seemingly contradictory findings in the literature, such as the tension between the simplicity of heuristics and the complexity of the computational problems minds face in uncertain choice. This work opens up several promising avenues for future research, such as examining how heuristics are learned or adapted over time within this framework, potentially providing new insights into cognitive development and expertise acquisition. By integrating insights from cognitive science, machine learning, and statistics, we provide a novel perspective on the nature and origin of heuristics in human cognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Fig.1: A light source inference example for rejection ABC. Three hypothetical sources (Spotlight, Helicopter, UFO) differ in their priors. The reasoner compares the summary statistics (average warmth and average speed of the light) between the observation (the gray star) and data simulated based on the generative models and hypothesis priors. Samples that fall within the tolerance threshold (ϵ) are accepted until the preset sample size N=10. When ϵ is small, more simulated data points are needed, and the final answer tends to reflect the ground truth (Helicopter). When ϵ is large, the final answer tends to reflect the priors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Scatterplots of the standard deviation of cue validity and performance accuracy in four models: Take-the-best, tallying, ABC version of Take-the-best, and ABC version of tallying. N indicates the sample size for acceptance in rejection ABC. Dotted lines indicate the change level (50%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Illustration of task completed by participants in Gong and Bramley (2023). (a) The two "control components" were shown to be intervened on, and participants can observe the following effects. b) Example stimulus sequences. c) The Count and Delay summary statistics. If no effect follows an intervention, the Delay will be calculated as the delay between it and the next intervention (or the end of the observation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Scatterplots of model predictions and human judgments show the proportion of choices for different causal types under various ground truths. Each ground truth is represented by nine data points, corresponding to the nine possible response types. The data are fitted at the individual stimulus level and visualized at the aggregate level by averaging responses within the corresponding categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: (a) A range of nuts of different gauges representing a history of decision problems. (b) Newly encountered nuts in a mixture of familiar and unfamiliar gauges. (c) A set of gauged wrenches representing an Adaptive Toolbox of fixed heuristics, purchased to solve the problems in a, but now hard to select from and unsuitable for some problems in b. (d) A monkey wrench, representing general likelihood-free inference mechanism such as ABC. While in use, it gives the appearance of a fixed strategy (fixed gauge), but can be endlessly adjusted to new problems.</figDesc><graphic coords="27,191.90,156.72,96.39,53.98" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The concept of intractability is central to our framework. We will engage with it in more detail later, but briefly: in computational theory and Bayesian inference, a problem is considered "intractable" when finding an exact solution is practically impossible within reasonable time or resource constraints. This can occur due to exponential growth in computational requirements as the problem size increases. In Bayesian contexts, exact calculation of conditional probability distributions is often infeasible due to high-dimensional parameter spaces, difficulties in evaluating likelihood functions, and the unavailability of a closed-form mathematical expression of a generative process. Among them, the difficulty in evaluating likelihoods is the focus of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>As they are sufficient to address the point we aim to make in this paper without requiring customized rules for handling negative cues.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This summary value can be computed directly for an observation in various ways. One example iss(y) = 1 × Sign(Binary(Sort(V A , rank V B )) -Binary(Sort(V B , rank V B )) -12 ),which treats the ranked cue values as a binary number, enforcing Take-the-best's non-compensatory property -e.g. 100000 (32) is larger than 011111 (31).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This idea has driven the modeling inGong and Bramley (2023), but the original paper does not involve the sampling or decision process scheme we use here. Instead it focuses more on a computational level analysis of the summary statistics' distributions and kernel based likelihood estimates from this.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>TG is supported by a <rs type="funder">British Academy</rs> <rs type="grantName">Postdoctoral Fellowship Scheme</rs> (<rs type="grantNumber">PFSS24\24009</rs>). This work was done prior to <rs type="institution">SV joining Amazon</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cpNHkar">
					<idno type="grant-number">PFSS24\24009</idno>
					<orgName type="grant-name">Postdoctoral Fellowship Scheme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary Statistics</head><p>Gong and <ref type="bibr">Bramley (2023)</ref> proposed people chunk the continuous time evidence and focuses on summary statistics of what happened in the period between two interventions (i.e., A's or B's activations). Two summary statistics are proposed: Count (how many effect events occur after an intervention and before the next intervention) and Delay (how long is the delay between the intervention and its following effect event, regardless of what truly generated that effect). Those cues have an intuitive relationship with diagnosing the causal types <ref type="bibr">(Gong et al., in press;</ref><ref type="bibr" target="#b123">Valentin, Bramley, &amp; Lucas, 2023)</ref>. For example, one will intuitively expect to see more effect activations following the intervention of a generative cause (due to the combination of base rate activations and generation). Conversely, fewer activations are likely to follow the intervention of a preventative cause.</p><p>We here implement the proposed summary statistics to the general ABC rejection sampling framework as shown in Algorithm 1. This will provide a parsimonious explanation of the variations in participants' responses, as directly arising from the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explorations of an incremental, bayesian algorithm for categorization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matessa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="275" to="308" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rational quantitative attribution of beliefs, desires and percepts in human mentalizing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jara-Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017. 0064</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A response-time approach to comparing generalized rational and take-the-best models of decision making</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Bergert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resource-rational decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meta-learned models of cognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Heuristics from bounded meta-learned inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Endres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1042</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual decision making: drift-diffusion model is equivalent to a bayesian model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Blankenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparative review of dimension reduction methods in approximate bayesian computation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Sisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intuitive experimentation in the physical world</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="9" to="38" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active inductive inference in children and adults: A constructivist perspective</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="page">105471</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Homo heuristicus and the bias-variance dilemma</title>
		<author>
			<persName><forename type="first">H</forename><surname>Brighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Action, percepction and the brain: Adaptation and cephalic expression</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Schulkin</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="68" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Less is more: Local focus in continuous time causal learning</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Btesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lagnado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rational use of cognitive resources in human planning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Opheusden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-022-01332-8</idno>
		<ptr target="https://doi.org/10.1038/s41562-022-01332-8" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining the flaws in human random generation as local sampling with momentum</title>
		<author>
			<persName><forename type="first">L</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1011739</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perception in chess</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The probabilistic mind: Prospects for bayesian cognitive science</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oaksford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Is learning summary statistics necessary for likelihood-free inference? International conference on machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4529" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural approximate sufficient statistics for implicit models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10079</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The now-or-never bottleneck: A fundamental constraint on language</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A taxonomy of external and internal attention</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Golomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Turk-Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayes in the brain-on bayesian modelling in neuroscience</title>
		<author>
			<persName><forename type="first">M</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seriès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The British Journal for the Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="697" to="723" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The magical mystery four: How is working memory capacity limited, and why?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The frontier of simulationbased inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="30055" to="30062" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theory of learning to infer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="441" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Causal structure learning in continuous systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rehder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">244</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The pigeon as particle filter</title>
		<author>
			<persName><forename type="first">N</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="369" to="376" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear models in decision making</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corrigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The helmholtz machine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal versus naive diversification: How inefficient is the 1/n portfolio strategy?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Demiguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garlappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uppal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The review of Financial studies</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1915" to="1953" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sable-Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anaya Pozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page">20220050</biblScope>
			<date type="published" when="2023">2023. 2251</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The free-energy principle: a unified brain theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6245</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A counterfactual simulation model of causal judgments for physical events</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lagnado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="936" to="975" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intuitive theories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The oxford handbook of causal reasoning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Waldmann</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="515" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How to make cognitive illusions disappear: Beyond &quot;heuristics and biases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Review of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="115" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Homo heuristicus: Why biased minds make better inferences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="143" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasoning the fast and frugal way: models of bounded rationality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="669" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic mental models: a brunswikian theory of confidence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hoffrage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kleinbölting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="506" to="528" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bounded rationality: The adaptive toolbox</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<title level="m">Simple heuristics that make us smart</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding expertise: A multi-disciplinary approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gobet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Bloomsbury Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The recognition heuristic and the less-ismore effect</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of experimental economics results</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="987" to="992" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Continuous time causal structure induction with prevention and generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page">105530</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Active causal structure learning in continuous time</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mayrhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">101542</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rational causal induction from events in time. Psychological Review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pacer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="371" to="377" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>in press Mechanisms of theory formation in young children</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bayesian networks, bayesian learning and cognitive development</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="287" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Subjective randomness as statistical inference</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Austerweil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="85" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="229" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimal predictions in everyday cognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="767" to="773" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bridging levels of analysis for probabilistic models of cognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="268" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Perceptual decision-making as probabilistic inference by neural sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haefner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Inferring mass in complex scenes by mental simulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="61" to="76" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hoffrage</surname></persName>
		</author>
		<title level="m">Simple heuristics in a social world</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">People construct simplified mental representations to plan</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">606</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="2022">2022. 7912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Planning with theory of mind</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="959" to="971" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Probability theory: The logic of science</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning summary statistic for approximate bayesian computation via deep neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="1595" to="1618" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dead rats, dopamine, performance metrics, and peacock tails: proxy failure is an inherent risk in goaloriented systems</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Braganza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bayesian fundamentalism or enlightenment? on the explanatory status and theoretical contributions of bayesian models of cognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="169" to="188" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Approximately sufficient statistics and bayesian computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marjoram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Applications in Genetics and Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Farrar, Straus and Giroux</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Representativeness revisited: Attribute substitution in intuitive</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frederick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Heuristics and biases: The psychology of intuitive judgment</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="49" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Subjective probability: A judgment of representativeness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="454" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the psychology of prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="237" to="251" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Simulation-based bayesian inference under model misspecification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Warne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Nott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Drovandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2503.12315</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning to learn causal models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1185" to="1243" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning overhypotheses with hierarchical bayesian models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="321" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bayesian integration in sensorimotor learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="244" to="247" />
			<date type="published" when="2004">2004. 6971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Identifying resource-rational heuristics for risky choice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Holistic component of image perception in mammogram interpretation: gaze-tracking study</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Kundel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Nodine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Conant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">How cognitive modeling can benefit from hierarchical bayesian models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Evidence accumulation in decision making: Unifying the &quot;take the best&quot; and the &quot;rational&quot; models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Moneyball: The art of winning an unfair game</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>WW Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Strategy selection as rational metareasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="762" to="794" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Overrepresentation of extreme events in decision making reflects rational use of cognitive resources</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The anchoring bias reflects rational use of cognitive resources</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="349" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fundamentals and recent developments in approximate bayesian computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lintusaari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic Biology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="e82" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Ten-month-old infants infer the value of goals from the costs of actions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Spelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">6366</biblScope>
			<biblScope unit="page" from="1038" to="1041" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning the form of causal relationships using hierarchical bayesian models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="147" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo without likelihoods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marjoram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Molitor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Plagnol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tavaré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="15324" to="15328" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Naive and yet enlightened: From natural frequencies to fast and frugal decision trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vitouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takezawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Forster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thinking: Psychological perspectives on reasoning, judgment and decision making</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hardman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Macchi</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="189" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning to choose: Cognitive aging and strategy selection learning in decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Von Helversen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology and Aging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="309" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Prioritized memory access explains planning and hippocampal replay</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1609" to="1617" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The magical number seven, plus or minus two: Some limits on our capacity for processing information</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Probabilistic machine learning: Advanced topics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://probml.github.io/book2" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dark nudges in gambling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Newall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Addiction Research &amp; Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="67" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Probability matching in risky choice: The interplay of feedback and strategy availability</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rakow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Ravenzwaaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="329" to="338" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Empirical tests of a fast-and-frugal heuristic: Not everyone &quot;takes-the-best</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Shanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Attention, similarity, and the identification-categorization relationship</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="57" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Bayesian rationality: The probabilistic approach to human reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oaksford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sterratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd international conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="837" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Heuristics as bayesian inference under extreme priors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Parpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="127" to="144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Diffusion decision model: Current issues and history</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="260" to="281" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Bayesian model of behaviour in economic games</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>King-Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The paradox of time in dynamic causal systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bramley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">863</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Ssl: a theory of how people learn to select strategies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="236" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A language of thought for the mental representation of geometric shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sablé-Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">101527</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Statistical learning by 8-month-old infants</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Saffran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Newport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="issue">5294</biblScope>
			<biblScope unit="page" from="1926" to="1928" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Types of approximation for probabilistic cognition: Sampling and variational</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="98" to="101" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Bayesian brains without probabilities</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="883" to="893" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Rational approximations to rational models: alternative algorithms for category learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1144" to="1167" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Heuristics made easy: an effort-reduction framework</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Oppenheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="222" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A re-examination of probability matching and rational choice</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Shanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tunney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="250" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Neural implementation of hierarchical bayesian inference by importance sampling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1669" to="1677" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Administrative behavior</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1945">1945</date>
			<publisher>Simon and Schuster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A behavioral model of rational choice</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="118" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Invariants of human behavior</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Sequential monte carlo without likelihoods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Sisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1760" to="1765" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A bayesian analysis of dynamics in free recall</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1714" to="1722" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Sources of suboptimality in a minimalistic explore-exploit task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bnaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Decision by sampling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Approximate bayesian computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sunnåker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Busetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Numminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Foll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dessimoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1002803</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Toward nonprobabilistic explanations of learning and decision-making</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szollosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="546" to="568" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Inferring coalescence times from dna sequence data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tavaré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Balding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Donnelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="505" to="518" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<title level="m">Nudge: The final edition</title>
		<imprint>
			<publisher>Yale University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Ecological rationality: Intelligence in the world</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>OUP USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">4157</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="315" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Learning physical parameters from dynamic scenes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="57" to="82" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Bayesian models of conceptual development: Learning as building models of the world</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="533" to="558" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Discovering common hidden causes in sequences of events</title>
		<author>
			<persName><forename type="first">S</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="399" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Expertise increases planning depth in human gameplay</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Opheusden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuperwajs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galbiati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bnaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="issue">7967</biblScope>
			<biblScope unit="page" from="1000" to="1005" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">The tractable cognition thesis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Van Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="939" to="984" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Cognition and intractability: A guide to classical and parameterized complexity analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blokpoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwisthout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wareham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">One and done? optimal decisions from very few samples</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="637" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">An economist&apos;s perspective on probability matching</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vulkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Probabilistic simulation supports generalizable intuitive physics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jedoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1953" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Replay in minds and machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wittkuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hall-Mcmaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Schuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="367" to="388" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Statistical inference for noisy nonlinear ecological dynamic systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">466</biblScope>
			<biblScope unit="issue">7310</biblScope>
			<biblScope unit="page" from="1102" to="1104" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Probability matching as a computational strategy used in perception</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Beierholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1000871</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Word learning as bayesian inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="272" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Bayesian reverse-engineering considered as a research strategy for cognitive science</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zednik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jäkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="3951" to="3985" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">The optimality of naive bayes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the 17th international florida artificial intelligence research society conference</title>
		<meeting>the the 17th international florida artificial intelligence research society conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">How do people generalize causal relations over objects? a non-parametric bayesian account</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="44" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">An introduction to psychologically plausible sampling schemes for approximating bayesian inference</title>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sampling in judgment and decision making</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Juslin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Denrell</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="467" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">The bayesian sampler: Generic bayesian inference causes incoherence in human probability judgments</title>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="719" to="748" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">The autocorrelated bayesian sampler: A rational process for probability judgments, estimates, confidence intervals, choices, confidence judgments, and response times</title>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="456" to="493" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
