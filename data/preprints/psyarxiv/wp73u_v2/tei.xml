<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotation-tolerant representations elucidate the timecourse of high-level object processing</title>
				<funder ref="#_NqzRMnq">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_SrsBVnp #_xUdHfNF #_NUbsDf8">
					<orgName type="full">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Denise</forename><surname>Moerel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Behaviour and Development</orgName>
								<orgName type="institution" key="instit1">The MARCS Institute for Brain</orgName>
								<orgName type="institution" key="instit2">Western Sydney University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tijl</forename><surname>Grootswagers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Behaviour and Development</orgName>
								<orgName type="institution" key="instit1">The MARCS Institute for Brain</orgName>
								<orgName type="institution" key="instit2">Western Sydney University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer, Data and Mathematical Sciences</orgName>
								<orgName type="institution">Western Sydney University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amanda</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Queensland Brain Institute</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">the University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Engeler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><forename type="middle">O</forename><surname>Holcombe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rotation-tolerant representations elucidate the timecourse of high-level object processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">24A7BCED7766976F00F2EABCAC9A2809</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-18T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=true, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the very different retinal images that result from different viewing conditions, humans have little difficulty recognising visual objects in varying circumstances. One source of variability is 2-D rotation, which results in an object having different orientations. Here, we studied how the brain transforms rotated object images into object representations that are tolerant to rotation. We measured time-varying electroencephalography responses to object images shown in eight different orientations, presented at either 5 Hz or 20 Hz. We used multivariate classification to assess when rotation-tolerant object information emerged, and whether the rotation-tolerant processing would be limited at the faster presentation rate. We compared this to fixed-rotation measures of object decoding, where the classifier is trained and tested on the same orientation. Our results showed that both fixed-rotation and rotation-tolerant object decoding emerged at an early stage of processing, less than 100 ms after stimulus onset.</p><p>However, rotation-tolerant information peaked later than fixed-rotation information, suggesting rotation-tolerant object representations are most prominent during a late stage of processing, around 200 ms after stimulus onset.</p><p>Both fixed-rotation and rotation-tolerant object information was lower for the 20 Hz compared to 5 Hz presentation rate, which suggests that object information processing is disrupted, but not eliminated, for fast presentation rates. Our results show that object information arises at similar times in the brain regardless of whether it is investigated with the fixed-rotation or rotation-tolerant object decoding method. An object representation that is tolerant to rotation and generalises across different exemplars of the same object is established in later stages of processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Humans have little difficulty recognising objects across changes in rotation, size, position, and lighting. This implies that the brain creates and maintains representations of objects that are robust to these changes <ref type="bibr" target="#b6">(DiCarlo &amp; Cox, 2007;</ref><ref type="bibr" target="#b44">Rust &amp; DiCarlo, 2010)</ref>. Robust object representations can provide efficient neural encoding by reducing multiple specific object representations (e.g., different rotations) into one. This suggests that a given image elicits image-specific neural responses based on low-level features and is then transformed into a general object representation. Previous work has used multivariate decoding methods to provide insight into the dynamics of visual object representations in the human brain <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>. However, these studies have not assessed whether the decoded object information from the brain is driven by image-specific lowlevel features or whether it represents an object representation that is invariant, or tolerant, to transformations such as rotation. Here, we use the term 'tolerant' <ref type="bibr" target="#b7">(DiCarlo et al., 2012;</ref><ref type="bibr" target="#b44">Rust &amp; DiCarlo, 2010;</ref><ref type="bibr" target="#b49">Zoccolan et al., 2007)</ref> to highlight the different possible degrees of transformation tolerance, as opposed to binary 'invariance'. The present study was designed to yield insights into how objects are represented in the human brain and the process by which this specific to general transformation occurs. To this end, we compare the time-course of image-specific and rotation-tolerant representations.</p><p>Non-human primate studies have shown that object representations that are tolerant to different transformations emerge in the ventral visual stream. For instance, single neurons in the inferior temporal cortex (IT) are tolerant to rotation <ref type="bibr" target="#b39">(Ratan Murty &amp; Arun, 2015)</ref> as well as size and position <ref type="bibr" target="#b17">(Ito et al., 1995;</ref><ref type="bibr" target="#b15">Hung et al., 2005;</ref><ref type="bibr" target="#b49">Zoccolan et al., 2007;</ref><ref type="bibr" target="#b27">Li et al., 2009)</ref> and image contrast <ref type="bibr" target="#b49">(Zoccolan et al., 2007)</ref>, whereas neurons in earlier visual areas such as primary visual cortex respond to image specific features <ref type="bibr" target="#b14">(Hubel &amp; Wiesel, 1963)</ref>. In addition, tolerance to image transformations increases as information propagates along the ventral visual stream from V4 to IT <ref type="bibr" target="#b44">(Rust &amp; DiCarlo, 2010)</ref>. A nonhuman primate study found that size-tolerance developed early after stimulus onset, followed by position-tolerant signals, and then by rotation-tolerant and view-tolerant signals <ref type="bibr" target="#b40">(Ratan Murty &amp; Arun, 2017)</ref>. For clarity, rotation-tolerant refers to rotations in the picture plane, which do not reveal new object features, whereas view-tolerance refers to rotations that have a depth component, such that different aspects of the object become (in)visible. In this study, we focus on rotation-tolerance. The studies from non-human primates provide evidence for the emergence of transformation-tolerant object representations as information propagates along the ventral visual stream.</p><p>Previous decoding work with human participants has used magnetoencephalography (MEG) and electroencephalography (EEG) to investigate the time-course of the emergence of object information in the brain <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>. Specifically, the studies looked at three different levels of categorical abstraction: the "animacy level" (judgments of whether an image represented something that can move on its own volition), high-level categories <ref type="bibr">(e.g., mammal, fruit, tool, etc.)</ref>, and the object category level (e.g., giraffe, apple, hammer, etc.). The results showed evidence for object-specific information in the brain earlier than 100 ms after stimulus onset in each of these studies. However, it is still unclear whether this information reflects a representation that is tolerant to transformation. A linear classifier will use all information that helps to distinguish two objects from the neural signal, so the decoded object information is likely to be driven both by low-level differences between objects as well as higher-level object representations. This means that decoding times found in previous work might not be a good reflection of when the brain has truly established a representation of the object that is tolerant to transformation.</p><p>Studies that have investigated the emergence of high-level category information have found a different temporal signature compared to object exemplar decoding <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>. In these studies, the first decoding peak for object exemplar decoding is around 120 ms, but this peak is not usually observed in high level category decoding. Instead, the high-level category decoding peak is found around 200 ms after stimulus onset. Information about animacy peaks later, usually around 400 ms after stimulus onset. This suggests that at 200 ms, a representation of the high-level object category has emerged and information from a specific exemplar is generalised to other exemplars within that category. This time-course is also consistent with object decoding when the training and test set contain different exemplars of the same object <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>, again suggesting that the information has generalised to other exemplars within the category. It is possible that the time-course of the emergence of rotation-tolerant object representations resembles that of highlevel category decoding, as both access representations that are more tolerant to low-level pixel-wise differences between stimulus images.</p><p>A few studies have used time-resolved neuroimaging to provide insight into how transformation tolerant object representations emerge throughout the human visual system. Using MEG, <ref type="bibr">Carlson and colleagues (2011)</ref> showed position-tolerant object representations emerged around 100 ms after stimulus onset. <ref type="bibr">Isik and colleagues (2014)</ref> used MEG to investigate the emergence of position-and size-tolerant information in object processing and found that tolerant representations emerged later than non-tolerant information. Sizetolerant information peaked first, with a later peak for position-tolerant information. In addition, tolerance to size and position increased along the ventral stream. Supporting this finding, EEG evidence showed tolerance to viewpoint followed size-and position-tolerance <ref type="bibr" target="#b20">(Karimi-Rouzbahani et al., 2017)</ref>.</p><p>These findings from humans are in line with the findings in non-human primates reviewed above <ref type="bibr" target="#b40">(Ratan Murty &amp; Arun, 2017)</ref>. Together, they suggest that transformation-tolerant representations emerge while undergoing additional processing stages within the ventral visual stream. The studies described above have investigated the tolerance to different transformations but have not explored the tolerance to rotation specifically. It is therefore still unclear how rotation-tolerant object representations emerge in the human brain. Rotation is arguably a more complex transformation than the scaling and translation needed to reconcile size-and position-tolerance, which may be why rotation tolerance emerges later in non-human primates (Ratan Murty &amp; Arun, 2017).</p><p>To investigate how the visual system reconciles different rotations into a robust representation, we investigated the time-course of rotation-tolerant information processing in the human brain in this study. In addition, we asked whether we could disrupt the rotation-tolerant object processing by presenting stimuli at faster speeds, as previous work has shown that faster presentation rates limit visual information processing <ref type="bibr" target="#b3">(Collins et al., 2018;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019;</ref><ref type="bibr" target="#b10">Grootswagers, Robinson, Shatek, et al., 2019;</ref><ref type="bibr" target="#b29">McKeeff et al., 2007;</ref><ref type="bibr" target="#b41">Robinson et al., 2019)</ref>, particularly high-level information processing <ref type="bibr" target="#b13">(Holcombe, 2009;</ref><ref type="bibr" target="#b29">McKeeff et al., 2007)</ref>. We presented sequences of object stimuli in eight in-plane rotations, at 5 Hz and at 20 Hz, and used multivariate decoding methods to investigate the coding of rotation-tolerant information over time. The comparison between rotation-tolerant and fixed-rotation object decoding can show how the brain transforms image-specific representations into a general object representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Participants</head><p>Sixteen healthy adults participated in the study (age range = 19-60 years, 14 female / 2 male, 14 right-handed / 2 left-handed). All participants reported having normal or corrected to normal vision. Participants received monetary payment or undergraduate course credit for their participation. The study was approved by the ethics committee of The University of Sydney, and participants provided informed consent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Stimuli and experimental design. A)</head><p>The 60 stimuli were grouped into two categorical levels: 1) animacy, and 2) object category. Each object category contained five exemplars. The 0° rotation is displayed in A. B) The eight rotations that each stimulus was presented in ranged from 0° to 315° in steps of 45°. C) The stimulus sequences were presented at two frequencies: 5 Hz and 20 Hz. The stimuli were presented for 33 ms in both presentation frequency conditions, so the interval between consecutive stimuli was 167 ms for the 5 Hz condition and 17 ms for the 20 Hz condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stimuli and experiment procedure</head><p>Figure <ref type="figure">1A</ref> provides an overview of the stimuli. The stimulus set consisted of 60 objects, taken from <ref type="bibr">Contini et al. (2020)</ref> and an online image bank (<ref type="url" target="http://www.pngimg.com/">www.pngimg.com</ref>). The images can be grouped at the animacy level, with 30 animate and 30 inanimate images, or at the category level, with 12 categories (e.g., humans, birds, toys). Each category contained five objects. We presented the object stimuli in eight rotations: 0°, 45°, 90°, 135°, 180°, 225°, 270°, and 315° (see Figure <ref type="figure">1B</ref>). The stimuli were presented at fixation on a mid-grey background <ref type="bibr">(RGB = 128,</ref><ref type="bibr">128,</ref><ref type="bibr">128)</ref>, at a distance of approximately 60 cm. The stimuli were 512</p><p>x 512 pixels (approximately 12.50 degrees visual angle) in size.</p><p>Participants viewed 120 sequences of images, and each sequence contained 120 object images. There were 15 repetitions per object, rotation, and presentation frequency condition, resulting in a total of 14,400 stimulus presentations. In each sequence the 60 images were shown twice in two different rotations. The stimulus items in each sequence were presented in random order, and each object-rotation combination in the sequence was unique compared to the other stimuli in the sequence. Four consecutive blocks contained a single repeat of all objects and rotations. We presented the image sequences at two frequencies, 5 Hz and 20 Hz. Figure <ref type="figure">1C</ref> shows part of an example sequence for both presentation frequency conditions. Stimuli were presented for 33 ms in both presentation frequency conditions. In the 5 Hz condition, the interval between successive stimuli was 167 ms, whereas the interval was 17 ms in the 20 Hz condition. Each sequence had a single stimulus presentation frequency, and the counterbalanced presentation frequencies were presented in pseudo-random order. The experiment took approximately 45 minutes to complete.</p><p>Participants were instructed to maintain fixation at a central bullseye for the duration of the experiment and to respond to an infrequent target by pressing a button. The purpose of this task was to keep participants engaged in the experiment, and to ensure participants remained fixating at the centre of the objects. The bullseye consisted of two black rings and a white ring in between (see Figure <ref type="figure">1C</ref>). The target consisted of a black filled circle the same size as the fixation bullseye and was presented instead of the fixation bullseye for the duration of a stimulus presentation (33 ms). Each sequence contained between two and four targets. We placed the targets at random in each sequence, using the following constraints. Two targets were spaced at least 10 stimulus presentations apart, and the first and last 10 stimulus presentations were never a target. We counted responses within 1 s of the target as correct. Participants had an average accuracy of 89% on the fixation colour change detection task (SEM = 2.1%, range = 70.5% to 99.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">EEG acquisition and pre-processing</head><p>We used a BrainVision ActiChamp system to record continuous EEG data from 128 electrodes, digitised at a sample rate of 1000Hz. The electrodes were placed according to the international standard 10-20 electrode placement system <ref type="bibr" target="#b37">(Oostenveld &amp; Praamstra, 2001)</ref>, and were referenced online to FCz. We used the EEGlab Matlab toolbox to pre-process the data <ref type="bibr" target="#b5">(Delorme &amp; Makeig, 2004)</ref>, following a pre-processing pipeline used in previous work <ref type="bibr" target="#b11">(Grootswagers et al., 2021;</ref><ref type="bibr" target="#b42">Robinson et al., 2020)</ref>. We interpolated bad channels that measured more than 5 standard deviations away from the average, using the kurtosis measure.</p><p>We re-referenced the data to an average reference and filtered the data using 0.1 Hz high pass and 100 Hz low pass Hamming windowed FIR filters. We down-sampled the data to 250 Hz and created epochs for each stimulus in the sequence from -100 ms to 800 ms relative to stimulus onset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Decoding analysis</head><p>We applied decoding analyses to determine the time-course of fixed-rotation measures of object information and rotation-tolerant object information, using the CoSMoMVPA toolbox for Matlab <ref type="bibr" target="#b38">(Oosterhof et al., 2016)</ref>. We performed the analyses within-subject, separately for the 5 Hz and 20 Hz sequences. For each time-point in the epoch, we trained a linear discriminant analysis (LDA) classifier to distinguish between the 60 objects (see Figure <ref type="figure">1A</ref>). As we used a 60-way object classification analysis, chance performance was 1.67%. To decode the fixedrotation object information, we followed the method used in previous work <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>, by training the classifier on the same rotation as used in the test set. For each unique combination of object and rotation, there were 15 repetitions of the identical stimulus over the entire experiment for each presentation speed condition. The training set consisted of 14 of the 15 identical stimulus repetitions of each of the 60 objects and the test set consisted of the leftout stimulus repetition of each object in the same rotation, for example training on 14 repetitions of all 60 objects at 0° rotation and testing on one repetition of all 60 objects at 0°. We repeated this analysis 15 times, leaving out a different stimulus repetition of each of the 60 objects each time, and averaged across decoding accuracies. We also repeated this analysis for all the rotations, and averaged decoding accuracies across them. This analysis was done for each timepoint. To decode rotation-tolerant object information, we trained the classifier to distinguish between the 60 objects on 7 different rotations of an object and tested the classifier on the eighth rotation. We ensured that the classifiers were trained and tested on the same amount of data as the fixed-rotation method by always training on 14 repetitions of each object and testing on one repetition of each object, for example training on 14 repetitions of all 60 objects, with 2 repeats per orientation (45°, 90°, 135°, 180°, 225°, 270°, and 315°) and testing on the left-out repetition of all 60 objects at 0°. Note that we tested the classifier on the exact same trials for the fixed-rotation and the rotation-tolerant object decoding analyses. The only difference between the different analyses was that the models were always trained and tested on the same rotation angle in the fixed-rotation object decoding analysis, consistent with previous studies <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>, whereas rotation-tolerant models were trained and tested on different rotation angles. It is therefore possible to directly compare classifier accuracies between fixed-rotation and rotation-tolerant object decoding analyses. In addition to the object decoding, we repeated the same analysis described above at the animacy level, decoding animate vs. inanimate, and at the category level, using 12-way category decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Exploratory analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Channel searchlight</head><p>To investigate which EEG channels were driving the classification accuracies, we obtained time-varying topographies by performing an exploratory channelsearchlight analysis, following an established pipeline <ref type="bibr" target="#b10">(Grootswagers, Robinson, Shatek, et al., 2019;</ref><ref type="bibr" target="#b41">Robinson et al., 2019)</ref>. We constructed a local cluster for each EEG channel by taking the closest 4 neighbouring channels and ran the decoding analysis described above separately for the fixed-rotation and rotationtolerant object coding analyses, for the 5 Hz and 20 Hz conditions. The decoding accuracy for each cluster was stored in the centre channel, resulting in a time-by-channel map of the decoding accuracies for each condition, separately for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">Representational Similarity Analysis</head><p>We used the Representational Similarity Analysis (RSA) framework <ref type="bibr" target="#b25">(Kriegeskorte et al., 2008;</ref><ref type="bibr" target="#b24">Kriegeskorte &amp; Kievit, 2013)</ref> to investigate the temporal dynamics of rotation-tolerant object representations, while reducing the contribution of lowlevel differences in image features. This framework allows us to compare the representational structure obtained by EEG to different models of object representations, using representational dissimilarity matrices (RDMs) that represent the dissimilarity between item activations. Figure <ref type="figure" target="#fig_0">2</ref> gives an overview of this analysis. We created a 480 by 480 neural RDM from the EEG data, which coded for the neural dissimilarity between each combination of unique stimuli (60 object images in 8 rotations, see Figure <ref type="figure" target="#fig_0">2B</ref>). We used cross-validated pairwise decoding accuracy as a measure of dissimilarity, as a more dissimilar neural response between items results in a higher decoding accuracy. We obtained a neural RDM for each participant, separately for each time-point and for the 5 Hz and 20 Hz stimulus presentation conditions (Figure <ref type="figure" target="#fig_0">2A</ref>). For each time-point, we correlated the lower triangle of the neural RDM to a model RDM that codes for the object in a rotation-tolerant way (Figure <ref type="figure" target="#fig_0">2C</ref>).</p><p>We partialled out three control models from this correlation (Figure <ref type="figure" target="#fig_0">2D</ref>), to control for low-level differences in image features, following previous work <ref type="bibr" target="#b32">(Moerel et al., 2024</ref><ref type="bibr" target="#b31">(Moerel et al., , 2025))</ref>. We used the layers "V1", "V2", and "V4", from CORnet-S <ref type="bibr" target="#b26">(Kubilius et al., 2019)</ref>, a deep convolutional artificial neural network, as control models. These layers are designed to be analogous to the ventral visual areas V1, V2, and V4 respectively. We used cosine as a distance measure to obtain the dissimilarity between each pair in the 480 by 480 RDM. The dissimilarites between the three CORnet control models and the rotation-tolerant object model are shown in Figure <ref type="figure" target="#fig_0">2E</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.">Temporal generalisation</head><p>We applied the temporal generalisation approach <ref type="bibr" target="#b0">(Carlson et al., 2011;</ref><ref type="bibr" target="#b22">King &amp; Dehaene, 2014;</ref><ref type="bibr" target="#b30">Meyers et al., 2008)</ref> to determine 1) whether object representations at one time-point generalised to other times, and 2) whether the object representations were similar for the 5 Hz and 20 Hz presentation conditions. Instead of training and testing the classifier on a single time-point, the classifier is trained and tested on all combinations of time-points. This results in a training time by test time time-generalisation matrix and allows us to capture similar object representations that occur at different times. To investigate the temporal generalisation between object representations over time, we trained and tested a classifier in the same way as described in the decoding analysis above for each combination of training and testing time-points. We did this separately for the 5 Hz and 20 Hz conditions. To determine the overlap in object representations between the 5 Hz and 20 Hz presentation conditions, we trained a classifier on the 5 Hz condition for a time-point in the epoch and then tested the classifier on every time-point in the 20 Hz condition. We repeated this analysis for all training time-points. In addition, we trained on the 20 Hz condition and tested on the 5 Hz condition, and averaged the two time-generalisation matrices <ref type="bibr">(Grootswagers, Robinson, &amp; Carlson, 2019;</ref><ref type="bibr" target="#b19">Kaiser et al., 2016)</ref>. The labels for all 480 by 480 matrices plotted here: the rows and columns consist of 12 categories x 5 exemplars x 8 rotations. C) The rotation-tolerant object model. This model predicts that the response to the same object is very similar, regardless of the orientation in which the object is presented, whereas the response to two different objects is dissimilar. D) The CORnet-S control models that were used to control for low-level visual differences between the stimuli. E) The dissimilarity between the four different models: the rotation-tolerant object model and the three control models..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Statistical inference</head><p>We used Bayesian statistics <ref type="bibr" target="#b8">(Dienes, 2011;</ref><ref type="bibr" target="#b21">Kass &amp; Raftery, 1995;</ref><ref type="bibr" target="#b34">Morey et al., 2016;</ref><ref type="bibr" target="#b43">Rouder et al., 2009;</ref><ref type="bibr" target="#b47">Wagenmakers, 2007)</ref> to determine whether decoding accuracies were at chance level, or instead substantially higher. We used the Bayes Factor R package <ref type="bibr" target="#b36">(Morey &amp; Rouder, 2018)</ref> to implement the statistical analyses. We implemented the same analysis for the fixed-rotation and the rotation-tolerant object decoding analyses, for both the 5 Hz and the 20 Hz presentation conditions. For each time-point, we applied a Bayesian t-test with a point null and a notched half-Cauchy prior for the alternative hypothesis to test for directional effects <ref type="bibr" target="#b35">(Morey &amp; Rouder, 2011)</ref>. The half-Cauchy prior was centred around d = 0 (i.e., chance-level decoding accuracy of 1.67%) and had the default width of 0.707 <ref type="bibr" target="#b18">(Jeffreys, 1998;</ref><ref type="bibr" target="#b43">Rouder et al., 2009;</ref><ref type="bibr" target="#b48">Wetzels et al., 2011)</ref>.</p><p>We excluded the interval between d = 0 and d = 0.5 (the "notch") from the prior <ref type="bibr" target="#b45">(Teichmann et al., 2022)</ref>. Because this analysis pits a point null hypothesis against a d &gt; 0.5 alternative hypothesis, this analysis tests whether the decoding is substantially above chance not just marginally above chance. We repeated this analysis for each time-point. All statistical analyses were applied at the group level.</p><p>To test whether there was a difference between the coding of object information found with the fixed-rotation and rotation-tolerant object decoding analyses, we calculated the difference between these decoding accuracies and used a Bayesian t-test to test the difference score against 0 for each time-point.</p><p>We used a point null and a full-Cauchy prior for the alternative hypothesis to test for differences in either direction between the coding of object information found with the fixed-rotation and rotation-tolerant object decoding analyses. For consistency with the t-test described above, we used the default prior width of 0.707 with a notch (excluding the interval between d = -0.5 and d = 0.5) from the prior.</p><p>We used the same statistical analysis described above for the representational similarity analysis. For the temporal generalisation analysis, we repeated the same statistical analysis for each combination of training and test time-points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fixed-rotation and rotation-tolerant object decoding analyses</head><p>The temporal dynamics of object representations obtained with fixed-rotation decoding and a rotation-tolerant decoding method are shown in Figure <ref type="figure">3</ref>. For the 5 Hz presentation condition (Figure <ref type="figure">3A</ref>), there was clear evidence (BF &gt; 10) for substantially above-chance fixed-rotation object decoding from ~84 ms after stimulus onset. Clear evidence for substantially above chance decoding of the rotation-tolerant object information emerged at a similar time, approximately 92 ms after stimulus onset. Visual inspection shows there are two distinct peaks, between approximately 100 ms -150 ms after stimulus onset, and between approximately 150 ms to 250 ms after stimulus onset. For the object coding found through the fixed-rotation method, the first peak was higher compared to the second peak, with the overall peak found at ~116 ms after stimulus onset. In contrast, the second peak was higher than the first for the rotation-tolerant decoding, with the overall peak found at ~192 ms after stimulus onset. From ~88 ms after stimulus onset, the fixed-rotation object decoding had higher accuracy than the rotation-tolerant object decoding. Together, these results suggest that object information emerges around a similar time regardless of whether we trained and tested on the same orientation or on different orientations. However, rotation-tolerant object information peaked later than the fixed-rotation analysis, suggesting that object representations are most tolerant to image rotation at later stages of processing.</p><p>We repeated the same analysis for the 20 Hz presentation condition. We aimed to provide insight into the depth of object processing by comparing object coding for fast and slower presentation rates. Previous work showed a decrease in object decoding accuracy when stimuli are presented at a fast compared to slower presentation rate <ref type="bibr" target="#b41">(Robinson et al., 2019)</ref>, suggesting that faster presentation rates impaired visual processing. Figure <ref type="figure">3B</ref> shows the decoding accuracies for object coding for the 20 Hz presentation condition, obtained for the fixed-rotation and rotation-tolerant analyses. Both fixed-rotation and rotationtolerant object information could be decoded at a similar time. For fixed-rotation decoding, when the classifier was trained and tested on the same rotation, the evidence began to clearly favour substantially above-chance decoding from ~96 ms after stimulus onset, with peak decoding at ~116 ms. There was clear evidence for substantially above-chance rotation-tolerant object decoding from ~112 ms after stimulus onset, peaking at ~120 ms after stimulus onset. There was substantial evidence for stronger object decoding for the fixed-rotation analysis compared to rotation-tolerant analysis starting at ~100 ms after stimulus onset. These results show that rotation-tolerant information still emerges for a high presentation rate, though there was not an obvious late peak as in the slower condition.</p><p>Directly comparing the 5 Hz and 20 Hz presentation conditions with a notched Bayesian t-test, object coding was stronger for the 5 Hz condition for both the fixed-rotation analysis, starting at ~92 ms after stimulus onset, and the rotation-tolerant analysis, starting at ~104 ms after stimulus onset. Together, these results suggest that the 20 Hz presentation speed limits, but does not eliminate, both fixed-rotation and rotation-tolerant object processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Time-course of object decoding accuracy for 5 Hz (A) and 20 Hz (B) presentation. A)</head><p>Object decoding data for the 5 Hz presentation. We used linear classifiers to decode object information via the fixed-rotation decoding method (blue), that were trained and tested on the same rotation, and in a rotation-tolerant way (orange), where we trained and tested on different object rotations. Decoding accuracies are based on 60-way decoding, which means chance is 1.67% decoding accuracy. The shaded areas around the plot lines show the bootstrapped 95% confidence intervals across participants. The time-varying topographies, which were obtained from the exploratory channel-searchlight analysis, are displayed below each plot, averaged within 50 ms time bins. We obtained separate topographies for the fixed-rotation (blue) and rotation-tolerant (orange) object decoding methods. Notched Bayes factors (excluding small positive effect sizes from the prior) are shown below the plot. Notched Bayes factors below 1/10 are shown in grey and those above 10 are shown in colour. B) Object decoding accuracy for the fixed-rotation analysis (blue) and the rotationtolerant (orange) analysis for the 20 Hz presentation condition. Plotting conventions are the same as in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Representational similarity analysis</head><p>In the analysis described above, we assessed the decoding accuracy for rotationtolerant object information by training the classifier on all but one rotation and testing the classifier on the left-out rotation. This means that successful rotationtolerant object decoding requires generalisation between different rotations and cannot depend on specific pixel-wise configurations between stimulus images. This decoding analysis is therefore less likely to reflect low-level differences in object features compared to the fixed-rotation object decoding. However, it is possible that the decoding of the rotation-tolerant object is driven in part by lowlevel visual differences between the object images. We used representational similarity analysis to account for low-level visual differences, by using control models based on CORnet-S. This allows us to investigate the time-course of rotation-tolerant object coding when further reducing the contribution of these low-level visual differences. partial correlation was ~184 ms after stimulus onset. The timings for the 20 Hz condition were very similar, we found substantial evidence for a partial correlation with the rotation-tolerant object model from ~100 ms after stimulus onset. The time-courses for the representational similarity analysis are very similar to those found in the decoding analysis, suggesting rotation-tolerant object representations are evident when images are presented at both 5 Hz and 20 Hz and are unlikely to be driven by low-level visual information about the different objects. Directly comparing the partial correlations of the rotationtolerant object model with the EEG data from the 5 Hz and 20 Hz conditions, a difference was evident from ~164 ms after stimulus onset, with a higher correlation with the rotation-tolerant object model for the 5 Hz compared to 20 Hz condition. We made a two-dimensional embedding of the RDM to visualise the dynamic representational structure for the images presented at 5 Hz, during the first peak (100 ms -150 ms) and second peak (150 ms -250 ms) (Figure 4, top panel). The distance between the images reflects their mean dissimilarity across participants. At 150 ms -250 ms after stimulus onset, a clear clustering emerges of different rotations of the same images, suggesting the object representation is then tolerant to rotation. In addition, the faces and bodies become separated from the other object categories at this time, suggesting there is a distinct representation of human bodies and human faces at 150 ms -250 ms after stimulus onset. The inset on the bottom left corner of each plot shows the same stimuli, with each dot representing a stimulus and the colours representing the 10 different categories. The categories become noticeably more separated at the later timewindow compared to the earlier time-window, especially the people and faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal generalisation</head><p>We used the temporal generalisation approach to investigate 1) whether activation patterns observed at one time-point generalise to other time-points, and 2) whether the object representations were similar between the 5 Hz and 20 Hz presentation conditions. Temporal generalisation results are displayed in Figure <ref type="figure">5</ref>. To determine whether activation patterns generalise between different time-windows, we trained and tested the classifier to distinguish between objects for all combinations of training and test time-points. We did this separately for the fixed-rotation (Figure <ref type="figure">5A</ref>) and rotation-tolerant (Figure <ref type="figure">5B</ref>) object decoding analyses, and for the 5 Hz and 20 Hz presentation conditions. Decoding was strongest along the diagonal for all conditions, when the classifier was trained and tested on time-points that were close in time. While some generalisation was present for neighbouring time-points, the Bayes factors suggest no generalisation between the two different decoding peaks, around 120 ms and 200 ms, which suggests that the decoding in these two time-windows is driven by different patterns of activation. However, there was some evidence for generalisation between the first decoding peak and a much later time-window, around 350 ms to 500 ms after stimulus onset.</p><p>To determine whether the object representations were similar for the two different presentation rates, we trained on the 5 Hz condition and tested on the 20 Hz condition and vice versa and collapsed the time-generalisation matrices for these two analyses. We transposed the train on 20 Hz, test on 5 Hz timegeneralisation matrix before averaging, which means above-chance decoding off the diagonal shows that similar activation occurs at different times for the two presentation speed conditions. We found evidence for cross-decoding between the 5 Hz and 20 Hz conditions for both the fixed-rotation and the rotation-tolerant object decoding analyses. This suggests that the object representations were similar for the two presentation rates. In addition, the decoding was primarily along the diagonal for the cross-decoding between the 5 Hz and 20 Hz conditions, indicating this information was processed around the same times in both speed conditions. However, there was stronger decoding above compared to below the diagonal for the rotation-tolerant object decoding, suggesting the rotationtolerant object coding for the 5 Hz condition lasted longer compared to the 20 Hz condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Temporal-generalisation for the fixed-rotation object decoding (A) and rotation-tolerant object decoding (B).</head><p>We used 60-way object decoding, which means that theoretical chance is at 1.67%. A) Shows the temporal-generalisation of decoding accuracies for the fixed-rotation object coding analysis (top row), with the corresponding notched Bayes factors on a logarithmic scale (bottom row). The left panels show the results for the 5 Hz stimulus presentation condition, and the middle panels for the 20 Hz presentation condition. The training times are shown on the y-axis and the test times on the xaxis. The right panels show the cross-decoding between the 5 Hz and 20 Hz condition. For this analysis, we trained on the 5 Hz condition and tested on the 20 Hz condition and vice versa. We averaged the results across the two analyses, transposing the train on 5 Hz test on 20 Hz matrix. The y-axis shows the times for the 5 Hz condition, and the x-axis the times for the 20 Hz condition B) shows the temporal-generalisation of decoding accuracies for the rotation-tolerant object coding (top row) and the corresponding notched Bayes factors on a logarithmic scale (bottom row). Plotting conventions are the same as in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>In this study we investigated the time-course of neural information about object representations that are tolerant to rotation. In line with previous studies <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>, our results showed evidence for object coding, including above-chance decoding for the object exemplar when we used the fixedrotation method of training and testing on the same object rotation. Our results add to these findings by providing evidence for the coding of rotation-tolerant object information, and directly contrasting object decoding found through the fixed-rotation decoding method and the rotation-tolerant object decoding method. The evidence began to favour substantially above-chance object decoding around similar times for the fixed-rotation and rotation-tolerant decoding methods, approximately 84-92 ms after stimulus onset. However, the decoding of the rotation-tolerant object information peaked later than the decoding accuracy obtained via the fixed-rotation method, suggesting that rotation-specific information is transformed into a higher-level representation, tolerant to rotation, at later stages of processing. When we presented object stimuli at a faster rate (20 Hz), there was still evidence of both fixed-rotation and rotation-tolerant neural representations, although decoding accuracies were lower for stimuli presented at the faster compared to slower (5 Hz) presentation rate. We suspect this happens in part due to masking degrading the signal quality generally, and in part due to the fast presentation speed reducing the depth of processing <ref type="bibr" target="#b13">(Holcombe, 2009;</ref><ref type="bibr" target="#b29">McKeeff et al., 2007)</ref> and therefore limiting the amount of rotation-tolerant object information. Previous work has investigated the time-course of object representations that are tolerant to size and position <ref type="bibr" target="#b16">(Isik et al., 2014)</ref>. The authors found that standard object coding, that was not necessarily tolerant to size or position, peaked at 135 ms. Size-tolerant decoding peaked at 170 ms, and position-tolerant decoding peaked at 180 ms after stimulus onset. Our findings add to this by investigating rotation-tolerant decoding. We showed that fixed-rotation object coding peaked ~116 ms after stimulus onset, whereas rotation-tolerant object coding peaked at ~192 ms after stimulus onset. Although it is not possible to directly compare the peak times across these different studies because of different stimuli and methods, our findings suggest that the rotation-tolerant representation is fully established later than the size-tolerant and positiontolerant representation. This would be in line with findings in non-human primates (Ratan Murty &amp; Arun, 2017) as well as findings in humans at the category level <ref type="bibr" target="#b20">(Karimi-Rouzbahani et al., 2017)</ref>. Our findings therefore contribute to our understanding of how the human brain arrives at object representations that are tolerant to different transformations. In addition, our RSA method adds to previous findings by controlling for the contribution of low-level visual differences between stimuli. An object presented in different sizes, positions, or rotations will have consistent low-level visual features, such as luminance, which can be used by a classifier. The RSA method used in this study controls for the contribution of these features, allowing us to focus on high level representations that are tolerant to differences in rotation.</p><p>Previous work has provided insight into the time-course of the emergence of object information in the brain using fixed-rotation decoding methods, where the classifier is trained and tested on objects presented in the same (canonical) orientation <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>. One limitation of this method is that differences in activation patterns caused by the different objects can be driven by low-level differences between objects. It is possible that there is information in perceptual areas about the low-level differences between objects, without having true object representations. The classifier cannot distinguish between these two possibilities and is likely to rely on both. By training and testing the classifier on different object rotations, we can assess the object-related information that generalises between different rotations. This helps us tap into the higher-level representations of objects that are tolerant to rotation, minimising the contribution of low-level differences in object features. Our results show a peak in rotation-tolerant object coding around 200 ms, which is approximately 80 ms later than the peak found for the fixed-rotation object decoding. The time-course of rotation-tolerant object information fits with that found for object decoding when training and testing on different exemplars of the same object in previous work <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>. A similar time-course has been found for category coding, which also shows a peak in decoding around 200 ms after stimulus onset <ref type="bibr" target="#b1">(Carlson et al., 2013;</ref><ref type="bibr" target="#b2">Cichy et al., 2014;</ref><ref type="bibr" target="#b4">Contini et al., 2017;</ref><ref type="bibr">Grootswagers, Robinson, &amp; Carlson, 2019)</ref>. This suggests that around this time a rich object representation has been established that is tolerant to rotation and generalises across different exemplars of the same object and category.</p><p>The time-course of decoding shows that the object representation persists after the presentation of the next stimulus in the sequence. If one assumes an afferent latency of around 50-70 ms to reach visual cortex <ref type="bibr" target="#b12">(Grootswagers et al., 2024;</ref><ref type="bibr" target="#b46">Thorpe &amp; Fabre-Thorpe, 2001)</ref>, in line with when substantially abovechance decoding seems to emerge, then 200 ms later in the 5 Hz condition and 50 ms later in the 20 Hz condition, the next stimulus presented will have reached visual cortex, presumably interrupting any feedback loops <ref type="bibr" target="#b33">(Mohsenzadeh et al., 2018)</ref>. However, substantially above-chance decoding persists well beyond these times, which could be explained by a combination of the new stimulus not yet reaching particularly high level representations and possibly multiplexing, wherein successive stimuli are represented by distinct activation patterns that can coexist <ref type="bibr" target="#b23">(King &amp; Wyart, 2021;</ref><ref type="bibr" target="#b28">Marti &amp; Dehaene, 2017)</ref>. Our results add to the previous literature by showing that not only low-level features can persist, but that high-level rotation-tolerant representations of different stimuli can be present in the brain simultaneously. Further, we showed that in the 20Hz condition, when processing was interrupted before the rotation-tolerant representation was fully formed (at ~120 ms), multiple object representations were maintained, but considerably less rotation-tolerant information propagated through the system.</p><p>The decoding results suggest that the fixed-rotation object decoding method relies more on low-level visual differences between images compared to the rotation-tolerant approach. However, it is still possible that the classifier uses low-level differences for the rotation-tolerant image classification. We therefore used RSA to further minimise the effect of these low-level visual differences. Specifically, we correlated RDMs of the EEG data for the two different presentation rates with a model RDM for the rotation-tolerant object, while partialling out different low-level visual models. For the 5 Hz condition, the peak time of the partial correlation between the EEG and the rotation-tolerant model was ~184 ms after stimulus onset. This peak time is a similar time as the peak of rotation-tolerant decoding, suggesting higher-level object information contributed more strongly to the rotation-tolerant object decoding than low-level differences between images, as partialling out the low-level visual models did not impact the peak decoding time. However, when comparing the RSA and decoding methods for the comparison between the 5 Hz and 20 Hz conditions, our RSA findings revealed stronger partial correlations for the 5 Hz compared to 20 Hz condition starting only after ~164 ms. This is ~76 ms later than the difference between decoding accuracies for the 5 Hz and 20 Hz conditions found through the decoding analysis. This implies that there are no differences for the first peak (at ~120 ms) between the 5 Hz and 20 Hz conditions when the low-level visual information is partialled out. However, there is lower rotation-tolerant information for the 20 Hz compared to 5 Hz condition in the second peak (at ~200ms). This suggests that faster presentation speeds limit higher-level object representations more than the representation of low-level visual information.</p><p>The cross-decoding indicates that the rotation-tolerant object representations for the fast and slow presentation rates are similar, although the asymmetry of some off-diagonal decoding suggests that the representation of rotation-tolerant object information lasted longer for the 5 Hz compared to the 20 Hz condition. This finding is in line with previous work <ref type="bibr">(Grootswagers, Robinson, &amp; Carlson, 2019;</ref><ref type="bibr" target="#b41">Robinson et al., 2019</ref>) that showed a disruption, but not elimination, of object and category processing when objects were presented at 20 Hz compared to 5 Hz. Together, these results suggest that although the faster presentation speed affects object processing, it does not fully disrupt the object processing.</p><p>One interesting aspect of the apparent transition from orientation-specific to rotation-tolerant representation is that there appears to be two distinct processing stages rather than a smooth transition, though we are limited by EEG's sensitivity to cortical responses strongest at the scalp. That is, one might have expected a gradual, smooth decrease in the fixed-rotation information as the rotation-tolerant information emerged. Instead, the data show two peaks in the fixed-rotation object decoding, as if two distinct processing stages are involved. This was also observed in some previous papers <ref type="bibr">(Grootswagers, Robinson, &amp; Carlson, 2019;</ref><ref type="bibr" target="#b10">Grootswagers, Robinson, Shatek, et al., 2019;</ref><ref type="bibr" target="#b41">Robinson et al., 2019)</ref>. An interesting aspect of the temporal generalisation data is also consistent with the theory that there are two discrete processing stages. At about the same time as the decoding accuracy peaks, the temporal generalisation matrix shows two peaks of generalisation, which are separated by a short low generalisation interval at about 160 ms. By "low generalisation interval", we mean that training on the EEG data at that time yields reasonable decoding accuracy but very little generalisation to other times, compared for instance to 220 ms, when the decoding accuracy is about the same but much more generalisation occurs. This is consistent with the brain's representations at 160 ms being quite transient (not generalising to other times), a sort of transitional period as the rotation-tolerant information emerges. Perhaps the calculations required for the rotation-tolerant information require an intermediate representation or code that is almost immediately discarded.</p><p>Taken together, this study provides insight into the temporal dynamics of the emergence of rotation-tolerant object information in the brain. The timecourse of rotation-tolerant object representations shows a peak around 200 ms for both the RSA and decoding analyses. This suggests that around this time the object information has been 'untangled <ref type="bibr" target="#b6">' (DiCarlo &amp; Cox, 2007)</ref>, and a rich object representation has been established that is tolerant to rotation and generalises across different exemplars of the same object. These results highlight the importance of looking beyond the simple object decoding methods, which cannot separate the contribution of low-level feature processing from object representations that are tolerant to rotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Models used in the Representational Similarity Analysis. A) The representational dissimilarity matrix (RDM) for the EEG data, averaged across the 100 ms to 400 ms time-window, for the 5 Hz (left) and the 20 Hz (right) condition. Each point in the 480 by 480 matrix represents the dissimilarity between two stimuli. B) The labels for all 480 by 480 matrices plotted here: the rows and columns consist of 12 categories x 5 exemplars x 8 rotations. C) The rotation-tolerant object model. This model predicts that the response to the same</figDesc><graphic coords="15,72.00,71.95,453.55,577.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure4shows the partial correlation between the EEG data and the rotation-tolerant model for each presentation speed condition after accounting for the three control models. For the 5 Hz presentation condition, substantial evidence for a partial correlation with the rotation-tolerant object representation model was present from ~100 ms after stimulus onset. The peak time of this</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Representation Similarity Analysis results. The blue and red lines shows the partial correlation of the rotation-tolerant object model with the EEG data for the 5 Hz condition (blue) and the 20 Hz condition (red) over time, with the three visual control models partialled out. The bootstrapped 95% confidence intervals across participants are displayed as shaded areas around the plot lines. Notched Bayes factors are shown below the plot, with Bayes factors below 1/10 displayed in grey and Bayes factors above 10 shown in the plot colour. Above the plot are projections of the neural dissimilarity of the stimuli (presented at 5 Hz) arranged in a two-dimensional space. The left plot is based on the EEG data between 100 ms and 150 ms after stimulus onset, roughly corresponding to the first peak of the partial correlation of the rotation-tolerant object model with the EEG data for the 5 Hz condition. The right plot is based on the EEG data between 150 ms and 250 ms after stimulus onset, roughly corresponding to the second peak. The distance between two stimuli reflects their pairwise distance, with larger distance between two stimuli reflecting more dissimilar neural responses. The inset on the bottom left corner of each plot shows the same stimuli, with each</figDesc><graphic coords="23,72.00,71.95,451.30,439.95" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGEMENTS</head></div>
<div><head>This work was supported by Australian Research Council (ARC) Discovery</head><p>Projects awarded to TAC (<rs type="grantNumber">DP160101300</rs> and <rs type="grantNumber">DP200101787</rs>), and by <rs type="funder">ARC</rs> <rs type="grantName">Discovery Early Career Researcher Award</rs> awarded to AKR (<rs type="grantNumber">DE200101159</rs>) and to TG (<rs type="grantNumber">DE230100380</rs>). We would like to acknowledge the <rs type="institution" subtype="infrastructure">University of Sydney HPC</rs> service for providing High Performance Computing resources.</p></div>
<div><head n="6.">DATA AND CODE AVAILABILITY</head><p>The experiment code, analysis codes, results, and figures can be found on the Open Science Framework (<ref type="url" target="https://osf.io/r93es">https://osf.io/r93es</ref>). The analysis plan was not officially pre-registered, but a timestamped document with the experiment design and analysis plan can also be found there (<ref type="url" target="https://osf.io/nu7b6">https://osf.io/nu7b6</ref>). The Representational Similarity Analysis was not part of the original analysis plan but was added as an exploratory analysis to be able to control for visual models. An example of the raw and pre-processed data for this study can be found on OpenNeuro: <ref type="url" target="https://doi.org/10.18112/openneuro.ds004252.v1.0.1">https://doi.org/10.18112/openneuro.ds004252.v1.0.1</ref>, the full dataset will be made publicly available upon publication.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NqzRMnq">
					<idno type="grant-number">DP160101300</idno>
				</org>
				<org type="funding" xml:id="_SrsBVnp">
					<idno type="grant-number">DP200101787</idno>
					<orgName type="grant-name">Discovery Early Career Researcher Award</orgName>
				</org>
				<org type="funding" xml:id="_xUdHfNF">
					<idno type="grant-number">DE200101159</idno>
				</org>
				<org type="funding" xml:id="_NUbsDf8">
					<idno type="grant-number">DE230100380</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">University of Sydney HPC</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High temporal resolution decoding of object position and category</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>References Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hogendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mesik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turret</surname></persName>
		</author>
		<idno type="DOI">10.1167/11.10.9</idno>
		<ptr target="https://doi.org/10.1167/11.10.9" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="9" to="9" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">REFERENCES Carlson, T. A., Hogendoorn, H., Kanai, R., Mesik, J., &amp; Turret, J. (2011). High temporal resolution decoding of object position and category. Journal of Vision, 11(10), 9-9. https://doi.org/10.1167/11.10.9</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representational dynamics of object vision: The first 1000 ms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Tovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<idno type="DOI">10.1167/13.10.1</idno>
		<ptr target="https://doi.org/10.1167/13.10.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Carlson, T. A., Tovar, D. A., Alink, A., &amp; Kriegeskorte, N. (2013). Representational dynamics of object vision: The first 1000 ms. Journal of Vision, 13(10), 1. https://doi.org/10.1167/13.10.1</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resolving human object recognition in space and time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn.3635</idno>
		<ptr target="https://doi.org/10.1038/nn.3635" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="462" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cichy, R. M., Pantazis, D., &amp; Oliva, A. (2014). Resolving human object recognition in space and time. Nature Neuroscience, 17(3), 455-462. https://doi.org/10.1038/nn.3635</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distinct neural processes for the perception of familiar versus unfamiliar faces along the visual hierarchy revealed by EEG</title>
		<author>
			<persName><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2018.06.080</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2018.06.080" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Collins, E., Robinson, A. K., &amp; Behrmann, M. (2018). Distinct neural processes for the perception of familiar versus unfamiliar faces along the visual hierarchy revealed by EEG. NeuroImage, 181, 120-131. https://doi.org/10.1016/j.neuroimage.2018.06.080</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Contini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Wardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2017.02.013</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2017.02.013" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="165" to="176" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Contini, E. W., Wardle, S. G., &amp; Carlson, T. A. (2017). Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions. Neuropsychologia, 105, 165-176. https://doi.org/10.1016/j.neuropsychologia.2017.02.013</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makeig</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2003.10.009</idno>
		<ptr target="https://doi.org/10.1016/j.jneumeth.2003.10.009" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="21" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Delorme, A., &amp; Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis. Journal of Neuroscience Methods, 134(1), 9-21. https://doi.org/10.1016/j.jneumeth.2003.10.009</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Untangling invariant object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2007.06.010</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2007.06.010" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="333" to="341" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">DiCarlo, J. J., &amp; Cox, D. D. (2007). Untangling invariant object recognition. Trends in Cognitive Sciences, 11(8), 333-341. https://doi.org/10.1016/j.tics.2007.06.010</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How Does the Brain Solve Visual Object Recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2012.01.010</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2012.01.010" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">DiCarlo, J. J., Zoccolan, D., &amp; Rust, N. C. (2012). How Does the Brain Solve Visual Object Recognition? Neuron, 73(3), 415-434. https://doi.org/10.1016/j.neuron.2012.01.010</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian Versus Orthodox Statistics: Which Side Are You On?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dienes</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691611406920</idno>
		<ptr target="https://doi.org/10.1177/1745691611406920" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="274" to="290" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dienes, Z. (2011). Bayesian Versus Orthodox Statistics: Which Side Are You On? Perspectives on Psychological Science, 6(3), 274-290. https://doi.org/10.1177/1745691611406920</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The representational dynamics of visual objects in rapid serial visual processing streams</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2018.12.046</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2018.12.046" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="668" to="679" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grootswagers, T., Robinson, A. K., &amp; Carlson, T. A. (2019). The representational dynamics of visual objects in rapid serial visual processing streams. NeuroImage, 188, 668-679. https://doi.org/10.1016/j.neuroimage.2018.12.046</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Untangling featural and conceptual object representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shatek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2019.116083</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2019.116083" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page">116083</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grootswagers, T., Robinson, A. K., Shatek, S. M., &amp; Carlson, T. A. (2019). Untangling featural and conceptual object representations. NeuroImage, 202, 116083. https://doi.org/10.1016/j.neuroimage.2019.116083</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The neural dynamics underlying prioritisation of task-relevant information</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shatek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.51628/001c.21174</idno>
		<idno>06.25.172643</idno>
		<ptr target="https://doi.org/10.51628/001c.21174" />
	</analytic>
	<monogr>
		<title level="j">Neurons, Behavior, Data Analysis, and Theory</title>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grootswagers, T., Robinson, A. K., Shatek, S. M., &amp; Carlson, T. A. (2021). The neural dynamics underlying prioritisation of task-relevant information. Neurons, Behavior, Data Analysis, and Theory, 2020.06.25.172643. https://doi.org/10.51628/001c.21174</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mapping the dynamics of visual feature coding: Insights into perception and integration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shatek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1011760</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1011760" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1011760</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grootswagers, T., Robinson, A. K., Shatek, S. M., &amp; Carlson, T. A. (2024). Mapping the dynamics of visual feature coding: Insights into perception and integration. PLOS Computational Biology, 20(1), e1011760. https://doi.org/10.1371/journal.pcbi.1011760</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seeing slow and seeing fast: Two limits on perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Holcombe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2009.02.005</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2009.02.005" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="216" to="221" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Holcombe, A. O. (2009). Seeing slow and seeing fast: Two limits on perception. Trends in Cognitive Sciences, 13(5), 216-221. https://doi.org/10.1016/j.tics.2009.02.005</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape and arrangement of columns in cat&apos;s striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
		<idno type="DOI">10.1113/jphysiol.1963</idno>
		<ptr target="https://doi.org/10.1113/jphysiol.1963" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="568" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hubel, D. H., &amp; Wiesel, T. N. (1963). Shape and arrangement of columns in cat&apos;s striate cortex. The Journal of Physiology, 165(3), 559-568. https://doi.org/10.1113/jphysiol.1963.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast Readout of Object Identity from Macaque Inferior Temporal Cortex</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1117593</idno>
		<ptr target="https://doi.org/10.1126/science.1117593" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">5749</biblScope>
			<biblScope unit="page" from="863" to="866" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hung, C. P., Kreiman, G., Poggio, T., &amp; DiCarlo, J. J. (2005). Fast Readout of Object Identity from Macaque Inferior Temporal Cortex. Science, 310(5749), 863-866. https://doi.org/10.1126/science.1117593</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The dynamics of invariant object recognition in the human visual system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00394.2013</idno>
		<ptr target="https://doi.org/10.1152/jn.00394.2013" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="102" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Isik, L., Meyers, E. M., Leibo, J. Z., &amp; Poggio, T. (2014). The dynamics of invariant object recognition in the human visual system. Journal of Neurophysiology, 111(1), 91-102. https://doi.org/10.1152/jn.00394.2013</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Size and position invariance of neuronal responses in monkey inferotemporal cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.1995.73.1.218</idno>
		<ptr target="https://doi.org/10.1152/jn.1995.73.1.218" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="218" to="226" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ito, M., Tamura, H., Fujita, I., &amp; Tanaka, K. (1995). Size and position invariance of neuronal responses in monkey inferotemporal cortex. Journal of Neurophysiology, 73(1), 218-226. https://doi.org/10.1152/jn.1995.73.1.218</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Theory of Probability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jeffreys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>OUP Oxford</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jeffreys, H. (1998). The Theory of Probability. OUP Oxford.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape-independent object category responses revealed by MEG and fMRI decoding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Azzalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Peelen</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.01074.2015</idno>
		<ptr target="https://doi.org/10.1152/jn.01074.2015" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2246" to="2250" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiser, D., Azzalini, D. C., &amp; Peelen, M. V. (2016). Shape-independent object category responses revealed by MEG and fMRI decoding. Journal of Neurophysiology, 115(4), 2246-2250. https://doi.org/10.1152/jn.01074.2015</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hard-wired feed-forward visual mechanisms of the brain compensate for affine variations in object recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Karimi-Rouzbahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroscience.2017.02.050</idno>
		<ptr target="https://doi.org/10.1016/j.neuroscience.2017.02.050" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="48" to="63" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Karimi-Rouzbahani, H., Bagheri, N., &amp; Ebrahimpour, R. (2017). Hard-wired feed-forward visual mechanisms of the brain compensate for affine variations in object recognition. Neuroscience, 349, 48-63. https://doi.org/10.1016/j.neuroscience.2017.02.050</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayes Factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1995.10476572</idno>
		<ptr target="https://doi.org/10.1080/01621459.1995.10476572" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kass, R. E., &amp; Raftery, A. E. (1995). Bayes Factors. Journal of the American Statistical Association, 90(430), 773-795. https://doi.org/10.1080/01621459.1995.10476572</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Characterizing the dynamics of mental representations: The temporal generalization method</title>
		<author>
			<persName><forename type="first">J.-R</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2014.01.002</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2014.01.002" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">King, J.-R., &amp; Dehaene, S. (2014). Characterizing the dynamics of mental representations: The temporal generalization method. Trends in Cognitive Sciences, 18(4), 203-210. https://doi.org/10.1016/j.tics.2014.01.002</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Human Brain Encodes a Chronicle of Visual Events at Each Instant of Time Through the Multiplexing of Traveling Waves</title>
		<author>
			<persName><forename type="first">J.-R</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2098-20.2021</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2098-20.2021" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="7224" to="7233" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">King, J.-R., &amp; Wyart, V. (2021). The Human Brain Encodes a Chronicle of Visual Events at Each Instant of Time Through the Multiplexing of Traveling Waves. Journal of Neuroscience, 41(34), 7224-7233. https://doi.org/10.1523/JNEUROSCI.2098-20.2021</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representational geometry: Integrating cognition, computation, and the brain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2013.06.007</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2013.06.007" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kriegeskorte, N., &amp; Kievit, R. A. (2013). Representational geometry: Integrating cognition, computation, and the brain. Trends in Cognitive Sciences, 17(8), 401-412. https://doi.org/10.1016/j.tics.2013.06.007</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representational similarity analysis-Connecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.06.004.2008</idno>
		<ptr target="https://doi.org/10.3389/neuro.06.004.2008" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Systems Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kriegeskorte, N., Mur, M., &amp; Bandettini, P. A. (2008). Representational similarity analysis-Connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2. https://doi.org/10.3389/neuro.06.004.2008</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajalingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prescott-Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kubilius, J., Schrimpf, M., Kar, K., Rajalingham, R., Hong, H., Majaj, N., Issa, E., Bashivan, P., Prescott-Roy, J., Schmidt, K., Nayebi, A., Bear, D., Yamins, D. L., &amp; DiCarlo, J. J. (2019). Brain-Like Object Recognition with High- Performing Shallow Recurrent ANNs. Advances in Neural Information Processing Systems, 32. https://proceedings.neurips.cc/paper_files/paper/2019/hash/7813d1590d2 8a7dd372ad54b5d29d033-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What Response Properties Do Individual Neurons Need to Underlie Position and Clutter &quot;Invariant&quot; Object Recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.90745.2008</idno>
		<ptr target="https://doi.org/10.1152/jn.90745.2008" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="360" to="376" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, N., Cox, D. D., Zoccolan, D., &amp; DiCarlo, J. J. (2009). What Response Properties Do Individual Neurons Need to Underlie Position and Clutter &quot;Invariant&quot; Object Recognition? Journal of Neurophysiology, 102(1), 360- 376. https://doi.org/10.1152/jn.90745.2008</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discrete and continuous mechanisms of temporal selection in rapid visual streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-017-02079-x</idno>
		<ptr target="https://doi.org/10.1038/s41467-017-02079-x" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1955">2017. 1955</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marti, S., &amp; Dehaene, S. (2017). Discrete and continuous mechanisms of temporal selection in rapid visual streams. Nature Communications, 8(1), 1955. https://doi.org/10.1038/s41467-017-02079-x</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal Limitations in Object Processing Across the Human Ventral Visual Pathway</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Mckeeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tong</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00568.2006</idno>
		<ptr target="https://doi.org/10.1152/jn.00568.2006" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="382" to="393" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McKeeff, T. J., Remus, D. A., &amp; Tong, F. (2007). Temporal Limitations in Object Processing Across the Human Ventral Visual Pathway. Journal of Neurophysiology, 98(1), 382-393. https://doi.org/10.1152/jn.00568.2006</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.90248.2008</idno>
		<ptr target="https://doi.org/10.1152/jn.90248.2008" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1407" to="1419" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Meyers, E. M., Freedman, D. J., Kreiman, G., Miller, E. K., &amp; Poggio, T. (2008). Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex. Journal of Neurophysiology, 100(3), 1407-1419. https://doi.org/10.1152/jn.90248.2008</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The effect of hunger and state preferences on the neural processing of food images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moerel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chenh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1101/2025.09.09.674354</idno>
		<idno>09.09.674354</idno>
		<ptr target="https://doi.org/10.1101/2025.09.09.674354" />
		<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moerel, D., Chenh, C., Bowman, S. A., &amp; Carlson, T. A. (2025). The effect of hunger and state preferences on the neural processing of food images (p. 2025.09.09.674354). bioRxiv. https://doi.org/10.1101/2025.09.09.674354</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Time-Course of Food Representation in the Human Brain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moerel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Psihoyos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1101-23.2024</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.1101-23.2024" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moerel, D., Psihoyos, J., &amp; Carlson, T. A. (2024). The Time-Course of Food Representation in the Human Brain. Journal of Neuroscience. https://doi.org/10.1523/JNEUROSCI.1101-23.2024</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mohsenzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pantazis</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.36329</idno>
		<ptr target="https://doi.org/10.7554/eLife.36329" />
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">36329</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mohsenzadeh, Y., Qin, S., Cichy, R. M., &amp; Pantazis, D. (2018). Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway. eLife, 7, e36329. https://doi.org/10.7554/eLife.36329</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The philosophy of Bayes factors and the quantification of statistical evidence</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Romeijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2015.11.001</idno>
		<ptr target="https://doi.org/10.1016/j.jmp.2015.11.001" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="6" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Morey, R. D., Romeijn, J.-W., &amp; Rouder, J. N. (2016). The philosophy of Bayes factors and the quantification of statistical evidence. Journal of Mathematical Psychology, 72, 6-18. https://doi.org/10.1016/j.jmp.2015.11.001</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayes factor approaches for testing interval null hypotheses</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0024377</idno>
		<ptr target="https://doi.org/10.1037/a0024377" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="406" to="419" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Morey, R. D., &amp; Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. Psychological Methods, 16(4), 406-419. https://doi.org/10.1037/a0024377</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=BayesFactor" />
		<title level="m">BayesFactor: Computation of Bayes Factors for Common Designs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Morey, R. D., &amp; Rouder, J. N. (2018). BayesFactor: Computation of Bayes Factors for Common Designs. https://CRAN.R-project.org/package=BayesFactor</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The five percent electrode system for high-resolution EEG and ERP measurements</title>
		<author>
			<persName><forename type="first">R</forename><surname>Oostenveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Praamstra</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1388-2457(00)00527-7</idno>
		<ptr target="https://doi.org/10.1016/S1388-2457(00)00527-7" />
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="719" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oostenveld, R., &amp; Praamstra, P. (2001). The five percent electrode system for high-resolution EEG and ERP measurements. Clinical Neurophysiology, 112(4), 713-719. https://doi.org/10.1016/S1388-2457(00)00527-7</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CoSMoMVPA: Multi-Modal Multivariate Pattern Analysis of Neuroimaging Data in Matlab/GNU Octave</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Oosterhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Haxby</surname></persName>
		</author>
		<idno type="DOI">10.3389/fninf.2016.00027</idno>
		<ptr target="https://doi.org/10.3389/fninf.2016.00027" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oosterhof, N. N., Connolly, A. C., &amp; Haxby, J. V. (2016). CoSMoMVPA: Multi- Modal Multivariate Pattern Analysis of Neuroimaging Data in Matlab/GNU Octave. Frontiers in Neuroinformatics, 10. https://doi.org/10.3389/fninf.2016.00027</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamics of 3D view invariance in monkey inferotemporal cortex</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Ratan Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Arun</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00810.2014</idno>
		<ptr target="https://doi.org/10.1152/jn.00810.2014" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2180" to="2194" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ratan Murty, N. A., &amp; Arun, S. P. (2015). Dynamics of 3D view invariance in monkey inferotemporal cortex. Journal of Neurophysiology, 113(7), 2180- 2194. https://doi.org/10.1152/jn.00810.2014</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Balanced Comparison of Object Invariances in Monkey IT Neurons</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Ratan Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Arun</surname></persName>
		</author>
		<idno type="DOI">10.1523/ENEURO.0333-16.2017</idno>
		<idno>ENEURO.0333-16</idno>
		<ptr target="https://doi.org/10.1523/ENEURO.0333-16.2017" />
	</analytic>
	<monogr>
		<title level="j">Eneuro</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ratan Murty, N. A., &amp; Arun, S. P. (2017). A Balanced Comparison of Object Invariances in Monkey IT Neurons. Eneuro, 4(2), ENEURO.0333-16.2017. https://doi.org/10.1523/ENEURO.0333-16.2017</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The influence of image masking on object representations during rapid serial visual presentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2019.04.050</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2019.04.050" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="224" to="231" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Robinson, A. K., Grootswagers, T., &amp; Carlson, T. A. (2019). The influence of image masking on object representations during rapid serial visual presentation. NeuroImage, 197, 224-231. https://doi.org/10.1016/j.neuroimage.2019.04.050</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Overlapping neural representations for the position of visible and imagined objects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shatek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gerboni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09932</idno>
		<ptr target="http://arxiv.org/abs/2010.09932" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>q-Bio</note>
	<note type="raw_reference">Robinson, A. K., Grootswagers, T., Shatek, S. M., Gerboni, J., Holcombe, A., &amp; Carlson, T. A. (2020). Overlapping neural representations for the position of visible and imagined objects. arXiv:2010.09932 [q-Bio]. http://arxiv.org/abs/2010.09932</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian t tests for accepting and rejecting the null hypothesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Speckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iverson</surname></persName>
		</author>
		<idno type="DOI">10.3758/PBR.16.2.225</idno>
		<ptr target="https://doi.org/10.3758/PBR.16.2.225" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="237" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., &amp; Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. Psychonomic Bulletin &amp; Review, 16(2), 225-237. https://doi.org/10.3758/PBR.16.2.225</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selectivity and Tolerance (&quot;Invariance&quot;) Both Increase as Visual Information Propagates from Cortical Area V4 to IT</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.0179-10.2010</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.0179-10.2010" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="12978" to="12995" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rust, N. C., &amp; DiCarlo, J. J. (2010). Selectivity and Tolerance (&quot;Invariance&quot;) Both Increase as Visual Information Propagates from Cortical Area V4 to IT. Journal of Neuroscience, 30(39), 12978-12995. https://doi.org/10.1523/JNEUROSCI.0179-10.2010</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An empirically-driven guide on using Bayes Factors for M/EEG decoding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moerel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.06.23.449663</idno>
		<ptr target="https://doi.org/10.1101/2021.06.23.449663" />
	</analytic>
	<monogr>
		<title level="j">Aperture Neuro</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Teichmann, L., Moerel, D., Baker, C. I., &amp; Grootswagers, T. (2022). An empirically-driven guide on using Bayes Factors for M/EEG decoding. Aperture Neuro, 1(8), 1-10. https://doi.org/10.1101/2021.06.23.449663</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Seeking Categories in the Brain</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fabre-Thorpe</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1058249</idno>
		<ptr target="https://doi.org/10.1126/science.1058249" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="issue">5502</biblScope>
			<biblScope unit="page" from="260" to="263" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thorpe, S. J., &amp; Fabre-Thorpe, M. (2001). Seeking Categories in the Brain. Science, 291(5502), 260-263. https://doi.org/10.1126/science.1058249</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A practical solution to the pervasive problems ofp values</title>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03194105</idno>
		<ptr target="https://doi.org/10.3758/BF03194105" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="804" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wagenmakers, E.-J. (2007). A practical solution to the pervasive problems ofp values. Psychonomic Bulletin &amp; Review, 14(5), 779-804. https://doi.org/10.3758/BF03194105</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Statistical Evidence in Experimental Psychology: An Empirical Comparison Using 855 t Tests</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691611406923</idno>
		<ptr target="https://doi.org/10.1177/1745691611406923" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wetzels, R., Matzke, D., Lee, M. D., Rouder, J. N., Iverson, G. J., &amp; Wagenmakers, E.-J. (2011). Statistical Evidence in Experimental Psychology: An Empirical Comparison Using 855 t Tests. Perspectives on Psychological Science, 6(3), 291-298. https://doi.org/10.1177/1745691611406923</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Trade-Off between Object Selectivity and Tolerance in Monkey Inferotemporal Cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kouh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1897-07.2007</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.1897-07.2007" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="12292" to="12307" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zoccolan, D., Kouh, M., Poggio, T., &amp; DiCarlo, J. J. (2007). Trade-Off between Object Selectivity and Tolerance in Monkey Inferotemporal Cortex. Journal of Neuroscience, 27(45), 12292-12307. https://doi.org/10.1523/JNEUROSCI.1897-07.2007</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
