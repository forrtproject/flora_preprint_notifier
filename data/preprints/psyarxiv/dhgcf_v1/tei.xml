<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Level AI Trustworthiness Labels Scale Potential Users&apos; Perceptions and Evaluations of AI Products</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Christina</forename><forename type="middle">U</forename><surname>Pfeuffer</surname></persName>
							<email>christina.pfeuffer@ku.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology -Human-Technology Interaction</orgName>
								<orgName type="institution">Catholic University of Eichstätt-Ingolstadt</orgName>
								<address>
									<settlement>Eichstätt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Level AI Trustworthiness Labels Scale Potential Users&apos; Perceptions and Evaluations of AI Products</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0B3C78A4A2B96AC831641240D6D3E45D</idno>
					<idno type="DOI">10.17605/OSF.IO/HD3NA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-18T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=true, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial intelligence</term>
					<term>data security and data privacy</term>
					<term>label</term>
					<term>regulation</term>
					<term>trust</term>
					<term>AI anxiety</term>
					<term>technology attitude</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Potential) users of artificial intelligence (AI) products (e.g., smart fridges, voice assistants) are most often not fully informed about the data security and data privacy (DSDP) of these AI products as such information is not available in an easily accessible format. Here, I argue that it is essential to provide (potential) users with such information presented in an easily accessible and quick-to-process format as this could help instill appropriate levels of (dis-)trust and thus support (trustworthy) AI acceptance and adoption. Testing the effect of AI trustworthiness labels, I presented participants with hypothetical AI products that were versus were not paired with a graphical or text-based label indicating a low to high DSDP level. Trust, AI anxiety, as well as the monetary value attributed to AI products scaled with an AI trustworthiness label's DSDP level. Importantly, participants' ratings of unlabeled AI products corresponded to their perceptions of AI products labeled with an intermediate DSDP level. This apparent bias towards intermediate DSDP judgements in the absence of information on AI products underscores the relevance of explicitly communicating AI trustworthiness to (potential) users. Interestingly, differences in trust for AI products with a high as compared to intermediate DSDP level further increased with more positive attitudes towards AI technology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Artificial intelligence (AI) and corresponding AI products are on the verge of becoming ubiquitous to our everyday lives. This includes both conversational AI user interfaces (e.g., chatbots, AI voice assistants) as well as other AI (e.g., recommender systems, computer vision). AI holds the potential to benefit both individuals, organizations, and society at large by, for instance, optimizing products and services, enhancing productivity and efficiency, or lowering costs <ref type="bibr" target="#b0">[1]</ref>. However, this potential can only be realized when human-AI interactions are appropriately shaped <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Concerns regarding AI trustworthiness, in particular, data security and data privacy (DSDP) concerns <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr">[4]</ref>, jeopardize a further widespread acceptance and broader adoption of AI products (see e.g., <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b6">[8]</ref>, for prominent theories of technology acceptance and adoption). AI products often gain access to users' (sensitive) data, raising concerns regarding data security and data privacy (see e.g., <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b9">[11]</ref> for corresponding considerations regarding algorithm auditing). Recent theorizing emphasizes especially the role of trust (e.g., Intelligent Systems Technology Acceptance Model, ISTAM, <ref type="bibr" target="#b10">[12]</ref>, linking trust to transparency) as an essential precursors of technology acceptance and adoption. As such, establishing and maintaining the public's trust in AI, derived from a trustworthiness assessment of AI products <ref type="bibr" target="#b11">[13]</ref>, appears paramount to its further acceptance and adoption.</p><p>Users, however, are hardly able to evaluate the trustworthiness of AI accurately <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[13]</ref>, as corresponding information is commonly not easily accessible. To access information on the trustworthiness of AI products, for instance, to gain data security and data privacy information, (potential) users need to read long legal statements in an AI product's fine print. (Potential) users therefore currently (dis-)trust AI products mainly based on heuristics <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>. Moreover, strong, often unjustified AI endorsement <ref type="bibr" target="#b0">[1]</ref>, is coupled with low understanding of AI in the general public <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[17]</ref>. Discrepancies between objective trustworthiness (e.g., adherence to criteria like those proposed by the European Commission and in the EU AI Act [4], <ref type="bibr" target="#b16">[18]</ref>) and how trustworthy individuals perceive AI to be call for corresponding affirmative action. At present, (potential) users generally presume that trustworthy AI principles and standards (e.g., data security and data privacy, accuracy, risk and impact mitigation, and AI literacy support) are in place (e.g., for AI used in human resources, healthcare, or security contexts <ref type="bibr" target="#b0">[1]</ref>). This notion is not necessarily justified for each AI product. It appears that the public's overall expectations regarding AI safeguards are currently not keeping up with objective protection measures and the speed of AI evolution.</p><p>Importantly, both misplaced distrust <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b17">[19]</ref> and misplaced trust (due to expectancy violations, <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>) prevent the further acceptance and adoption of new (and trustworthy) AI technologies and obstruct corresponding benefits of AI usage. I propose that informative, multi-level labels (e.g., similar to the Nutri-Score indicating the nutritional value of food, e.g., <ref type="bibr" target="#b20">[22]</ref>; for prior studies on technology/AI certification labels see e.g., <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref>) constitute the best-suited means of achieving accurate assessments of AI trustworthiness with very limited (potential) user effort across varying levels of AI literacy. At present, empirically-validated product labels like the Nutri-Score do not yet exit for AI products (existing labels typically do not indicate the degree and/or criteria of trustworthiness, but see <ref type="bibr" target="#b25">[27]</ref> for a notable exception).</p><p>In this experiment, I communicated the data security and data privacy (DSDP; i.e., a specific AI trustworthiness criterion) level of hypothetical AI products using three-level labels (low vs. intermediate vs. high) of two label types (graphical vs. text-based label). My hypotheses were clearcut: I expected trust and attributed monetary value to increase and AI anxiety to decrease for AI products with higher DSDP levels communicated by a corresponding DSDP/trust-worthiness label. Furthermore, I expected to observe differences between the two label types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPERIMENTAL METHODS</head><p>An extended preprint (<ref type="url" target="https://osf.io/preprints/psyarxiv/">https://osf.io/preprints/psyarxiv/</ref> q25nr_v1), the preregistration of this study (<ref type="url" target="https://osf.io/">https://osf.io/</ref> vbxqy) and all study materials (<ref type="url" target="https://doi.org/">https://doi.org/</ref> 10.17605/OSF.IO/HD3NA) are available online.</p><p>A priori, I planned Bayesian linear mixed model analyses and thus used a Bayesian approach to sample size estimation <ref type="bibr" target="#b26">[28]</ref>. The Bayesian stopping criterion for ending data collection was a Bayes factor BF 10 larger than 3 (or smaller than 1/3) for the effects of label type and DSDP level on the dependent variables trust and AI anxiety. 102 participants (35 male, 64 female, 3 diverse; age: M = 26.7 years, SD = 8.9; attitude towards technology <ref type="bibr" target="#b17">[19]</ref>: M = 14.4, SD = 2.86, <ref type="bibr">[4;20]</ref>; recruited via open online advertisements) took part after providing informed consent. The study was conducted in line with the regulations of the local ethics committee.</p><p>First, participants were informed about the features and functions of two hypothetical AI product types (smart fridge, voice assistant). Subsequently, they rated their trust (4-item trust questionnaire by Choung et al. <ref type="bibr" target="#b17">[19]</ref>; 1 = strongly disagree to 5 = strongly agree) and (state) anxiety (4-item sociotechnical blindness subscale of the AI anxiety scale by Wang &amp; Wang <ref type="bibr" target="#b27">[29]</ref>; 1 = strongly disagree to 5 = strongly agree) regarding each AI product (represented by a corresponding name text and icon). First (baseline; 2 trials: 1 per AI product; order randomized), participants were presented only with the AI product (name text and icon) without further information. Then, second (after an introduction of the DSDP labels; DSDP criteria adapted from [30]; compare [4], <ref type="bibr" target="#b16">[18]</ref>), participants were again presented with the AI products now paired with a DSDP label (label type: graphical vs. text-based; within) indicating a low, intermediate, or high level of trustworthiness (DSDP level; within; 12 trials: 2 AI products x 2 label types x 3 DSDP levels; order randomized) and indicated their trust and AI anxiety (see Fig. <ref type="figure" target="#fig_0">1</ref> for experimental design, procedure, and label appearance). In a final phase, the monetary value participants attributed to the respective labeled AI products was assessed by showing two different levels of the same label type per AI product and trial (level comparison: low-intermediate vs. intermediate-high vs. low-high; within; 12 trials: 2 AI products x 2 label types x 3 DSDP level comparisons; order randomized). Here, participants' task was to indicate how much more (percent acceptable price increase) they were willing to pay for the AI product with the respective higher DSDP level. Finally, participants rated their attitude towards (AI) technology (4-item attitude toward (AI) technologies scale by Choung et al. <ref type="bibr" target="#b17">[19]</ref>; 1 = strongly disagree to 5 = strongly agree) and were debriefed.</p><p>The two DSDP label types compared were a graphical (horizontal traffic light: green, yellow, red) and a text-based label (vertical tick boxes with bullet point category descriptions: secure, data protected, fair user interaction; see Fig. <ref type="figure" target="#fig_0">1</ref> for label appearances). Label types differed in appearance, but conveyed the same information. That is, the degree to which the respective hypothetical AI product fulfilled the three main DSDP criteria: Secure, data protection, and fair user interaction (label content adapted from the Swiss Digital Initiative [30]). Participants were instructed about the meaning of the labels and DSDP criteria, that label types conveyed the same information in different visual ways, and the criteria for the respective DSDP levels (i.e., how many criteria were fulfilled to a satisfactory degree) before they began to evaluate the hypothetical AI products. They were explicitly told that both labels represented the same information. Importantly, however, the graphical label summarized the three criteria in one traffic light-like label, whereas the text-based label listed each criterion separately. The selected criteria themselves were used as exemplary material. The thresholds per criterion and for the three DSDP levels in this study were intentionally kept very simplistic and hypothetical (criterion: degree/percentage to which a corresponding ideal was fulfilled; DSDP level: number of criteria reaching this threshold). The focus of this study was not a dissemination of the best threshold criteria, but an assessment of (potential) users' perception of labels conveying trustworthiness criteria like DSDP information to determine whether pursuing further research on such labels and threshold criteria appears promising. Future studies will have to revisit the set of criteria as well as criterion and level thresholds for such labels if DSDP labels or broader AI trustworthiness labels (e.g., building on the criteria detailed in the EU AI Act [4], <ref type="bibr" target="#b16">[18]</ref>) are to be implemented. The purpose of the present study was only to confirm whether such DSDP labels (irrespective of their exact level criteria) successfully instilled appropriate levels of (dis-)trust in participants. That is, this study assessed whether participants' perceived trust, AI anxiety, and the monetary value they attribute to DSDP-labeled AI products indeed scaled with the labels' DSDP level as well as how graphical and text-based labels generally differed in this regard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>A Bayesian linear mixed model analysis approach (criterion: BF 10 &gt; 3 or &lt; 1/3) was used. To account for differences between a person's ratings of the respective AI product type at baseline (i.e., without a DSDP label) and when presented with a DSDP label, I analyzed corresponding difference scores (i.e., dependent variable in respective DSDP label condition -dependent variable at baseline).  Trust. Trust ratings increased with increasing DSDP levels, BF 10 = 1.36 x 10 44 ±1.16%, showing a strong impact of the DSDP labels on participants' trust both between low and intermediate, BF 10 = 1.23 x 10 18 ±5.25%, and between intermediate and high DSDP levels, BF 10 = 1.72 x 10 19 ±1.46% (see Fig. <ref type="figure" target="#fig_2">3A</ref>). Moreover, trust ratings were higher for text-based as compared to graphical labels, BF 10 = 7.18 x 10 7 ±0.88%. Label type and DSDP level interacted, BF 10 = 4.03 ±1.61%.</p><p>Trust -Attitudes towards Technology. A post-hoc, exploratory analysis including attitudes towards technology showed evidence against an effect of attitudes towards technology, BF 10 = 0.16 ±1.58%, as well as against an interaction between label type and attitudes towards technology, BF 10 = 0.20 ±2.88%, and against a three-way interaction, BF 10 = 0.13 ±2.14%. There was, however, evidence in favour of an interaction of DSDP level and attitude towards technology, BF 10 = 6.73 ±3.57%. Specifically, there was evidence against differences in the slopes of individual attitudes towards technology scores between low and intermediate DSDP levels, BF 10 = 0.27 ±2.06%, but evidence in favour of differences in the slopes of individual attitudes towards technology scores between intermediate and high DSDP levels, BF 10 = 5.47 ±4.48%. Differences in trust scores between intermediate and high DSDP levels increased with more positive attitudes towards technology (see Fig. <ref type="figure" target="#fig_3">4</ref>). Whereas, the evidence in favour of the effects of label type, BF 10 = 4.6 x 10 5 ±1.51%, and DSDP level, BF 10 = 1.12 x 10 147 ±1.88%, on trust ratings remained substantial, there was inconclusive evidence against an interaction of label type and DSDP level, BF 10 = 0.70 ±2.07%, when additionally accounting for attitudes towards technology.</p><p>AI Anxiety. AI anxiety ratings decreased with increasing DSDP levels, BF 10 = 8.4 x 10 25 ±1.76%, showing a strong impact of the DSDP labels on participants' AI anxiety both between low and intermediate, BF 10 = 7.5 x 10 7 ±2.29%, and between intermediate and high DSDP levels, BF 10 = 4.9 x 10 10 ±1.45% (see Fig. <ref type="figure" target="#fig_2">3B</ref>). AI anxiety ratings were lower for text-based as compared to graphical labels, BF 10 = 9.5 ±1.75%. There was evidence against an interaction of label type and DSDP level, BF 10 = 0.1 ±2.23%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI anxiety -Attitudes towards Technology.</head><p>A post-hoc, exploratory analysis including attitudes towards technology showed evidence against an effect of attitudes towards technology, BF 10 = 0.20 ±3.47%, as well as against an interaction between label type and attitudes towards technology, BF 10 = 0.18 ±1.67%, against an interaction between DSDP level and attitudes towards technology, BF 10 = 0.05 ±9.07%, and against the three-way interaction, BF 10 = 0.04 ±1.67%. The evidence in favour of the effects of label type, BF 10 = 9.5 ±2.93%, and DSDP level, BF 10 = 8.3 x 10 25 ±3.05%, on AI anxiety ratings remained substantial, when additionally accounting for attitudes towards technology. Similarly, there was still evidence against an interaction of label type and DSDP level, BF 10 = 0.09 ±2.30%. Attributed Value. Attributed monetary value (acceptable percent price increase for a higher DSDP level) increased across DSDP level comparisons, BF 10 = 5.8 x 10 29 ±1.26% (see Fig. <ref type="figure" target="#fig_2">3C</ref>). The increase in attributed value was substantially larger between the intermediate and high DSDP level than between the low and intermediate DSDP level, BF 10 = 1.2 x 10 7 ±8.78%. Higher monetary value was attributed to AI products labelled with graphical as compared to text-based labels, BF 10 = 8.1 ±1.15%. There was inconclusive evidence against an interaction of label type and DSDP level comparison, BF 10 = 0.44 ±1.81%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attributed Value -Attitudes towards Technology. A posthoc, exploratory analysis including attitudes towards technology showed evidence against an effect of attitudes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B C</head><p>towards technology, BF 10 = 0.35 ±3.31%, well as against an interaction between label type and attitudes towards technology, BF 10 = 0.19 ±9.27%, against an interaction between DSDP level comparison and attitudes towards technology, BF 10 = 0.06 ±9.04%, and against the three-way interaction, BF 10 = 0.05 ±14.96%. The evidence in favour of the effects of label type, BF 10 = 9.9 ±3.09%, and DSDP level comparison, BF 10 = 8.0 x 10 111 ±2.66%, remained substantial, when additionally accounting for individual attitudes towards technology. The evidence against an interaction of label type and DSDP level comparison was conclusive when additionally accounting for individual attitudes towards technology, BF 10 = 0.07 ±7.92%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>In this study, I either paired AI products (smart fridge, voice assistant) with an AI trustworthiness label indicating a low to high DSDP level or presented them without such a label (baseline). As expected, DSDP labels effectively communicated AI trustworthiness and correspondingly scaled participants' perceptions and evaluations of the AI products. That is, participants' trust increased and AI anxiety decreased from low to high DSDP levels. Moreover, participants attributed a higher monetary value to AI products with a higher DSDP level as indicated by the corresponding AI trustworthiness label.</p><p>Importantly, for the conducted Bayesian analyses, difference scores were used which subtracted a participant's baseline rating of the respective AI product from its rating in the DSDP level and label type conditions. Due to this baseline adjustment of trust and AI anxiety ratings, values of 0 equated a participant's perception of the respective AI product at baseline in the absence of DSDP information. The analyses revealed that baseline assessments of AI products corresponded to participants' assessment of AI products labeled with an intermediate DSDP level. This means that participants unjustifiedly attributed an intermediate DSDP level to AI products in the absence of DSDP information, suggesting a bias. The present findings thus underscore the importance of introducing corresponding DSDP labels for AI products to prevent both unjustified trust and unjustified distrust in (potential) users.</p><p>Furthermore, text-based DSDP labels indicating AI trustworthiness were associated with higher trust and lower AI anxiety than graphical labels. Conversely, however, participants attributed higher monetary value to AI products labeled with a graphical rather than text-based label. This suggests that text-based labels might be better suited to increase trust <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b28">[31]</ref> in (potential) users and thereby the acceptance and adoption of AI. However, graphical labels might appear more appealing to companies selling AI products as products with graphical labels were attributed a higher monetary value. Furthermore, graphical labels can be processed even faster and more easily by (potential) users than text-based labels. Therefore, one might also prefer graphical labels to potentially increase AI companies' acceptance of regulatory AI trustworthiness labels and further optimize user effort.</p><p>Interestingly, participants' attitudes towards AI technology did not yield a general influence on their trust, AI anxiety, and the value attributed to the AI products. These findings for DSDP labeled AI products appear to be in stark contrast to prior findings regarding perceptions and evaluations of AI in the literature. Prior studies found evidence for positive correlations between attitudes towards technology and trust in AI-supported technologies <ref type="bibr" target="#b29">[32]</ref>, <ref type="bibr" target="#b30">[33]</ref>, <ref type="bibr" target="#b31">[34]</ref> as well as for negative correlations between attitudes towards technology and AI anxiety <ref type="bibr" target="#b30">[33]</ref>. Additionally, there was conclusive evidence against an impact of a person's attitude towards technology on the influence of DSDP labels on AI anxiety and attributed monetary value. Trust in the corresponding AI products, however, differed more strongly between the three DSDP levels for persons with more positive general attitudes towards technology and AI. Participants with more positive attitudes towards technology and AI appeared to increase more strongly in their trust for AI products labeled with a high DSDP level in comparison to AI products labeled with an intermediate DSDP level. That is, the DSDP information contained in the labels had a stronger positive impact on the trust of participants with more positive attitudes towards AI. Further systematic comparison studies will be required to fully address what underlies the observed discrepancies between findings regarding attitude towards technology and AI observed here as compared to reported in prior studies (see also e.g., <ref type="bibr" target="#b32">[35]</ref> for a prior study showing that information on disruptive technologies, there videos on cryptocurrency, can both increase and decrease trust). Regarding trust, however, the present findings already indicate a potential explanation that merits further investigation. That is, the relationship between attitudes towards technology and trust appears to crucially depend on the information participants have received on an AI product's DSDP level. Put differently, persons with more positive attitudes towards technology and AI appear to be more susceptible to positive DSDP information on AI products.</p><p>It is important to note that the best-suited AI trustworthiness criteria and the best-suited level thresholds for AI trustworthiness labels were never the focus of this study. The simplistic, exemplary trustworthiness criteria (here 3 DSDP criteria) and simplistic threshold descriptions used in this study can be considered as placeholders for future, better-suited criteria derived from corresponding research. The aim of this psychological study was to determine whether multi-level AI trustworthiness labels (here focused on DSDP information as an example) are able to instill suitable degrees of (dis-)trust in (potential) users. The present findings clearly confirm that multi-level AI trustworthiness labels can fulfill this purpose. Separate future studies are, however, required to determine the exact, appropriate criteria for such an AI trustworthiness labels as well as their thresholds. Future research will, for instance, need to incorporate additional trustworthiness criteria (e.g., <ref type="bibr" target="#b16">[18]</ref>), select more informed thresholds for AI trustworthiness levels, and account for label effects at different AI literacy levels (e.g., <ref type="bibr" target="#b33">[36]</ref>) as well as further investigate the impact of individual attitudes towards technology and other individual characteristics of (potential) users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Study design and time course.</figDesc><graphic coords="2,45.00,508.00,243.00,117.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of trust and AI anxiety ratings at baseline.</figDesc><graphic coords="2,388.51,651.24,136.19,108.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Effects of data security and data privacy (DSDP) level/level comparison and label type on A) trust, B) AI anxiety, and C) attributed monetary value. Trust and AI anxiety scores are displayed relative to a participant's respective baseline rating of the corresponding AI product (0 = rating equivalent to baseline). Violins around the respective mean depict the corresponding rating distribution per condition.Baseline. Trust at baseline was 9.8 (SD = 2.7; [0;20])/10.8 (SD = 3.0) for the AI voice assistant/smart fridge and AI anxiety at baseline was 13.1 (SD = 3.7; [0;20])/12.5 (SD = 3.5) for the AI voice assistant/smart fridge (see Fig.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Influence of individual attitudes towards (AI) technology on trust ratings by data security and data privacy (DSDP) level.</figDesc><graphic coords="3,306.50,438.80,158.50,144.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>I thank <rs type="person">Anja Ruff</rs> for her help with material preparation as well as setting up and advertising the online study.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Trust in Artificial Intelligence: A global study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lockey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Akbari</surname></persName>
		</author>
		<idno type="DOI">10.14264/00d3c94</idno>
		<imprint>
			<date type="published" when="2023-02">Feb. 2023</date>
			<publisher>KPMG Australia</publisher>
			<pubPlace>Brisbane, Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The University of Queensland</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">N. Gillespie, S. Lockey, C. Curtis, J. Pool, and Ali Akbari, &quot;Trust in Artificial Intelligence: A global study,&quot; The University of Queensland; KPMG Australia, Brisbane, Australia, Feb. 2023. doi: 10.14264/00d3c94.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guidelines for Interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300233</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>Glasgow Scotland Uk</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Amershi et al., &quot;Guidelines for Interaction,&quot; in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, Glasgow Scotland Uk: ACM, May 2019, pp. 1- 13. doi: 10.1145/3290605.3300233.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Evaluating Human-AI Collaboration: A Review and Methodological Framework</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fragiadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kousiouris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolaidou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.19098</idno>
		<idno type="arXiv">arXiv:2407.19098</idno>
		<imprint>
			<date type="published" when="2024-07-09">July 09, 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Fragiadakis, C. Diou, G. Kousiouris, and M. Nikolaidou, &quot;Evaluating Human-AI Collaboration: A Review and Methodological Framework,&quot; July 09, 2024, arXiv: arXiv:2407.19098. doi: 10.48550/arXiv.2407.19098.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.2307/249008</idno>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">319</biblScope>
			<date type="published" when="1989-09">Sept. 1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. D. Davis, &quot;Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology,&quot; MIS Quarterly, vol. 13, no. 3, p. 319, Sept. 1989, doi: 10.2307/249008.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Technology acceptance model: a literature review from 1986 to 2013</title>
		<author>
			<persName><forename type="first">N</forename><surname>Marangunić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Granić</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10209-014-0348-1</idno>
	</analytic>
	<monogr>
		<title level="j">Univ Access Inf Soc</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Marangunić and A. Granić, &quot;Technology acceptance model: a literature review from 1986 to 2013,&quot; Univ Access Inf Soc, vol. 14, no. 1, pp. 81-95, Mar. 2015, doi: 10.1007/s10209-014-0348-1.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">User Acceptance of Information Technology: Toward a Unified View</title>
		<author>
			<persName><forename type="first">Morris</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename></persName>
		</author>
		<idno type="DOI">10.2307/30036540</idno>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">425</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Venkatesh, Morris, Davis, and Davis, &quot;User Acceptance of Information Technology: Toward a Unified View,&quot; MIS Quarterly, vol. 27, no. 3, p. 425, 2003, doi: 10.2307/30036540.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Theoretical Extension of the Technology Acceptance Model: Four Longitudinal Field Studies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.46.2.186.11926</idno>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="204" />
			<date type="published" when="2000-02">Feb. 2000</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Venkatesh and F. D. Davis, &quot;A Theoretical Extension of the Technology Acceptance Model: Four Longitudinal Field Studies,&quot; Management Science, vol. 46, no. 2, pp. 186-204, Feb. 2000, doi: 10.1287/mnsc.46.2.186.11926.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Addressing the regulatory gap: moving towards an EU AI audit ecosystem beyond the AI Act by including civil society</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R L</forename><surname>De Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Streitbörger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berendt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s43681-024-00595-3</idno>
	</analytic>
	<monogr>
		<title level="j">AI Ethics</title>
		<imprint>
			<date type="published" when="2024-11">Nov. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Hartmann, J. R. L. De Pereira, C. Streitbörger, and B. Berendt, &quot;Addressing the regulatory gap: moving towards an EU AI audit ecosystem beyond the AI Act by including civil society,&quot; AI Ethics, Nov. 2024, doi: 10.1007/s43681-024-00595-3.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards algorithm auditing: managing legal, ethical and technological risks of AI, ML and associated algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koshiyama</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.230859</idno>
	</analytic>
	<monogr>
		<title level="j">R. Soc. Open Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">230859</biblScope>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Koshiyama et al., &quot;Towards algorithm auditing: managing legal, ethical and technological risks of AI, ML and associated algorithms,&quot; R. Soc. Open Sci., vol. 11, no. 5, p. 230859, May 2024, doi: 10.1098/rsos.230859.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ethics-based AI auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minkkinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mäntymäki</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.im.2024.103969</idno>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Management</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">103969</biblScope>
			<date type="published" when="2024-07">July 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Laine, M. Minkkinen, and M. Mäntymäki, &quot;Ethics-based AI auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders,&quot; Information &amp; Management, vol. 61, no. 5, p. 103969, July 2024, doi: 10.1016/j.im.2024.103969.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrating Transparency, Trust, and Acceptance: The Intelligent Systems Technology Acceptance Model (ISTAM)</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Vorm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J Y</forename><surname>Combs</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2022.2070107</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">18-20</biblScope>
			<biblScope unit="page" from="1828" to="1845" />
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. S. Vorm and D. J. Y. Combs, &quot;Integrating Transparency, Trust, and Acceptance: The Intelligent Systems Technology Acceptance Model (ISTAM),&quot; International Journal of Human-Computer Interaction, vol. 38, no. 18-20, pp. 1828-1845, Dec. 2022, doi: 10.1080/10447318.2022.2070107.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How do we assess the trustworthiness of AI? Introducing the trustworthiness assessment model (TrAM)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schlicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sterz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2025.108671</idno>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page">108671</biblScope>
			<date type="published" when="2025-09">Sept. 2025</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Schlicker, K. Baum, A. Uhde, S. Sterz, M. C. Hirsch, and M. Langer, &quot;How do we assess the trustworthiness of AI? Introducing the trustworthiness assessment model (TrAM),&quot; Computers in Human Behavior, vol. 170, p. 108671, Sept. 2025, doi: 10.1016/j.chb.2025.108671.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AIassisted Decision-making</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Buçinca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Malaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<idno type="DOI">10.1145/3449287</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Buçinca, M. B. Malaya, and K. Z. Gajos, &quot;To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI- assisted Decision-making,&quot; Proc. ACM Hum.-Comput. Interact., vol. 5, no. CSCW1, pp. 1-21, Apr. 2021, doi: 10.1145/3449287.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Designing for Responsible Trust in AI Systems: A Communication Perspective</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sundar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3531146.3533182</idno>
	</analytic>
	<monogr>
		<title level="m">2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
	<note type="raw_reference">Q. V. Liao and S. S. Sundar, &quot;Designing for Responsible Trust in AI Systems: A Communication Perspective,&quot; in 2022 ACM Conference on Fairness, Accountability, and Transparency, June 2022, pp. 1257- 1268. doi: 10.1145/3531146.3533182.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445562</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems<address><addrLine>Yokohama Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Lu and M. Yin, &quot;Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks,&quot; in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, Yokohama Japan: ACM, May 2021, pp. 1-16. doi: 10.1145/3411764.3445562.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Promoting AI Literacy for the Public</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kasinidou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3545947.3573292</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2</title>
		<meeting>the 54th ACM Technical Symposium on Computer Science Education V. 2<address><addrLine>Toronto ON Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023-03">Mar. 2023</date>
			<biblScope unit="page" from="1237" to="1237" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Kasinidou, &quot;Promoting AI Literacy for the Public,&quot; in Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2, Toronto ON Canada: ACM, Mar. 2023, pp. 1237- 1237. doi: 10.1145/3545947.3573292.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<idno type="DOI">10.2759/346720</idno>
		<ptr target="https://data.europa.eu/doi/10.2759/346720" />
		<title level="m">High Level Expert Group on Artificial Intelligence</title>
		<imprint>
			<publisher>LU: Publications Office</publisher>
			<date type="published" when="2019-02-28">2019. Feb. 28, 2025</date>
		</imprint>
		<respStmt>
			<orgName>European Commission ; Directorate General for Communications Networks, Content and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Ethics guidelines for trustworthy AI</note>
	<note type="raw_reference">European Commission. Directorate General for Communications Networks, Content and Technology. and High Level Expert Group on Artificial Intelligence., Ethics guidelines for trustworthy AI. LU: Publications Office, 2019. Accessed: Feb. 28, 2025. [Online]. Available: https://data.europa.eu/doi/10.2759/346720</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trust in AI and Its Role in the Acceptance of AI Technologies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2022.2050543</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1727" to="1739" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Choung, P. David, and A. Ross, &quot;Trust in AI and Its Role in the Acceptance of AI Technologies,&quot; International Journal of Human- Computer Interaction, vol. 39, no. 9, pp. 1727-1739, May 2023, doi: 10.1080/10447318.2022.2050543.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When a Chatbot Disappoints You: Expectancy Violation in Human-Chatbot Interaction in a Social Support Context</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">(</forename><surname>Mj) Rheu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">(</forename><surname>Nancy) Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1177/00936502231221669</idno>
	</analytic>
	<monogr>
		<title level="j">Communication Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="782" to="814" />
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. (Mj) Rheu, Y. (Nancy) Dai, J. Meng, and W. Peng, &quot;When a Chatbot Disappoints You: Expectancy Violation in Human-Chatbot Interaction in a Social Support Context,&quot; Communication Research, vol. 51, no. 7, pp. 782-814, Oct. 2024, doi: 10.1177/00936502231221669.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Artificial intelligence ( AI ), don&apos;t surprise me and stay in your lane: An experimental testing of perceiving humanlike performances of AI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbe2.292</idno>
	</analytic>
	<monogr>
		<title level="j">Human Behav and Emerg Tech</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1023" to="1032" />
			<date type="published" when="2021-12">Dec. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Hong, &quot;Artificial intelligence ( AI ), don&apos;t surprise me and stay in your lane: An experimental testing of perceiving humanlike performances of AI,&quot; Human Behav and Emerg Tech, vol. 3, no. 5, pp. 1023-1032, Dec. 2021, doi: 10.1002/hbe2.292.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The influence of the Nutri-Score on the perceived healthiness of foods labelled with a nutrition claim of sugar</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jürkenbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mehlhose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zühlsdorf</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0272220</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">272220</biblScope>
			<date type="published" when="2022-08">Aug. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Jürkenbeck, C. Mehlhose, and A. Zühlsdorf, &quot;The influence of the Nutri-Score on the perceived healthiness of foods labelled with a nutrition claim of sugar,&quot; PLoS ONE, vol. 17, no. 8, p. e0272220, Aug. 2022, doi: 10.1371/journal.pone.0272220.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Why so skeptical? Investigating the emergence and consequences of consumer skepticism toward web seals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greulich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Löbbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benlian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sunyaev</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.im.2024.103920</idno>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Management</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">103920</biblScope>
			<date type="published" when="2024-03">Mar. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Lins, M. Greulich, J. Löbbers, A. Benlian, and A. Sunyaev, &quot;Why so skeptical? Investigating the emergence and consequences of consumer skepticism toward web seals,&quot; Information &amp; Management, vol. 61, no. 2, p. 103920, Mar. 2024, doi: 10.1016/j.im.2024.103920.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TRUESSEC Trustworthiness Label Recommendations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Guamán</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781003337492-10</idno>
	</analytic>
	<monogr>
		<title level="m">Challenges in Cybersecurity and Privacy -the European Research Landscape</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>River Publishers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="207" to="230" />
		</imprint>
	</monogr>
	<note>1st ed.</note>
	<note type="raw_reference">D. S. Guamán et al., &quot;TRUESSEC Trustworthiness Label Recommendations,&quot; in Challenges in Cybersecurity and Privacy -the European Research Landscape, 1st ed., New York: River Publishers, 2022, pp. 207-230. doi: 10.1201/9781003337492-10.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Scharowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kühne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wettstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brühlmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3593013.3593994</idno>
	</analytic>
	<monogr>
		<title level="m">2023 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting><address><addrLine>Chicago IL USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023-06">June 2023</date>
			<biblScope unit="page" from="248" to="260" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Scharowski, M. Benk, S. J. Kühne, L. Wettstein, and F. Brühlmann, &quot;Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study,&quot; in 2023 ACM Conference on Fairness, Accountability, and Transparency, Chicago IL USA: ACM, June 2023, pp. 248-260. doi: 10.1145/3593013.3593994.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">In Seal We Trust? Investigating the Effect of Certifications on Perceived Trustworthiness of AI Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wischnewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krämer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janiesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schnitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Newen</surname></persName>
		</author>
		<idno type="DOI">10.30658/hmc.8.7</idno>
	</analytic>
	<monogr>
		<title level="j">HMC</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="141" to="162" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Wischnewski, N. Krämer, C. Janiesch, E. Müller, T. Schnitzler, and C. Newen, &quot;In Seal We Trust? Investigating the Effect of Certifications on Perceived Trustworthiness of AI Systems,&quot; HMC, vol. 8, pp. 141- 162, 2024, doi: 10.30658/hmc.8.7.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring How Privacy and Security Factor into IoT Device Purchase Behavior</title>
		<author>
			<persName><forename type="first">P</forename><surname>Emami-Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Cranor</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300764</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>Glasgow Scotland Uk</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Emami-Naeini, H. Dixon, Y. Agarwal, and L. F. Cranor, &quot;Exploring How Privacy and Security Factor into IoT Device Purchase Behavior,&quot; in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, Glasgow Scotland Uk: ACM, May 2019, pp. 1- 12. doi: 10.1145/3290605.3300764.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Schönbrodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zehetleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perugini</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000061</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="339" />
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. D. Schönbrodt, E.-J. Wagenmakers, M. Zehetleitner, and M. Perugini, &quot;Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences.,&quot; Psychological Methods, vol. 22, no. 2, pp. 322-339, June 2017, doi: 10.1037/met0000061.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Development and validation of an artificial intelligence anxiety scale: an initial application in predicting motivated learning behavior</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1080/10494820.2019.1674887</idno>
	</analytic>
	<monogr>
		<title level="j">Interactive Learning Environments</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="634" />
			<date type="published" when="2022-04">Apr. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-Y. Wang and Y.-S. Wang, &quot;Development and validation of an artificial intelligence anxiety scale: an initial application in predicting motivated learning behavior,&quot; Interactive Learning Environments, vol. 30, no. 4, pp. 619-634, Apr. 2022, doi: 10.1080/10494820.2019.1674887.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factors affecting user trust and intention in adopting chatbots: the moderating role of technology anxiety in insurtech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dekkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arcand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rajaobelina</surname></persName>
		</author>
		<author>
			<persName><surname>Ricard</surname></persName>
		</author>
		<idno type="DOI">10.1057/s41264-023-00230-y</idno>
	</analytic>
	<monogr>
		<title level="j">J Financ Serv Mark</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="699" to="728" />
			<date type="published" when="2024-09">Sept. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Dekkal, M. Arcand, S. Prom Tep, L. Rajaobelina, and L. Ricard, &quot;Factors affecting user trust and intention in adopting chatbots: the moderating role of technology anxiety in insurtech,&quot; J Financ Serv Mark, vol. 29, no. 3, pp. 699-728, Sept. 2024, doi: 10.1057/s41264- 023-00230-y.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AI-enabled investment advice: Will users buy it?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y K</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2022.107481</idno>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page">107481</biblScope>
			<date type="published" when="2023-01">Jan. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Y. K. Chua, A. Pal, and S. Banerjee, &quot;AI-enabled investment advice: Will users buy it?,&quot; Computers in Human Behavior, vol. 138, p. 107481, Jan. 2023, doi: 10.1016/j.chb.2022.107481.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The propensity to trust in (automated) technology mediates the links between technology self-efficacy and fear and acceptance of artificial intelligence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Montag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rozgonjuk</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chbr.2023.100315</idno>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">100315</biblScope>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Montag, J. Kraus, M. Baumann, and D. Rozgonjuk, &quot;The propensity to trust in (automated) technology mediates the links between technology self-efficacy and fear and acceptance of artificial intelligence,&quot; Computers in Human Behavior Reports, vol. 11, p. 100315, Aug. 2023, doi: 10.1016/j.chbr.2023.100315.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linking Personality and Trust in Intelligent Assistants</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schadelbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlögl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Groth</surname></persName>
		</author>
		<idno type="DOI">10.3390/mti7060054</idno>
	</analytic>
	<monogr>
		<title level="j">MTI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Schadelbauer, S. Schlögl, and A. Groth, &quot;Linking Personality and Trust in Intelligent Assistants,&quot; MTI, vol. 7, no. 6, p. 54, May 2023, doi: 10.3390/mti7060054.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the Malleability of Consumer Attitudes toward Disruptive Technologies: A Pilot Study of Cryptocurrencies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Treiblmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gorbunov</surname></persName>
		</author>
		<idno type="DOI">10.3390/info13060295</idno>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">295</biblScope>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Treiblmaier and E. Gorbunov, &quot;On the Malleability of Consumer Attitudes toward Disruptive Technologies: A Pilot Study of Cryptocurrencies,&quot; Information, vol. 13, no. 6, p. 295, June 2022, doi: 10.3390/info13060295.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MAILS -Meta AI literacy scale: Development and testing of an AI literacy questionnaire based on well-founded competency models and psychological change-and meta-competencies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carolus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Latoschik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wienrich</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chbah.2023.100014</idno>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior: Artificial Humans</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">100014</biblScope>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Carolus, M. J. Koch, S. Straka, M. E. Latoschik, and C. Wienrich, &quot;MAILS -Meta AI literacy scale: Development and testing of an AI literacy questionnaire based on well-founded competency models and psychological change-and meta-competencies,&quot; Computers in Human Behavior: Artificial Humans, vol. 1, no. 2, p. 100014, Aug. 2023, doi: 10.1016/j.chbah.2023.100014.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
